{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 抓資料 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1951,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import statsmodels.api as sm \n",
    "import scipy.stats as scs \n",
    "from scipy.fftpack import fft,ifft\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import datetime, time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1952,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取股票清單\n",
    "tai50 = \"https://www.taifex.com.tw/cht/9/futuresQADetail\"\n",
    "response_tai50 = requests.get(tai50)\n",
    "soup = bs(response_tai50.text)\n",
    "stock_list = re.findall(r\"[\\d][\\d][\\d][\\d][\\ ]\",soup.text)#抓出股票代碼的形式\n",
    "stock_list = re.findall(r\"[\\d][\\d][\\d][\\d]\",str(stock_list[::2]))#取權重前450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2000,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datetime.datetime.today()#抓日期\n",
    "#int(time.mktime(t.timetuple()))yahoo財經的日期為秒,因此轉秒\n",
    "names = locals()\n",
    "for etf in stock_list:\n",
    "    try:\n",
    "        site = \"https://query1.finance.yahoo.com/v7/finance/download/%s.TW?period1=1483228800&period2=%s&interval=1d&events=history\"%(etf,int(time.mktime(t.timetuple())))\n",
    "        response = requests.get(site)\n",
    "        names[\"df_%s\"%etf] = pd.read_csv(StringIO(response.text))\n",
    "        names[\"df_%s\"%etf] = names[\"df_%s\"%etf].fillna(method='ffill')\n",
    "        names[\"df_%s\"%etf][\"價格漲幅\"]=names[\"df_%s\"%etf][\"Adj Close\"].diff()\n",
    "        names[\"df_%s\"%etf][\"成交量變動\"]=names[\"df_%s\"%etf][\"Volume\"].diff()\n",
    "        names[\"df_%s\"%etf] = names[\"df_%s\"%etf].dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)    \n",
    "        names[\"df_%s\"%etf] = names[\"df_%s\"%etf].rename(columns={\n",
    " 'Open':'Open_%s'% etf,\n",
    " 'High':'High_%s'% etf,\n",
    " 'Low':'Low_%s'% etf,\n",
    " 'Close':'Close_%s'% etf,\n",
    " 'Adj Close':'Adj Close_%s'% etf,\n",
    " 'Volume':'Volume_%s'% etf,\n",
    " '價格漲幅':'價格漲幅_%s'% etf,\n",
    " '漲幅...':'漲幅..._%s'% etf,\n",
    " '振幅...':'振幅..._%s'% etf,\n",
    " '成交量變動':'成交量變動_%s'% etf,})\n",
    "    except:\n",
    "        print(names[\"df_%s\"%etf],\"抓不到\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2001,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#建立df清單\n",
    "stock_df_list=[]\n",
    "for stock in stock_list:\n",
    "    stock_df =\"df_\"+stock\n",
    "    stock_df_list.append(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2002,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併成一個df\n",
    "all_stock_df=df_2330.iloc[:,[0]]\n",
    "for stock_df in stock_df_list:\n",
    "    all_stock_df = pd.merge(all_stock_df, names[stock_df], how='left', on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 資料預處理 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1956,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量自回歸抓領先變數\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.base.datetools import dates_from_str\n",
    "import math\n",
    "from statsmodels.tsa.vector_ar.hypothesis_test_results import \\\n",
    "    CausalityTestResults, NormalityTestResults, WhitenessTestResults\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2003,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先把shift的收盤價帶入當成train\n",
    "FITX[\"收盤價shift\"] = FITX[\"收盤價\"].shift()\n",
    "all_stock_df[\"Date\"] = all_stock_df[\"Date\"].astype(str)\n",
    "all_stock_df_FITX = all_stock_df\n",
    "all_stock_df_FITX = pd.merge(all_stock_df_FITX, FITX.iloc[:,[0,19]], how='left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前資料共 834 根日K\n"
     ]
    }
   ],
   "source": [
    "print(\"目前資料共\",df_2330.shape[0],\"根日K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2041,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "顯著的因子有 481 個\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#逐一併入再找出有領先因子\n",
    "#統計區間終點=日前日K-1\n",
    "間隔 = 500\n",
    "統計區間起點 = 統計區間終點-間隔\n",
    "統計區間終點 =780     #樣本內\n",
    "Significant_factor = df_2330.iloc[0:,[0]]\n",
    "FITX_close = pd.read_csv(r\"C:\\Users\\user\\Desktop\\class\\金融\\台指近.csv\")\n",
    "FITX_close = FITX_close.rename(columns={\"時間\":\"Date\"}).iloc[0:,[0,4]]\n",
    "FITX_close[\"Date\"] = pd.to_datetime(FITX_close[\"Date\"])  #日期格式化\n",
    "FITX_close[\"Date\"] = FITX_close[\"Date\"].astype(str)  #merge的那個形態要一樣\n",
    "for i in stock_df_list:\n",
    "    try:\n",
    "        names['%s' % i][\"Date\"] = names['%s' % i][\"Date\"].astype(str)\n",
    "        append_FITX = pd.merge(names['%s' % i], FITX_close, how='left', on='Date')\n",
    "        model = VAR(append_FITX.iloc[統計區間起點:統計區間終點,1:append_FITX.shape[1]])  #用樣本內跑檢定,避免選擇性偏誤\n",
    "        results = model.fit(1)\n",
    "        for a in range(1,append_FITX.shape[1]-1):\n",
    "            p_value = results.test_causality(\"收盤價\",causing=append_FITX.iloc[:,[a]], kind='f', signif=0.1).pvalue\n",
    "            if p_value<0.1:\n",
    "                Significant_factor = pd.merge(Significant_factor, append_FITX.iloc[:,[0,a]], how='left', on='Date')\n",
    "    except:\n",
    "        pass\n",
    "Significant_factor = Significant_factor.dropna(axis=1, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "# 共線性,去除vif>100的\n",
    "cc = sp.corrcoef(Significant_factor.iloc[:,1:Significant_factor.shape[1]], rowvar=False)\n",
    "VIF = np.linalg.inv(cc)\n",
    "VIF_list=VIF.diagonal()\n",
    "VIF_100 = np.where(VIF_list>100)    #找VIF>100的列\n",
    "VIF_100 = VIF_100[0][1:]            #留日期\n",
    "Significant_factor.drop(Significant_factor.iloc[:,1:Significant_factor.shape[1]].columns[VIF_100], axis=1,inplace=True)\n",
    "Significant_factor.shape\n",
    "\n",
    "\n",
    "print(\"顯著的因子有\",Significant_factor.shape[1],\"個\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 建模ML </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2042,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, auc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error,mean_absolute_error\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sqlite3\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2043,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "測試每個模型的準確度\n",
    "樣本太少了,怕overfit暫時不調參數\n",
    "\"\"\"\n",
    "model_xgb = xgb.XGBRegressor()\n",
    "model_RandomForestRegressor = RandomForestRegressor()\n",
    "model_LinearSVR = LinearSVR()#不好\n",
    "model_SVR = SVR()#不好\n",
    "model_AdaBoostRegressor = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "model_BaggingRegressor = BaggingRegressor(base_estimator=SVR(),n_estimators=10, random_state=0)#不好\n",
    "model_KernelRidge = KernelRidge()   #不能標準化與正規化,why?\n",
    "model_NuSVR = NuSVR()#不好\n",
    "# model_DecisionTreeRegressor = DecisionTreeRegressor()\n",
    "Vote = VotingRegressor([(\"model_xgb\",model_xgb),\n",
    "                        (\"model_RandomForestRegressor\",model_RandomForestRegressor),\n",
    "                        (\"model_AdaBoostRegressor\",model_AdaBoostRegressor),\n",
    "#                        (\"model_KNeighborsRegressor\",model_KNeighborsRegressor),\n",
    "#                        (\"model_DecisionTreeRegressor\",model_DecisionTreeRegressor)\n",
    "                        ])\n",
    "model_list = [Vote,model_xgb,model_RandomForestRegressor,model_LinearSVR,model_AdaBoostRegressor\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2044,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併\n",
    "FITX = pd.read_csv(r\"C:\\Users\\user\\Desktop\\class\\金融\\台指近.csv\")\n",
    "FITX = FITX.rename(columns={\"時間\":\"Date\"})\n",
    "FITX[\"Date\"] = pd.to_datetime(FITX[\"Date\"])  #日期格式化\n",
    "FITX[\"Date\"] = FITX[\"Date\"].astype(str)  #merge的那個形態要一樣\n",
    "FITX[\"(期權)未平倉\"] = FITX[\"(期權)未平倉\"].str.replace(\"口\",\"\")\n",
    "FITX[\"(期權)未平倉\"] = FITX[\"(期權)未平倉\"].astype(float)\n",
    "FITX[\"(期權)未平倉變化\"] = FITX[\"(期權)未平倉變化\"].str.replace(\"口\",\"\")\n",
    "FITX[\"(期權)未平倉變化\"] = FITX[\"(期權)未平倉變化\"].astype(float)\n",
    "FITX[\"diff收盤價\"] = FITX[\"收盤價\"].diff()\n",
    "FITX[\"Target\"] = FITX[\"收盤價\"].shift(-1)\n",
    "FITX_target = FITX.iloc[:,[0,4,FITX.shape[1]-1]]\n",
    "Significant_factor_test =  pd.merge(Significant_factor, FITX_target, how='left', on='Date')\n",
    "\n",
    "#拆分訓練測試\n",
    "\n",
    "all_feature= Significant_factor_test\n",
    "train_data = all_feature.iloc[統計區間起點:統計區間終點,1:all_feature.shape[1]-1]\n",
    "train_targets = all_feature.iloc[統計區間起點:統計區間終點,[all_feature.shape[1]-1]]\n",
    "test_data = all_feature.iloc[統計區間終點:,1:all_feature.shape[1]-1]\n",
    "test_targets = all_feature.iloc[統計區間終點:,all_feature.shape[1]-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<statsmodels.tsa.vector_ar.var_model.VAR object at 0x0000026C63730E48> 平均誤差點數為 185.6991909969442\n"
     ]
    }
   ],
   "source": [
    "#訓練\n",
    "#XGboost\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "n_estimators =2000,\n",
    "learning_rate =0.01,\n",
    "max_depth=2000 ,\n",
    "booster =\"gbtree\" ,\n",
    "n_jobs = 500,\n",
    "\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "model_xgb.fit(train_data, train_targets)\n",
    "y_pred = model_xgb.predict(test_data)\n",
    "print(model,\"平均誤差點數為\",mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2046,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#視覺化,紅色為標準答案\n",
    "plt.style.use('seaborn')\n",
    "test_targets_array=test_targets.values\n",
    "plt.figure(figsize=(15, 6)) \n",
    "xt = test_data[:]\n",
    "plt.plot(test_targets_array, 'rd', label='test_targets_array')\n",
    "plt.plot(model_xgb.predict(xt), 'gd', label='model_xgb')\n",
    "# plt.plot(model_AdaBoostRegressor.predict(xt), '#00008B', label='model_AdaBoostRegressor')\n",
    "# plt.plot(Vote.predict(xt), 'bd', label='Vote')\n",
    "\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False,\n",
    "                labelbottom=False)\n",
    "plt.ylabel('predicted')\n",
    "plt.xlabel('training samples')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Comparison of individual predictions with test_targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天的預測價格是 11622.679\n",
      "上漲 37.30078 點\n"
     ]
    }
   ],
   "source": [
    "#預測結果\n",
    "print(\"明天的預測價格是\",y_pred[y_pred.shape[0]-1])\n",
    "if (y_pred[y_pred.shape[0]-1] - y_pred[y_pred.shape[0]-2])>=0:\n",
    "    print(\"上漲\",y_pred[y_pred.shape[0]-1] - y_pred[y_pred.shape[0]-2],\"點\")\n",
    "else:\n",
    "    print(\"下跌\",y_pred[y_pred.shape[0]-2] - y_pred[y_pred.shape[0]-1],\"點\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2067,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 天積獲利 46.0 第 1 天累積獲利 46.0\n",
      "第 2 天積獲利 -52.0 第 2 天累積獲利 -6.0\n",
      "第 3 天積獲利 -13.0 第 3 天累積獲利 -19.0\n",
      "第 4 天積獲利 -29.0 第 4 天累積獲利 -48.0\n",
      "第 5 天積獲利 -40.0 第 5 天累積獲利 -88.0\n",
      "第 6 天積獲利 268.0 第 6 天累積獲利 180.0\n",
      "第 7 天積獲利 175.0 第 7 天累積獲利 355.0\n",
      "第 8 天積獲利 124.0 第 8 天累積獲利 479.0\n",
      "第 9 天積獲利 22.0 第 9 天累積獲利 501.0\n",
      "第 10 天積獲利 -33.0 第 10 天累積獲利 468.0\n",
      "第 11 天積獲利 -54.0 第 11 天累積獲利 414.0\n",
      "第 12 天積獲利 264.0 第 12 天累積獲利 678.0\n",
      "第 13 天積獲利 108.0 第 13 天累積獲利 786.0\n",
      "第 14 天積獲利 -150.0 第 14 天累積獲利 636.0\n",
      "第 15 天積獲利 -262.0 第 15 天累積獲利 374.0\n",
      "第 16 天積獲利 -88.0 第 16 天累積獲利 286.0\n",
      "第 17 天積獲利 279.0 第 17 天累積獲利 565.0\n",
      "第 18 天積獲利 -26.0 第 18 天累積獲利 539.0\n",
      "第 19 天積獲利 -70.0 第 19 天累積獲利 469.0\n",
      "第 20 天積獲利 -24.0 第 20 天累積獲利 445.0\n",
      "第 21 天積獲利 277.0 第 21 天累積獲利 722.0\n",
      "第 22 天積獲利 38.0 第 22 天累積獲利 760.0\n",
      "第 23 天積獲利 126.0 第 23 天累積獲利 886.0\n",
      "第 24 天積獲利 238.0 第 24 天累積獲利 1124.0\n",
      "第 25 天積獲利 -337.0 第 25 天累積獲利 787.0\n",
      "第 26 天積獲利 -109.0 第 26 天累積獲利 678.0\n",
      "第 27 天積獲利 30.0 第 27 天累積獲利 708.0\n",
      "第 28 天積獲利 84.0 第 28 天累積獲利 792.0\n",
      "第 29 天積獲利 61.0 第 29 天累積獲利 853.0\n",
      "第 30 天積獲利 88.0 第 30 天累積獲利 941.0\n",
      "第 31 天積獲利 -117.0 第 31 天累積獲利 824.0\n",
      "第 32 天積獲利 -49.0 第 32 天累積獲利 775.0\n",
      "第 33 天積獲利 -165.0 第 33 天累積獲利 610.0\n",
      "第 34 天積獲利 -68.0 第 34 天累積獲利 542.0\n",
      "第 35 天積獲利 -56.0 第 35 天累積獲利 486.0\n",
      "第 36 天積獲利 -107.0 第 36 天累積獲利 379.0\n",
      "第 37 天積獲利 19.0 第 37 天累積獲利 398.0\n",
      "第 38 天積獲利 66.0 第 38 天累積獲利 464.0\n",
      "第 39 天積獲利 -209.0 第 39 天累積獲利 255.0\n",
      "第 40 天積獲利 -77.0 第 40 天累積獲利 178.0\n",
      "第 41 天積獲利 163.0 第 41 天累積獲利 341.0\n",
      "第 42 天積獲利 -11.0 第 42 天累積獲利 330.0\n",
      "第 43 天積獲利 62.0 第 43 天累積獲利 392.0\n",
      "第 44 天積獲利 -8.0 第 44 天累積獲利 384.0\n",
      "第 45 天積獲利 138.0 第 45 天累積獲利 522.0\n",
      "第 46 天積獲利 50.0 第 46 天累積獲利 572.0\n",
      "第 47 天積獲利 176.0 第 47 天累積獲利 748.0\n",
      "第 48 天積獲利 75.0 第 48 天累積獲利 823.0\n",
      "第 49 天積獲利 99.0 第 49 天累積獲利 922.0\n",
      "第 50 天積獲利 114.0 第 50 天累積獲利 1036.0\n",
      "第 51 天積獲利 39.0 第 51 天累積獲利 1075.0\n",
      "第 52 天積獲利 -96.0 第 52 天累積獲利 979.0\n",
      "第 53 天積獲利 nan 第 53 天累積獲利 nan\n"
     ]
    }
   ],
   "source": [
    "#簡易回測,方法1  視預測值漲跌決定買賣\n",
    "predict = model_xgb.predict(xt)\n",
    "all_profit = 0\n",
    "profit = 0\n",
    "for i in range(1,len(predict)):\n",
    "    if predict[i]>=predict[i-1]:\n",
    "        profit = test_targets_array[i]-test_targets_array[i-1]\n",
    "        all_profit+=profit\n",
    "    else:\n",
    "        profit = test_targets_array[i-1]-test_targets_array[i]\n",
    "        all_profit+=profit\n",
    "    print(\"第\", i,\"天積獲利\",profit,\"第\", i,\"天累積獲利\",all_profit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2068,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 天累積獲利 -92.0\n",
      "第 2 天累積獲利 12.0\n",
      "第 3 天累積獲利 38.0\n",
      "第 4 天累積獲利 -20.0\n",
      "第 5 天累積獲利 60.0\n",
      "第 6 天累積獲利 -476.0\n",
      "第 7 天累積獲利 -826.0\n",
      "第 8 天累積獲利 -1074.0\n",
      "第 9 天累積獲利 -1030.0\n",
      "第 10 天累積獲利 -1096.0\n",
      "第 11 天累積獲利 -988.0\n",
      "第 12 天累積獲利 -1516.0\n",
      "第 13 天累積獲利 -1732.0\n",
      "第 14 天累積獲利 -1432.0\n",
      "第 15 天累積獲利 -1956.0\n",
      "第 16 天累積獲利 -1780.0\n",
      "第 17 天累積獲利 -1222.0\n",
      "第 18 天累積獲利 -1274.0\n",
      "第 19 天累積獲利 -1414.0\n",
      "第 20 天累積獲利 -1366.0\n",
      "第 21 天累積獲利 -1920.0\n",
      "第 22 天累積獲利 -1996.0\n",
      "第 23 天累積獲利 -2248.0\n",
      "第 24 天累積獲利 -2724.0\n",
      "第 25 天累積獲利 -3398.0\n",
      "第 26 天累積獲利 -3616.0\n",
      "第 27 天累積獲利 -3676.0\n",
      "第 28 天累積獲利 -3508.0\n",
      "第 29 天累積獲利 -3386.0\n",
      "第 30 天累積獲利 -3562.0\n",
      "第 31 天累積獲利 -3328.0\n",
      "第 32 天累積獲利 -3426.0\n",
      "第 33 天累積獲利 -3756.0\n",
      "第 34 天累積獲利 -3892.0\n",
      "第 35 天累積獲利 -4004.0\n",
      "第 36 天累積獲利 -4218.0\n",
      "第 37 天累積獲利 -4256.0\n",
      "第 38 天累積獲利 -4124.0\n",
      "第 39 天累積獲利 -4542.0\n",
      "第 40 天累積獲利 -4696.0\n",
      "第 41 天累積獲利 -5022.0\n",
      "第 42 天累積獲利 -5000.0\n",
      "第 43 天累積獲利 -4876.0\n",
      "第 44 天累積獲利 -4860.0\n",
      "第 45 天累積獲利 -4584.0\n",
      "第 46 天累積獲利 -4484.0\n",
      "第 47 天累積獲利 -4132.0\n",
      "第 48 天累積獲利 -3982.0\n",
      "第 49 天累積獲利 -3784.0\n",
      "第 50 天累積獲利 -3556.0\n",
      "第 51 天累積獲利 -3478.0\n",
      "第 52 天累積獲利 -3670.0\n",
      "第 53 天累積獲利 nan\n"
     ]
    }
   ],
   "source": [
    "#簡易回測,方法2  視預測值與實際值差額決定買賣\n",
    "predict = model_xgb.predict(xt)\n",
    "all_profit = 0\n",
    "profit = 0\n",
    "for i in range(1,len(predict)):\n",
    "    if predict[i]>=test_targets_array[i-1]:\n",
    "        profit = test_targets_array[i]-test_targets_array[i-1]\n",
    "        all_profit+=profit\n",
    "    else:\n",
    "        profit = test_targets_array[i-1]-test_targets_array[i]\n",
    "        all_profit+=profit\n",
    "    all_profit+=profit\n",
    "    print(\"第\", i,\"天累積獲利\",all_profit)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8B4513 size=100 face=\"標楷體\"> 建模DL </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_2330</th>\n",
       "      <th>成交量變動_2330</th>\n",
       "      <th>Low_6505</th>\n",
       "      <th>成交量變動_6505</th>\n",
       "      <th>成交量變動_2454</th>\n",
       "      <th>成交量變動_2882</th>\n",
       "      <th>Volume_2881</th>\n",
       "      <th>成交量變動_2881</th>\n",
       "      <th>Volume_2886</th>\n",
       "      <th>成交量變動_2886</th>\n",
       "      <th>...</th>\n",
       "      <th>成交量變動_3617</th>\n",
       "      <th>成交量變動_8996</th>\n",
       "      <th>High_1701</th>\n",
       "      <th>成交量變動_1701</th>\n",
       "      <th>成交量變動_1709</th>\n",
       "      <th>價格漲幅_2010</th>\n",
       "      <th>Volume_3530</th>\n",
       "      <th>價格漲幅_3454</th>\n",
       "      <th>收盤價</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24369000.0</td>\n",
       "      <td>1739000.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>1288000.0</td>\n",
       "      <td>-71000.0</td>\n",
       "      <td>3499925.0</td>\n",
       "      <td>10406000.0</td>\n",
       "      <td>5136000.0</td>\n",
       "      <td>13865000.0</td>\n",
       "      <td>4862000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>271000.0</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>-232638.0</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>17201.0</td>\n",
       "      <td>-0.244285</td>\n",
       "      <td>9271.0</td>\n",
       "      <td>9342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20979000.0</td>\n",
       "      <td>-3390000.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>-309000.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>5123312.0</td>\n",
       "      <td>10166000.0</td>\n",
       "      <td>-240000.0</td>\n",
       "      <td>17662000.0</td>\n",
       "      <td>3797000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>-196000.0</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>360786.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9342.0</td>\n",
       "      <td>9365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22443000.0</td>\n",
       "      <td>1464000.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>-1788000.0</td>\n",
       "      <td>599000.0</td>\n",
       "      <td>1482747.0</td>\n",
       "      <td>6993000.0</td>\n",
       "      <td>-3173000.0</td>\n",
       "      <td>11226000.0</td>\n",
       "      <td>-6436000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-47000.0</td>\n",
       "      <td>-41000.0</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>-13000.0</td>\n",
       "      <td>-325299.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>-1.221344</td>\n",
       "      <td>9365.0</td>\n",
       "      <td>9342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18569000.0</td>\n",
       "      <td>-3874000.0</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>244000.0</td>\n",
       "      <td>-1805000.0</td>\n",
       "      <td>-1105028.0</td>\n",
       "      <td>13755000.0</td>\n",
       "      <td>6762000.0</td>\n",
       "      <td>9055000.0</td>\n",
       "      <td>-2171000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>142000.0</td>\n",
       "      <td>-34000.0</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>-71000.0</td>\n",
       "      <td>235596.0</td>\n",
       "      <td>0.085321</td>\n",
       "      <td>58260.0</td>\n",
       "      <td>0.895691</td>\n",
       "      <td>9342.0</td>\n",
       "      <td>9341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20198000.0</td>\n",
       "      <td>1629000.0</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>1599000.0</td>\n",
       "      <td>1513000.0</td>\n",
       "      <td>-684113.0</td>\n",
       "      <td>10076000.0</td>\n",
       "      <td>-3679000.0</td>\n",
       "      <td>13438000.0</td>\n",
       "      <td>4383000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>-103505.0</td>\n",
       "      <td>-0.085321</td>\n",
       "      <td>13100.0</td>\n",
       "      <td>-0.162827</td>\n",
       "      <td>9341.0</td>\n",
       "      <td>9337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29107000.0</td>\n",
       "      <td>8909000.0</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>-1410000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>2134713.0</td>\n",
       "      <td>9462000.0</td>\n",
       "      <td>-614000.0</td>\n",
       "      <td>13908000.0</td>\n",
       "      <td>470000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-75000.0</td>\n",
       "      <td>-84000.0</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>-58000.0</td>\n",
       "      <td>376559.0</td>\n",
       "      <td>0.085321</td>\n",
       "      <td>56230.0</td>\n",
       "      <td>-0.651405</td>\n",
       "      <td>9337.0</td>\n",
       "      <td>9411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41130000.0</td>\n",
       "      <td>12023000.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>646000.0</td>\n",
       "      <td>-1253000.0</td>\n",
       "      <td>1938822.0</td>\n",
       "      <td>15440000.0</td>\n",
       "      <td>5978000.0</td>\n",
       "      <td>16765000.0</td>\n",
       "      <td>2857000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-94000.0</td>\n",
       "      <td>-27000.0</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>-327271.0</td>\n",
       "      <td>0.085319</td>\n",
       "      <td>4050.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9411.0</td>\n",
       "      <td>9375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52352000.0</td>\n",
       "      <td>11222000.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>-1281000.0</td>\n",
       "      <td>1009000.0</td>\n",
       "      <td>-2220101.0</td>\n",
       "      <td>12490000.0</td>\n",
       "      <td>-2950000.0</td>\n",
       "      <td>11565000.0</td>\n",
       "      <td>-5200000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>-21000.0</td>\n",
       "      <td>-58159.0</td>\n",
       "      <td>-0.042661</td>\n",
       "      <td>6610.0</td>\n",
       "      <td>-0.407112</td>\n",
       "      <td>9375.0</td>\n",
       "      <td>9285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30756000.0</td>\n",
       "      <td>-21596000.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>2859000.0</td>\n",
       "      <td>1114000.0</td>\n",
       "      <td>5643679.0</td>\n",
       "      <td>11326000.0</td>\n",
       "      <td>-1164000.0</td>\n",
       "      <td>10179000.0</td>\n",
       "      <td>-1386000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-384000.0</td>\n",
       "      <td>-2000.0</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>29572.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>-0.977074</td>\n",
       "      <td>9285.0</td>\n",
       "      <td>9340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13159000.0</td>\n",
       "      <td>-17597000.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>-1569000.0</td>\n",
       "      <td>-1607000.0</td>\n",
       "      <td>-4130796.0</td>\n",
       "      <td>8636000.0</td>\n",
       "      <td>-2690000.0</td>\n",
       "      <td>13976000.0</td>\n",
       "      <td>3797000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>131000.0</td>\n",
       "      <td>-35000.0</td>\n",
       "      <td>18.049999</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>-105476.0</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.569954</td>\n",
       "      <td>9340.0</td>\n",
       "      <td>9340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23693000.0</td>\n",
       "      <td>10534000.0</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>1594000.0</td>\n",
       "      <td>1454000.0</td>\n",
       "      <td>3726959.0</td>\n",
       "      <td>15337000.0</td>\n",
       "      <td>6701000.0</td>\n",
       "      <td>15018000.0</td>\n",
       "      <td>1042000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-143000.0</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>-140963.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>9340.0</td>\n",
       "      <td>9304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24627000.0</td>\n",
       "      <td>934000.0</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>-753000.0</td>\n",
       "      <td>-861000.0</td>\n",
       "      <td>-3746045.0</td>\n",
       "      <td>9835000.0</td>\n",
       "      <td>-5502000.0</td>\n",
       "      <td>14378000.0</td>\n",
       "      <td>-640000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>-169000.0</td>\n",
       "      <td>338114.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>-0.162826</td>\n",
       "      <td>9304.0</td>\n",
       "      <td>9330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23429000.0</td>\n",
       "      <td>-1198000.0</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>-456000.0</td>\n",
       "      <td>-162000.0</td>\n",
       "      <td>-751419.0</td>\n",
       "      <td>10684000.0</td>\n",
       "      <td>849000.0</td>\n",
       "      <td>14870000.0</td>\n",
       "      <td>492000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-90000.0</td>\n",
       "      <td>-35000.0</td>\n",
       "      <td>18.150000</td>\n",
       "      <td>123000.0</td>\n",
       "      <td>625955.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.244285</td>\n",
       "      <td>9330.0</td>\n",
       "      <td>9407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36992000.0</td>\n",
       "      <td>13563000.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>209000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>4606962.0</td>\n",
       "      <td>11111000.0</td>\n",
       "      <td>427000.0</td>\n",
       "      <td>14375000.0</td>\n",
       "      <td>-495000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>-399231.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9407.0</td>\n",
       "      <td>9442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>55022000.0</td>\n",
       "      <td>18030000.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>-1070000.0</td>\n",
       "      <td>2085000.0</td>\n",
       "      <td>-3062937.0</td>\n",
       "      <td>9815000.0</td>\n",
       "      <td>-1296000.0</td>\n",
       "      <td>16408000.0</td>\n",
       "      <td>2033000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-52000.0</td>\n",
       "      <td>-76000.0</td>\n",
       "      <td>18.150000</td>\n",
       "      <td>-125000.0</td>\n",
       "      <td>-48302.0</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>9442.0</td>\n",
       "      <td>9375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>94089000.0</td>\n",
       "      <td>39067000.0</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>3164000.0</td>\n",
       "      <td>11689000.0</td>\n",
       "      <td>9932194.0</td>\n",
       "      <td>23721000.0</td>\n",
       "      <td>13906000.0</td>\n",
       "      <td>23315000.0</td>\n",
       "      <td>6907000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>-62000.0</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>163636.0</td>\n",
       "      <td>0.298620</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>-0.081367</td>\n",
       "      <td>9375.0</td>\n",
       "      <td>9437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29518000.0</td>\n",
       "      <td>-64571000.0</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>-2751000.0</td>\n",
       "      <td>-9797000.0</td>\n",
       "      <td>-10086898.0</td>\n",
       "      <td>26314000.0</td>\n",
       "      <td>2593000.0</td>\n",
       "      <td>19274000.0</td>\n",
       "      <td>-4041000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>412045.0</td>\n",
       "      <td>-0.127980</td>\n",
       "      <td>11700.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9437.0</td>\n",
       "      <td>9505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24779000.0</td>\n",
       "      <td>-4739000.0</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>1419000.0</td>\n",
       "      <td>19456531.0</td>\n",
       "      <td>23328000.0</td>\n",
       "      <td>-2986000.0</td>\n",
       "      <td>18709000.0</td>\n",
       "      <td>-565000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-83000.0</td>\n",
       "      <td>-51000.0</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>292000.0</td>\n",
       "      <td>386416.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>9520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23616000.0</td>\n",
       "      <td>-1163000.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>525000.0</td>\n",
       "      <td>-278000.0</td>\n",
       "      <td>-19183287.0</td>\n",
       "      <td>9368000.0</td>\n",
       "      <td>-13960000.0</td>\n",
       "      <td>13725000.0</td>\n",
       "      <td>-4984000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>137000.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>-137000.0</td>\n",
       "      <td>3316081.0</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>15600.0</td>\n",
       "      <td>1.628472</td>\n",
       "      <td>9520.0</td>\n",
       "      <td>9515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29168000.0</td>\n",
       "      <td>5552000.0</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>621000.0</td>\n",
       "      <td>-314000.0</td>\n",
       "      <td>1952886.0</td>\n",
       "      <td>11629000.0</td>\n",
       "      <td>2261000.0</td>\n",
       "      <td>14079000.0</td>\n",
       "      <td>354000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>-91000.0</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>-106000.0</td>\n",
       "      <td>-2780816.0</td>\n",
       "      <td>-0.042660</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>-0.651322</td>\n",
       "      <td>9515.0</td>\n",
       "      <td>9574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18967000.0</td>\n",
       "      <td>-10201000.0</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>-686000.0</td>\n",
       "      <td>-3649000.0</td>\n",
       "      <td>-7378574.0</td>\n",
       "      <td>11403000.0</td>\n",
       "      <td>-226000.0</td>\n",
       "      <td>16335000.0</td>\n",
       "      <td>2256000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13000.0</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>18.549999</td>\n",
       "      <td>-92000.0</td>\n",
       "      <td>-562866.0</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>-0.325744</td>\n",
       "      <td>9574.0</td>\n",
       "      <td>9657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>34747000.0</td>\n",
       "      <td>15780000.0</td>\n",
       "      <td>104.500000</td>\n",
       "      <td>927000.0</td>\n",
       "      <td>1514000.0</td>\n",
       "      <td>30070828.0</td>\n",
       "      <td>25120000.0</td>\n",
       "      <td>13717000.0</td>\n",
       "      <td>21715000.0</td>\n",
       "      <td>5380000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-32000.0</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>3054000.0</td>\n",
       "      <td>302627.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.407112</td>\n",
       "      <td>9657.0</td>\n",
       "      <td>9704.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25124000.0</td>\n",
       "      <td>-9623000.0</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>-197000.0</td>\n",
       "      <td>-806000.0</td>\n",
       "      <td>-30077860.0</td>\n",
       "      <td>12204000.0</td>\n",
       "      <td>-12916000.0</td>\n",
       "      <td>9382000.0</td>\n",
       "      <td>-12333000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>-107000.0</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>-2148000.0</td>\n",
       "      <td>-699886.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>0.081459</td>\n",
       "      <td>9704.0</td>\n",
       "      <td>9708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45696000.0</td>\n",
       "      <td>20572000.0</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>935000.0</td>\n",
       "      <td>-945000.0</td>\n",
       "      <td>11260236.0</td>\n",
       "      <td>15711000.0</td>\n",
       "      <td>3507000.0</td>\n",
       "      <td>12934000.0</td>\n",
       "      <td>3552000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>-345000.0</td>\n",
       "      <td>-403174.0</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>-0.570030</td>\n",
       "      <td>9708.0</td>\n",
       "      <td>9799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>39347000.0</td>\n",
       "      <td>-6349000.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>-586000.0</td>\n",
       "      <td>1202000.0</td>\n",
       "      <td>-4408058.0</td>\n",
       "      <td>19813000.0</td>\n",
       "      <td>4102000.0</td>\n",
       "      <td>21066000.0</td>\n",
       "      <td>8132000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-158000.0</td>\n",
       "      <td>-88000.0</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>-250000.0</td>\n",
       "      <td>459362.0</td>\n",
       "      <td>0.170640</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>0.651405</td>\n",
       "      <td>9799.0</td>\n",
       "      <td>9770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27018000.0</td>\n",
       "      <td>-12329000.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>2180000.0</td>\n",
       "      <td>-569000.0</td>\n",
       "      <td>10401328.0</td>\n",
       "      <td>15390000.0</td>\n",
       "      <td>-4423000.0</td>\n",
       "      <td>24554000.0</td>\n",
       "      <td>3488000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-101000.0</td>\n",
       "      <td>-66000.0</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>4422000.0</td>\n",
       "      <td>-986.0</td>\n",
       "      <td>-0.042660</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>-0.732780</td>\n",
       "      <td>9770.0</td>\n",
       "      <td>9763.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20389000.0</td>\n",
       "      <td>-6629000.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>-4189000.0</td>\n",
       "      <td>10452000.0</td>\n",
       "      <td>-12910746.0</td>\n",
       "      <td>12569000.0</td>\n",
       "      <td>-2821000.0</td>\n",
       "      <td>16627000.0</td>\n",
       "      <td>-7927000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>-15000.0</td>\n",
       "      <td>20.700001</td>\n",
       "      <td>-1022000.0</td>\n",
       "      <td>6679477.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26500.0</td>\n",
       "      <td>0.732780</td>\n",
       "      <td>9763.0</td>\n",
       "      <td>9786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31159000.0</td>\n",
       "      <td>17905000.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>-342000.0</td>\n",
       "      <td>2312000.0</td>\n",
       "      <td>2588779.0</td>\n",
       "      <td>14365000.0</td>\n",
       "      <td>3315000.0</td>\n",
       "      <td>14429000.0</td>\n",
       "      <td>-416000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-103000.0</td>\n",
       "      <td>-212000.0</td>\n",
       "      <td>20.450001</td>\n",
       "      <td>-2254000.0</td>\n",
       "      <td>505692.0</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>28200.0</td>\n",
       "      <td>0.244194</td>\n",
       "      <td>9767.0</td>\n",
       "      <td>9783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25677000.0</td>\n",
       "      <td>-5482000.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>2629000.0</td>\n",
       "      <td>-2417000.0</td>\n",
       "      <td>4089609.0</td>\n",
       "      <td>12082000.0</td>\n",
       "      <td>-2283000.0</td>\n",
       "      <td>15386000.0</td>\n",
       "      <td>957000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-45000.0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>20.200001</td>\n",
       "      <td>-61000.0</td>\n",
       "      <td>-185322.0</td>\n",
       "      <td>-0.085320</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>2.279869</td>\n",
       "      <td>9783.0</td>\n",
       "      <td>9773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35725000.0</td>\n",
       "      <td>10048000.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>-2219000.0</td>\n",
       "      <td>1221000.0</td>\n",
       "      <td>-2288413.0</td>\n",
       "      <td>23171000.0</td>\n",
       "      <td>11089000.0</td>\n",
       "      <td>14408000.0</td>\n",
       "      <td>-978000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>20.150000</td>\n",
       "      <td>1393000.0</td>\n",
       "      <td>-195179.0</td>\n",
       "      <td>-0.170640</td>\n",
       "      <td>18500.0</td>\n",
       "      <td>0.407120</td>\n",
       "      <td>9773.0</td>\n",
       "      <td>9760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>39474478.0</td>\n",
       "      <td>-2718102.0</td>\n",
       "      <td>79.199997</td>\n",
       "      <td>174239.0</td>\n",
       "      <td>-3273360.0</td>\n",
       "      <td>-2841784.0</td>\n",
       "      <td>10879769.0</td>\n",
       "      <td>-3475061.0</td>\n",
       "      <td>25997195.0</td>\n",
       "      <td>2556082.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>-713501.0</td>\n",
       "      <td>20.700001</td>\n",
       "      <td>332459.0</td>\n",
       "      <td>-52411.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1063364.0</td>\n",
       "      <td>0.700005</td>\n",
       "      <td>10292.0</td>\n",
       "      <td>10268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>26008090.0</td>\n",
       "      <td>-13466388.0</td>\n",
       "      <td>79.599998</td>\n",
       "      <td>-529700.0</td>\n",
       "      <td>-2765544.0</td>\n",
       "      <td>-3701294.0</td>\n",
       "      <td>9989353.0</td>\n",
       "      <td>-890416.0</td>\n",
       "      <td>24180472.0</td>\n",
       "      <td>-1816723.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-37015.0</td>\n",
       "      <td>-755321.0</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>2293613.0</td>\n",
       "      <td>168423.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1088381.0</td>\n",
       "      <td>-0.599999</td>\n",
       "      <td>10268.0</td>\n",
       "      <td>10545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>38493406.0</td>\n",
       "      <td>12485316.0</td>\n",
       "      <td>80.599998</td>\n",
       "      <td>2221204.0</td>\n",
       "      <td>7169519.0</td>\n",
       "      <td>7947650.0</td>\n",
       "      <td>11851531.0</td>\n",
       "      <td>1862178.0</td>\n",
       "      <td>22145272.0</td>\n",
       "      <td>-2035200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75103.0</td>\n",
       "      <td>2528610.0</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>-2119458.0</td>\n",
       "      <td>88385.0</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1790928.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10545.0</td>\n",
       "      <td>10583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>37395165.0</td>\n",
       "      <td>-1098241.0</td>\n",
       "      <td>81.199997</td>\n",
       "      <td>-1994448.0</td>\n",
       "      <td>-2599134.0</td>\n",
       "      <td>-3206657.0</td>\n",
       "      <td>7655849.0</td>\n",
       "      <td>-4195682.0</td>\n",
       "      <td>27855091.0</td>\n",
       "      <td>5709819.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23897.0</td>\n",
       "      <td>-1005356.0</td>\n",
       "      <td>20.700001</td>\n",
       "      <td>-377415.0</td>\n",
       "      <td>-202085.0</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1740985.0</td>\n",
       "      <td>1.299995</td>\n",
       "      <td>10583.0</td>\n",
       "      <td>10709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>44059301.0</td>\n",
       "      <td>6664136.0</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>4097664.0</td>\n",
       "      <td>20912123.0</td>\n",
       "      <td>10628045.0</td>\n",
       "      <td>15274925.0</td>\n",
       "      <td>7619076.0</td>\n",
       "      <td>37487108.0</td>\n",
       "      <td>9632017.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92000.0</td>\n",
       "      <td>-1131878.0</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>-242639.0</td>\n",
       "      <td>77383.0</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>4387623.0</td>\n",
       "      <td>0.300004</td>\n",
       "      <td>10709.0</td>\n",
       "      <td>10947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>55126085.0</td>\n",
       "      <td>11066784.0</td>\n",
       "      <td>83.900002</td>\n",
       "      <td>4626133.0</td>\n",
       "      <td>-9750296.0</td>\n",
       "      <td>20338182.0</td>\n",
       "      <td>20246120.0</td>\n",
       "      <td>4971195.0</td>\n",
       "      <td>38706740.0</td>\n",
       "      <td>1219632.0</td>\n",
       "      <td>...</td>\n",
       "      <td>103049.0</td>\n",
       "      <td>1839018.0</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>7532590.0</td>\n",
       "      <td>158692.0</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>1963770.0</td>\n",
       "      <td>1.699996</td>\n",
       "      <td>10947.0</td>\n",
       "      <td>10610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>71581861.0</td>\n",
       "      <td>16455776.0</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>-4280802.0</td>\n",
       "      <td>-10009376.0</td>\n",
       "      <td>-12057753.0</td>\n",
       "      <td>24649105.0</td>\n",
       "      <td>4402985.0</td>\n",
       "      <td>36012972.0</td>\n",
       "      <td>-2693768.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-114041.0</td>\n",
       "      <td>-2319323.0</td>\n",
       "      <td>22.450001</td>\n",
       "      <td>3496598.0</td>\n",
       "      <td>-257002.0</td>\n",
       "      <td>-0.160000</td>\n",
       "      <td>1270986.0</td>\n",
       "      <td>-1.900001</td>\n",
       "      <td>10610.0</td>\n",
       "      <td>10719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>23547405.0</td>\n",
       "      <td>-48034456.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>-2030668.0</td>\n",
       "      <td>-4460404.0</td>\n",
       "      <td>-19005400.0</td>\n",
       "      <td>8102348.0</td>\n",
       "      <td>-16546757.0</td>\n",
       "      <td>20851452.0</td>\n",
       "      <td>-15161520.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-95808.0</td>\n",
       "      <td>4212327.0</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>4618223.0</td>\n",
       "      <td>46877.0</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>2934573.0</td>\n",
       "      <td>0.800003</td>\n",
       "      <td>10719.0</td>\n",
       "      <td>10749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>34240479.0</td>\n",
       "      <td>10693074.0</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>619986.0</td>\n",
       "      <td>1265873.0</td>\n",
       "      <td>1982001.0</td>\n",
       "      <td>8991553.0</td>\n",
       "      <td>889205.0</td>\n",
       "      <td>24088407.0</td>\n",
       "      <td>3236955.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-34167.0</td>\n",
       "      <td>-4178984.0</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>11242138.0</td>\n",
       "      <td>1663813.0</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>4405813.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10749.0</td>\n",
       "      <td>10833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>27621198.0</td>\n",
       "      <td>-6619281.0</td>\n",
       "      <td>84.099998</td>\n",
       "      <td>755245.0</td>\n",
       "      <td>-469542.0</td>\n",
       "      <td>-1486916.0</td>\n",
       "      <td>10433712.0</td>\n",
       "      <td>1442159.0</td>\n",
       "      <td>24250749.0</td>\n",
       "      <td>162342.0</td>\n",
       "      <td>...</td>\n",
       "      <td>105104.0</td>\n",
       "      <td>1527733.0</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>-20978664.0</td>\n",
       "      <td>-1170081.0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1580210.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>10833.0</td>\n",
       "      <td>10894.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>32295893.0</td>\n",
       "      <td>4674695.0</td>\n",
       "      <td>85.800003</td>\n",
       "      <td>2314378.0</td>\n",
       "      <td>-1506266.0</td>\n",
       "      <td>-301090.0</td>\n",
       "      <td>9626405.0</td>\n",
       "      <td>-807307.0</td>\n",
       "      <td>18787381.0</td>\n",
       "      <td>-5463368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-47777.0</td>\n",
       "      <td>808219.0</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>278551.0</td>\n",
       "      <td>-209386.0</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>1119000.0</td>\n",
       "      <td>-0.699997</td>\n",
       "      <td>10894.0</td>\n",
       "      <td>10982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>28727019.0</td>\n",
       "      <td>-3568874.0</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>-3334379.0</td>\n",
       "      <td>-97024.0</td>\n",
       "      <td>2603783.0</td>\n",
       "      <td>10618884.0</td>\n",
       "      <td>992479.0</td>\n",
       "      <td>20909260.0</td>\n",
       "      <td>2121879.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5350.0</td>\n",
       "      <td>-1040125.0</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>-3816129.0</td>\n",
       "      <td>29001.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1047210.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10982.0</td>\n",
       "      <td>10865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>52089788.0</td>\n",
       "      <td>23362769.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>739251.0</td>\n",
       "      <td>-1387899.0</td>\n",
       "      <td>-1457557.0</td>\n",
       "      <td>14820252.0</td>\n",
       "      <td>4201368.0</td>\n",
       "      <td>19928977.0</td>\n",
       "      <td>-980283.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-60912.0</td>\n",
       "      <td>1939738.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>339007.0</td>\n",
       "      <td>-564607.0</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>997200.0</td>\n",
       "      <td>-0.900001</td>\n",
       "      <td>10865.0</td>\n",
       "      <td>10914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>26156418.0</td>\n",
       "      <td>-25933370.0</td>\n",
       "      <td>85.099998</td>\n",
       "      <td>-2322926.0</td>\n",
       "      <td>5421699.0</td>\n",
       "      <td>-2997459.0</td>\n",
       "      <td>9772855.0</td>\n",
       "      <td>-5047397.0</td>\n",
       "      <td>16241655.0</td>\n",
       "      <td>-3687322.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-35098.0</td>\n",
       "      <td>1161665.0</td>\n",
       "      <td>22.200001</td>\n",
       "      <td>1343045.0</td>\n",
       "      <td>8979.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>769713.0</td>\n",
       "      <td>-0.800004</td>\n",
       "      <td>10914.0</td>\n",
       "      <td>10749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>34315948.0</td>\n",
       "      <td>8159530.0</td>\n",
       "      <td>85.900002</td>\n",
       "      <td>858958.0</td>\n",
       "      <td>-2210652.0</td>\n",
       "      <td>1280664.0</td>\n",
       "      <td>12588440.0</td>\n",
       "      <td>2815585.0</td>\n",
       "      <td>24415579.0</td>\n",
       "      <td>8173924.0</td>\n",
       "      <td>...</td>\n",
       "      <td>136193.0</td>\n",
       "      <td>-2800755.0</td>\n",
       "      <td>22.549999</td>\n",
       "      <td>777941.0</td>\n",
       "      <td>1102664.0</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>1898735.0</td>\n",
       "      <td>-1.900001</td>\n",
       "      <td>10749.0</td>\n",
       "      <td>10817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>42494804.0</td>\n",
       "      <td>8178856.0</td>\n",
       "      <td>86.400002</td>\n",
       "      <td>-189618.0</td>\n",
       "      <td>-2540291.0</td>\n",
       "      <td>227466.0</td>\n",
       "      <td>16569146.0</td>\n",
       "      <td>3980706.0</td>\n",
       "      <td>25618479.0</td>\n",
       "      <td>1202900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-15205.0</td>\n",
       "      <td>-540596.0</td>\n",
       "      <td>21.799999</td>\n",
       "      <td>-4365755.0</td>\n",
       "      <td>-709018.0</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>1446877.0</td>\n",
       "      <td>-0.699997</td>\n",
       "      <td>10817.0</td>\n",
       "      <td>10761.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>84986065.0</td>\n",
       "      <td>42491261.0</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>2982742.0</td>\n",
       "      <td>19486193.0</td>\n",
       "      <td>-2978512.0</td>\n",
       "      <td>7305693.0</td>\n",
       "      <td>-9263453.0</td>\n",
       "      <td>19846882.0</td>\n",
       "      <td>-5771597.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-77704.0</td>\n",
       "      <td>378809.0</td>\n",
       "      <td>22.299999</td>\n",
       "      <td>1882171.0</td>\n",
       "      <td>299492.0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>865377.0</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>10761.0</td>\n",
       "      <td>10868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>51250214.0</td>\n",
       "      <td>-33735851.0</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>4556264.0</td>\n",
       "      <td>-3250826.0</td>\n",
       "      <td>6403229.0</td>\n",
       "      <td>21735909.0</td>\n",
       "      <td>14430216.0</td>\n",
       "      <td>27628213.0</td>\n",
       "      <td>7781331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-30284.0</td>\n",
       "      <td>6033500.0</td>\n",
       "      <td>22.549999</td>\n",
       "      <td>47814.0</td>\n",
       "      <td>-217960.0</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>1225550.0</td>\n",
       "      <td>1.400001</td>\n",
       "      <td>10868.0</td>\n",
       "      <td>10887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>32677994.0</td>\n",
       "      <td>-18572220.0</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>-8196303.0</td>\n",
       "      <td>-3581597.0</td>\n",
       "      <td>-954862.0</td>\n",
       "      <td>16158614.0</td>\n",
       "      <td>-5577295.0</td>\n",
       "      <td>19620699.0</td>\n",
       "      <td>-8007514.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>2998873.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>-2188179.0</td>\n",
       "      <td>-340457.0</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>705192.0</td>\n",
       "      <td>1.599999</td>\n",
       "      <td>10887.0</td>\n",
       "      <td>10953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>30997468.0</td>\n",
       "      <td>-1680526.0</td>\n",
       "      <td>91.300003</td>\n",
       "      <td>610034.0</td>\n",
       "      <td>-9409831.0</td>\n",
       "      <td>-735660.0</td>\n",
       "      <td>13284024.0</td>\n",
       "      <td>-2874590.0</td>\n",
       "      <td>13258636.0</td>\n",
       "      <td>-6362063.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>-3732386.0</td>\n",
       "      <td>21.950001</td>\n",
       "      <td>-210225.0</td>\n",
       "      <td>143798.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>918282.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>10953.0</td>\n",
       "      <td>10744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>40192923.0</td>\n",
       "      <td>9195455.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1129782.0</td>\n",
       "      <td>6393288.0</td>\n",
       "      <td>1300858.0</td>\n",
       "      <td>15423030.0</td>\n",
       "      <td>2139006.0</td>\n",
       "      <td>20233906.0</td>\n",
       "      <td>6975270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23095.0</td>\n",
       "      <td>-3256658.0</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>4901530.0</td>\n",
       "      <td>54557.0</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>861539.0</td>\n",
       "      <td>-1.699997</td>\n",
       "      <td>10744.0</td>\n",
       "      <td>10821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>26803223.0</td>\n",
       "      <td>-13389700.0</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>-1553424.0</td>\n",
       "      <td>-4083282.0</td>\n",
       "      <td>-6538020.0</td>\n",
       "      <td>8215465.0</td>\n",
       "      <td>-7207565.0</td>\n",
       "      <td>9292308.0</td>\n",
       "      <td>-10941598.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-58045.0</td>\n",
       "      <td>-958657.0</td>\n",
       "      <td>23.299999</td>\n",
       "      <td>6100062.0</td>\n",
       "      <td>-53596.0</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>729917.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>10821.0</td>\n",
       "      <td>10984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>33856481.0</td>\n",
       "      <td>7053258.0</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>-923213.0</td>\n",
       "      <td>-4633483.0</td>\n",
       "      <td>19038280.0</td>\n",
       "      <td>18596544.0</td>\n",
       "      <td>10381079.0</td>\n",
       "      <td>16443468.0</td>\n",
       "      <td>7151160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200231.0</td>\n",
       "      <td>-493292.0</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>-1882303.0</td>\n",
       "      <td>758696.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>531153.0</td>\n",
       "      <td>0.699997</td>\n",
       "      <td>10984.0</td>\n",
       "      <td>10973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>23375959.0</td>\n",
       "      <td>-10480522.0</td>\n",
       "      <td>88.400002</td>\n",
       "      <td>-410135.0</td>\n",
       "      <td>1464971.0</td>\n",
       "      <td>-14416957.0</td>\n",
       "      <td>10481595.0</td>\n",
       "      <td>-8114949.0</td>\n",
       "      <td>10073700.0</td>\n",
       "      <td>-6369768.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-206236.0</td>\n",
       "      <td>2063819.0</td>\n",
       "      <td>22.700001</td>\n",
       "      <td>-6142110.0</td>\n",
       "      <td>7137816.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1236250.0</td>\n",
       "      <td>-0.599999</td>\n",
       "      <td>10973.0</td>\n",
       "      <td>10911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>28444568.0</td>\n",
       "      <td>5068609.0</td>\n",
       "      <td>87.699997</td>\n",
       "      <td>1900908.0</td>\n",
       "      <td>2819305.0</td>\n",
       "      <td>7746688.0</td>\n",
       "      <td>11204676.0</td>\n",
       "      <td>723081.0</td>\n",
       "      <td>16542358.0</td>\n",
       "      <td>6468658.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94055.0</td>\n",
       "      <td>-3052452.0</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>-597706.0</td>\n",
       "      <td>9687802.0</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>692580.0</td>\n",
       "      <td>-0.599998</td>\n",
       "      <td>10911.0</td>\n",
       "      <td>10919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>79774181.0</td>\n",
       "      <td>51329613.0</td>\n",
       "      <td>87.199997</td>\n",
       "      <td>6906148.0</td>\n",
       "      <td>-1431352.0</td>\n",
       "      <td>11438113.0</td>\n",
       "      <td>24010130.0</td>\n",
       "      <td>12805454.0</td>\n",
       "      <td>29416131.0</td>\n",
       "      <td>12873773.0</td>\n",
       "      <td>...</td>\n",
       "      <td>133940.0</td>\n",
       "      <td>329450.0</td>\n",
       "      <td>21.549999</td>\n",
       "      <td>-1898649.0</td>\n",
       "      <td>-16545139.0</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>446636.0</td>\n",
       "      <td>-0.099998</td>\n",
       "      <td>10919.0</td>\n",
       "      <td>11057.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>35811214.0</td>\n",
       "      <td>-43962967.0</td>\n",
       "      <td>88.800003</td>\n",
       "      <td>-5981360.0</td>\n",
       "      <td>-814917.0</td>\n",
       "      <td>-15051555.0</td>\n",
       "      <td>10918155.0</td>\n",
       "      <td>-13091975.0</td>\n",
       "      <td>18193311.0</td>\n",
       "      <td>-11222820.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-87970.0</td>\n",
       "      <td>-705688.0</td>\n",
       "      <td>21.799999</td>\n",
       "      <td>-850970.0</td>\n",
       "      <td>-748720.0</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>657514.0</td>\n",
       "      <td>1.199996</td>\n",
       "      <td>11057.0</td>\n",
       "      <td>11107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>24771587.0</td>\n",
       "      <td>-11039627.0</td>\n",
       "      <td>89.300003</td>\n",
       "      <td>-2315187.0</td>\n",
       "      <td>8318479.0</td>\n",
       "      <td>1438912.0</td>\n",
       "      <td>9343354.0</td>\n",
       "      <td>-1574801.0</td>\n",
       "      <td>10687719.0</td>\n",
       "      <td>-7505592.0</td>\n",
       "      <td>...</td>\n",
       "      <td>386960.0</td>\n",
       "      <td>-285020.0</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>349285.0</td>\n",
       "      <td>-244575.0</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>883440.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>11107.0</td>\n",
       "      <td>11283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>65176337.0</td>\n",
       "      <td>40404750.0</td>\n",
       "      <td>91.599998</td>\n",
       "      <td>6574907.0</td>\n",
       "      <td>-7604900.0</td>\n",
       "      <td>22757168.0</td>\n",
       "      <td>14849561.0</td>\n",
       "      <td>5506207.0</td>\n",
       "      <td>25684443.0</td>\n",
       "      <td>14996724.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-225980.0</td>\n",
       "      <td>1369857.0</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>-534806.0</td>\n",
       "      <td>19251.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1173000.0</td>\n",
       "      <td>1.400002</td>\n",
       "      <td>11283.0</td>\n",
       "      <td>11358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>46572451.0</td>\n",
       "      <td>-18603886.0</td>\n",
       "      <td>92.599998</td>\n",
       "      <td>-3945420.0</td>\n",
       "      <td>-1258655.0</td>\n",
       "      <td>-12453833.0</td>\n",
       "      <td>11540589.0</td>\n",
       "      <td>-3308972.0</td>\n",
       "      <td>19316722.0</td>\n",
       "      <td>-6367721.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-161019.0</td>\n",
       "      <td>1900015.0</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>-6083.0</td>\n",
       "      <td>93154.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1369549.0</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>11358.0</td>\n",
       "      <td>11457.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Volume_2330  成交量變動_2330    Low_6505  成交量變動_6505  成交量變動_2454  成交量變動_2882  \\\n",
       "0     24369000.0   1739000.0  108.000000   1288000.0    -71000.0   3499925.0   \n",
       "1     20979000.0  -3390000.0  108.000000   -309000.0    315000.0   5123312.0   \n",
       "2     22443000.0   1464000.0  108.500000  -1788000.0    599000.0   1482747.0   \n",
       "3     18569000.0  -3874000.0  109.000000    244000.0  -1805000.0  -1105028.0   \n",
       "4     20198000.0   1629000.0  107.500000   1599000.0   1513000.0   -684113.0   \n",
       "5     29107000.0   8909000.0  107.500000  -1410000.0     13000.0   2134713.0   \n",
       "6     41130000.0  12023000.0  108.000000    646000.0  -1253000.0   1938822.0   \n",
       "7     52352000.0  11222000.0  108.000000  -1281000.0   1009000.0  -2220101.0   \n",
       "8     30756000.0 -21596000.0  104.000000   2859000.0   1114000.0   5643679.0   \n",
       "9     13159000.0 -17597000.0  104.000000  -1569000.0  -1607000.0  -4130796.0   \n",
       "10    23693000.0  10534000.0  103.500000   1594000.0   1454000.0   3726959.0   \n",
       "11    24627000.0    934000.0  103.000000   -753000.0   -861000.0  -3746045.0   \n",
       "12    23429000.0  -1198000.0  103.000000   -456000.0   -162000.0   -751419.0   \n",
       "13    36992000.0  13563000.0  105.000000    209000.0     25000.0   4606962.0   \n",
       "14    55022000.0  18030000.0  105.000000  -1070000.0   2085000.0  -3062937.0   \n",
       "15    94089000.0  39067000.0  103.500000   3164000.0  11689000.0   9932194.0   \n",
       "16    29518000.0 -64571000.0  103.500000  -2751000.0  -9797000.0 -10086898.0   \n",
       "17    24779000.0  -4739000.0  103.500000     23000.0   1419000.0  19456531.0   \n",
       "18    23616000.0  -1163000.0  104.000000    525000.0   -278000.0 -19183287.0   \n",
       "19    29168000.0   5552000.0  103.000000    621000.0   -314000.0   1952886.0   \n",
       "20    18967000.0 -10201000.0  103.500000   -686000.0  -3649000.0  -7378574.0   \n",
       "21    34747000.0  15780000.0  104.500000    927000.0   1514000.0  30070828.0   \n",
       "22    25124000.0  -9623000.0  106.500000   -197000.0   -806000.0 -30077860.0   \n",
       "23    45696000.0  20572000.0  106.500000    935000.0   -945000.0  11260236.0   \n",
       "24    39347000.0  -6349000.0  107.000000   -586000.0   1202000.0  -4408058.0   \n",
       "25    27018000.0 -12329000.0  108.500000   2180000.0   -569000.0  10401328.0   \n",
       "26    20389000.0  -6629000.0  107.000000  -4189000.0  10452000.0 -12910746.0   \n",
       "27    31159000.0  17905000.0  107.000000   -342000.0   2312000.0   2588779.0   \n",
       "28    25677000.0  -5482000.0  108.000000   2629000.0  -2417000.0   4089609.0   \n",
       "29    35725000.0  10048000.0  108.500000  -2219000.0   1221000.0  -2288413.0   \n",
       "..           ...         ...         ...         ...         ...         ...   \n",
       "796   39474478.0  -2718102.0   79.199997    174239.0  -3273360.0  -2841784.0   \n",
       "797   26008090.0 -13466388.0   79.599998   -529700.0  -2765544.0  -3701294.0   \n",
       "798   38493406.0  12485316.0   80.599998   2221204.0   7169519.0   7947650.0   \n",
       "799   37395165.0  -1098241.0   81.199997  -1994448.0  -2599134.0  -3206657.0   \n",
       "800   44059301.0   6664136.0   81.500000   4097664.0  20912123.0  10628045.0   \n",
       "801   55126085.0  11066784.0   83.900002   4626133.0  -9750296.0  20338182.0   \n",
       "802   71581861.0  16455776.0   85.000000  -4280802.0 -10009376.0 -12057753.0   \n",
       "803   23547405.0 -48034456.0   86.000000  -2030668.0  -4460404.0 -19005400.0   \n",
       "804   34240479.0  10693074.0   84.500000    619986.0   1265873.0   1982001.0   \n",
       "805   27621198.0  -6619281.0   84.099998    755245.0   -469542.0  -1486916.0   \n",
       "806   32295893.0   4674695.0   85.800003   2314378.0  -1506266.0   -301090.0   \n",
       "807   28727019.0  -3568874.0   88.800003  -3334379.0    -97024.0   2603783.0   \n",
       "808   52089788.0  23362769.0   86.000000    739251.0  -1387899.0  -1457557.0   \n",
       "809   26156418.0 -25933370.0   85.099998  -2322926.0   5421699.0  -2997459.0   \n",
       "810   34315948.0   8159530.0   85.900002    858958.0  -2210652.0   1280664.0   \n",
       "811   42494804.0   8178856.0   86.400002   -189618.0  -2540291.0    227466.0   \n",
       "812   84986065.0  42491261.0   87.000000   2982742.0  19486193.0  -2978512.0   \n",
       "813   51250214.0 -33735851.0   91.500000   4556264.0  -3250826.0   6403229.0   \n",
       "814   32677994.0 -18572220.0   91.000000  -8196303.0  -3581597.0   -954862.0   \n",
       "815   30997468.0  -1680526.0   91.300003    610034.0  -9409831.0   -735660.0   \n",
       "816   40192923.0   9195455.0   90.000000   1129782.0   6393288.0   1300858.0   \n",
       "817   26803223.0 -13389700.0   88.000000  -1553424.0  -4083282.0  -6538020.0   \n",
       "818   33856481.0   7053258.0   89.000000   -923213.0  -4633483.0  19038280.0   \n",
       "819   23375959.0 -10480522.0   88.400002   -410135.0   1464971.0 -14416957.0   \n",
       "820   28444568.0   5068609.0   87.699997   1900908.0   2819305.0   7746688.0   \n",
       "821   79774181.0  51329613.0   87.199997   6906148.0  -1431352.0  11438113.0   \n",
       "822   35811214.0 -43962967.0   88.800003  -5981360.0   -814917.0 -15051555.0   \n",
       "823   24771587.0 -11039627.0   89.300003  -2315187.0   8318479.0   1438912.0   \n",
       "824   65176337.0  40404750.0   91.599998   6574907.0  -7604900.0  22757168.0   \n",
       "825   46572451.0 -18603886.0   92.599998  -3945420.0  -1258655.0 -12453833.0   \n",
       "\n",
       "     Volume_2881  成交量變動_2881  Volume_2886  成交量變動_2886  ...  成交量變動_3617  \\\n",
       "0     10406000.0   5136000.0   13865000.0   4862000.0  ...     41000.0   \n",
       "1     10166000.0   -240000.0   17662000.0   3797000.0  ...      9000.0   \n",
       "2      6993000.0  -3173000.0   11226000.0  -6436000.0  ...    -47000.0   \n",
       "3     13755000.0   6762000.0    9055000.0  -2171000.0  ...    142000.0   \n",
       "4     10076000.0  -3679000.0   13438000.0   4383000.0  ...    118000.0   \n",
       "5      9462000.0   -614000.0   13908000.0    470000.0  ...    -75000.0   \n",
       "6     15440000.0   5978000.0   16765000.0   2857000.0  ...    -94000.0   \n",
       "7     12490000.0  -2950000.0   11565000.0  -5200000.0  ...    361000.0   \n",
       "8     11326000.0  -1164000.0   10179000.0  -1386000.0  ...   -384000.0   \n",
       "9      8636000.0  -2690000.0   13976000.0   3797000.0  ...    131000.0   \n",
       "10    15337000.0   6701000.0   15018000.0   1042000.0  ...   -143000.0   \n",
       "11     9835000.0  -5502000.0   14378000.0   -640000.0  ...     81000.0   \n",
       "12    10684000.0    849000.0   14870000.0    492000.0  ...    -90000.0   \n",
       "13    11111000.0    427000.0   14375000.0   -495000.0  ...     23000.0   \n",
       "14     9815000.0  -1296000.0   16408000.0   2033000.0  ...    -52000.0   \n",
       "15    23721000.0  13906000.0   23315000.0   6907000.0  ...     19000.0   \n",
       "16    26314000.0   2593000.0   19274000.0  -4041000.0  ...     54000.0   \n",
       "17    23328000.0  -2986000.0   18709000.0   -565000.0  ...    -83000.0   \n",
       "18     9368000.0 -13960000.0   13725000.0  -4984000.0  ...     68000.0   \n",
       "19    11629000.0   2261000.0   14079000.0    354000.0  ...     15000.0   \n",
       "20    11403000.0   -226000.0   16335000.0   2256000.0  ...    -13000.0   \n",
       "21    25120000.0  13717000.0   21715000.0   5380000.0  ...    -32000.0   \n",
       "22    12204000.0 -12916000.0    9382000.0 -12333000.0  ...    279000.0   \n",
       "23    15711000.0   3507000.0   12934000.0   3552000.0  ...     11000.0   \n",
       "24    19813000.0   4102000.0   21066000.0   8132000.0  ...   -158000.0   \n",
       "25    15390000.0  -4423000.0   24554000.0   3488000.0  ...   -101000.0   \n",
       "26    12569000.0  -2821000.0   16627000.0  -7927000.0  ...     46000.0   \n",
       "27    14365000.0   3315000.0   14429000.0   -416000.0  ...   -103000.0   \n",
       "28    12082000.0  -2283000.0   15386000.0    957000.0  ...    -45000.0   \n",
       "29    23171000.0  11089000.0   14408000.0   -978000.0  ...     75000.0   \n",
       "..           ...         ...          ...         ...  ...         ...   \n",
       "796   10879769.0  -3475061.0   25997195.0   2556082.0  ...      1015.0   \n",
       "797    9989353.0   -890416.0   24180472.0  -1816723.0  ...    -37015.0   \n",
       "798   11851531.0   1862178.0   22145272.0  -2035200.0  ...     75103.0   \n",
       "799    7655849.0  -4195682.0   27855091.0   5709819.0  ...     23897.0   \n",
       "800   15274925.0   7619076.0   37487108.0   9632017.0  ...     92000.0   \n",
       "801   20246120.0   4971195.0   38706740.0   1219632.0  ...    103049.0   \n",
       "802   24649105.0   4402985.0   36012972.0  -2693768.0  ...   -114041.0   \n",
       "803    8102348.0 -16546757.0   20851452.0 -15161520.0  ...    -95808.0   \n",
       "804    8991553.0    889205.0   24088407.0   3236955.0  ...    -34167.0   \n",
       "805   10433712.0   1442159.0   24250749.0    162342.0  ...    105104.0   \n",
       "806    9626405.0   -807307.0   18787381.0  -5463368.0  ...    -47777.0   \n",
       "807   10618884.0    992479.0   20909260.0   2121879.0  ...     -5350.0   \n",
       "808   14820252.0   4201368.0   19928977.0   -980283.0  ...    -60912.0   \n",
       "809    9772855.0  -5047397.0   16241655.0  -3687322.0  ...    -35098.0   \n",
       "810   12588440.0   2815585.0   24415579.0   8173924.0  ...    136193.0   \n",
       "811   16569146.0   3980706.0   25618479.0   1202900.0  ...    -15205.0   \n",
       "812    7305693.0  -9263453.0   19846882.0  -5771597.0  ...    -77704.0   \n",
       "813   21735909.0  14430216.0   27628213.0   7781331.0  ...    -30284.0   \n",
       "814   16158614.0  -5577295.0   19620699.0  -8007514.0  ...     35000.0   \n",
       "815   13284024.0  -2874590.0   13258636.0  -6362063.0  ...     10000.0   \n",
       "816   15423030.0   2139006.0   20233906.0   6975270.0  ...     23095.0   \n",
       "817    8215465.0  -7207565.0    9292308.0 -10941598.0  ...    -58045.0   \n",
       "818   18596544.0  10381079.0   16443468.0   7151160.0  ...    200231.0   \n",
       "819   10481595.0  -8114949.0   10073700.0  -6369768.0  ...   -206236.0   \n",
       "820   11204676.0    723081.0   16542358.0   6468658.0  ...     94055.0   \n",
       "821   24010130.0  12805454.0   29416131.0  12873773.0  ...    133940.0   \n",
       "822   10918155.0 -13091975.0   18193311.0 -11222820.0  ...    -87970.0   \n",
       "823    9343354.0  -1574801.0   10687719.0  -7505592.0  ...    386960.0   \n",
       "824   14849561.0   5506207.0   25684443.0  14996724.0  ...   -225980.0   \n",
       "825   11540589.0  -3308972.0   19316722.0  -6367721.0  ...   -161019.0   \n",
       "\n",
       "     成交量變動_8996  High_1701  成交量變動_1701  成交量變動_1709  價格漲幅_2010  Volume_3530  \\\n",
       "0      271000.0  18.250000     38000.0   -232638.0   0.042661      17201.0   \n",
       "1     -196000.0  18.250000     64000.0    360786.0   0.000000       5400.0   \n",
       "2      -41000.0  18.200001    -13000.0   -325299.0   0.000000       4000.0   \n",
       "3      -34000.0  18.250000    -71000.0    235596.0   0.085321      58260.0   \n",
       "4       60000.0  18.250000     55000.0   -103505.0  -0.085321      13100.0   \n",
       "5      -84000.0  18.100000    -58000.0    376559.0   0.085321      56230.0   \n",
       "6      -27000.0  18.100000     17000.0   -327271.0   0.085319       4050.0   \n",
       "7       17000.0  18.100000    -21000.0    -58159.0  -0.042661       6610.0   \n",
       "8       -2000.0  18.100000     69000.0     29572.0   0.000000      19000.0   \n",
       "9      -35000.0  18.049999     37000.0   -105476.0   0.042661      10000.0   \n",
       "10      86000.0  18.200001     52000.0   -140963.0   0.000000      19000.0   \n",
       "11      65000.0  18.200001   -169000.0    338114.0   0.000000      11000.0   \n",
       "12     -35000.0  18.150000    123000.0    625955.0   0.000000       5000.0   \n",
       "13      61000.0  18.200001     35000.0   -399231.0   0.000000      27500.0   \n",
       "14     -76000.0  18.150000   -125000.0    -48302.0   0.042660      28000.0   \n",
       "15     -62000.0  18.350000    160000.0    163636.0   0.298620      60000.0   \n",
       "16      90000.0  18.350000     32000.0    412045.0  -0.127980      11700.0   \n",
       "17     -51000.0  18.500000    292000.0    386416.0   0.000000      25000.0   \n",
       "18     137000.0  18.600000   -137000.0   3316081.0   0.042660      15600.0   \n",
       "19     -91000.0  18.600000   -106000.0  -2780816.0  -0.042660       8000.0   \n",
       "20      36000.0  18.549999    -92000.0   -562866.0   0.042660       7100.0   \n",
       "21      64000.0  19.350000   3054000.0    302627.0   0.000000      20000.0   \n",
       "22    -107000.0  19.299999  -2148000.0   -699886.0   0.000000      27000.0   \n",
       "23     135000.0  19.350000   -345000.0   -403174.0   0.042660      11200.0   \n",
       "24     -88000.0  19.100000   -250000.0    459362.0   0.170640      11000.0   \n",
       "25     -66000.0  20.400000   4422000.0      -986.0  -0.042660      11000.0   \n",
       "26     -15000.0  20.700001  -1022000.0   6679477.0   0.000000      26500.0   \n",
       "27    -212000.0  20.450001  -2254000.0    505692.0   0.042660      28200.0   \n",
       "28      82000.0  20.200001    -61000.0   -185322.0  -0.085320      85000.0   \n",
       "29      17000.0  20.150000   1393000.0   -195179.0  -0.170640      18500.0   \n",
       "..          ...        ...         ...         ...        ...          ...   \n",
       "796   -713501.0  20.700001    332459.0    -52411.0   0.100000    1063364.0   \n",
       "797   -755321.0  20.049999   2293613.0    168423.0   0.000000    1088381.0   \n",
       "798   2528610.0  20.400000  -2119458.0     88385.0   0.070000    1790928.0   \n",
       "799  -1005356.0  20.700001   -377415.0   -202085.0   0.060000    1740985.0   \n",
       "800  -1131878.0  20.400000   -242639.0     77383.0   0.110000    4387623.0   \n",
       "801   1839018.0  22.100000   7532590.0    158692.0   0.110000    1963770.0   \n",
       "802  -2319323.0  22.450001   3496598.0   -257002.0  -0.160000    1270986.0   \n",
       "803   4212327.0  23.750000   4618223.0     46877.0   0.110000    2934573.0   \n",
       "804  -4178984.0  24.250000  11242138.0   1663813.0  -0.040000    4405813.0   \n",
       "805   1527733.0  23.400000 -20978664.0  -1170081.0   0.010000    1580210.0   \n",
       "806    808219.0  22.900000    278551.0   -209386.0  -0.030000    1119000.0   \n",
       "807  -1040125.0  21.850000  -3816129.0     29001.0   0.050000    1047210.0   \n",
       "808   1939738.0  22.000000    339007.0   -564607.0  -0.090000     997200.0   \n",
       "809   1161665.0  22.200001   1343045.0      8979.0   0.000000     769713.0   \n",
       "810  -2800755.0  22.549999    777941.0   1102664.0  -0.010000    1898735.0   \n",
       "811   -540596.0  21.799999  -4365755.0   -709018.0  -0.010000    1446877.0   \n",
       "812    378809.0  22.299999   1882171.0    299492.0   0.080000     865377.0   \n",
       "813   6033500.0  22.549999     47814.0   -217960.0  -0.010000    1225550.0   \n",
       "814   2998873.0  22.000000  -2188179.0   -340457.0  -0.030000     705192.0   \n",
       "815  -3732386.0  21.950001   -210225.0    143798.0   0.050000     918282.0   \n",
       "816  -3256658.0  22.500000   4901530.0     54557.0  -0.070000     861539.0   \n",
       "817   -958657.0  23.299999   6100062.0    -53596.0   0.070000     729917.0   \n",
       "818   -493292.0  23.900000  -1882303.0    758696.0   0.050000     531153.0   \n",
       "819   2063819.0  22.700001  -6142110.0   7137816.0   0.050000    1236250.0   \n",
       "820  -3052452.0  22.150000   -597706.0   9687802.0   0.130000     692580.0   \n",
       "821    329450.0  21.549999  -1898649.0 -16545139.0   0.130000     446636.0   \n",
       "822   -705688.0  21.799999   -850970.0   -748720.0   0.060000     657514.0   \n",
       "823   -285020.0  21.600000    349285.0   -244575.0  -0.060000     883440.0   \n",
       "824   1369857.0  21.600000   -534806.0     19251.0   0.040000    1173000.0   \n",
       "825   1900015.0  21.700001     -6083.0     93154.0   0.000000    1369549.0   \n",
       "\n",
       "     價格漲幅_3454      收盤價   Target  \n",
       "0    -0.244285   9271.0   9342.0  \n",
       "1     0.000000   9342.0   9365.0  \n",
       "2    -1.221344   9365.0   9342.0  \n",
       "3     0.895691   9342.0   9341.0  \n",
       "4    -0.162827   9341.0   9337.0  \n",
       "5    -0.651405   9337.0   9411.0  \n",
       "6     0.000000   9411.0   9375.0  \n",
       "7    -0.407112   9375.0   9285.0  \n",
       "8    -0.977074   9285.0   9340.0  \n",
       "9     0.569954   9340.0   9340.0  \n",
       "10    0.081367   9340.0   9304.0  \n",
       "11   -0.162826   9304.0   9330.0  \n",
       "12    0.244285   9330.0   9407.0  \n",
       "13    0.000000   9407.0   9442.0  \n",
       "14   -0.081459   9442.0   9375.0  \n",
       "15   -0.081367   9375.0   9437.0  \n",
       "16    0.000000   9437.0   9505.0  \n",
       "17    0.081367   9505.0   9520.0  \n",
       "18    1.628472   9520.0   9515.0  \n",
       "19   -0.651322   9515.0   9574.0  \n",
       "20   -0.325744   9574.0   9657.0  \n",
       "21    0.407112   9657.0   9704.0  \n",
       "22    0.081459   9704.0   9708.0  \n",
       "23   -0.570030   9708.0   9799.0  \n",
       "24    0.651405   9799.0   9770.0  \n",
       "25   -0.732780   9770.0   9763.0  \n",
       "26    0.732780   9763.0   9786.0  \n",
       "27    0.244194   9767.0   9783.0  \n",
       "28    2.279869   9783.0   9773.0  \n",
       "29    0.407120   9773.0   9760.0  \n",
       "..         ...      ...      ...  \n",
       "796   0.700005  10292.0  10268.0  \n",
       "797  -0.599999  10268.0  10545.0  \n",
       "798   2.000000  10545.0  10583.0  \n",
       "799   1.299995  10583.0  10709.0  \n",
       "800   0.300004  10709.0  10947.0  \n",
       "801   1.699996  10947.0  10610.0  \n",
       "802  -1.900001  10610.0  10719.0  \n",
       "803   0.800003  10719.0  10749.0  \n",
       "804  -1.000000  10749.0  10833.0  \n",
       "805   1.500000  10833.0  10894.0  \n",
       "806  -0.699997  10894.0  10982.0  \n",
       "807   1.000000  10982.0  10865.0  \n",
       "808  -0.900001  10865.0  10914.0  \n",
       "809  -0.800004  10914.0  10749.0  \n",
       "810  -1.900001  10749.0  10817.0  \n",
       "811  -0.699997  10817.0  10761.0  \n",
       "812   0.199997  10761.0  10868.0  \n",
       "813   1.400001  10868.0  10887.0  \n",
       "814   1.599999  10887.0  10953.0  \n",
       "815   1.500000  10953.0  10744.0  \n",
       "816  -1.699997  10744.0  10821.0  \n",
       "817   1.500000  10821.0  10984.0  \n",
       "818   0.699997  10984.0  10973.0  \n",
       "819  -0.599999  10973.0  10911.0  \n",
       "820  -0.599998  10911.0  10919.0  \n",
       "821  -0.099998  10919.0  11057.0  \n",
       "822   1.199996  11057.0  11107.0  \n",
       "823  -0.500000  11107.0  11283.0  \n",
       "824   1.400002  11283.0  11358.0  \n",
       "825   0.099998  11358.0  11457.0  \n",
       "\n",
       "[826 rows x 221 columns]"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feature= Significant_factor_test\n",
    "all_feature = all_feature.drop(columns=[\"Date\"])\n",
    "all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Target\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (712, 30, 221)\n",
      "Y_train (712, 5)\n",
      "X_val (79, 30, 221)\n",
      "Y_val (79, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(all_feature, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"Y_train\",Y_train.shape)\n",
    "print(\"X_val\",X_val.shape)\n",
    "print(\"Y_val\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(250, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    model.add(LSTM(100,return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(742, 1, 1)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(250, return_sequences=True, input_shape=(1, 221))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 1, 250)            472000    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 1, 100)            140400    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 1, 1)              101       \n",
      "=================================================================\n",
      "Total params: 612,501\n",
      "Trainable params: 612,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 742 samples, validate on 82 samples\n",
      "Epoch 1/3000\n",
      "742/742 [==============================] - 1s 1ms/step - loss: 10568.5608 - val_loss: 10539.7627\n",
      "Epoch 2/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10568.0194 - val_loss: 10539.1650\n",
      "Epoch 3/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10567.3771 - val_loss: 10538.4375\n",
      "Epoch 4/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10566.6139 - val_loss: 10537.5713\n",
      "Epoch 5/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10565.7144 - val_loss: 10536.5723\n",
      "Epoch 6/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10564.6972 - val_loss: 10535.4736\n",
      "Epoch 7/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 10563.6034 - val_loss: 10534.3047\n",
      "Epoch 8/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10562.4603 - val_loss: 10533.1055\n",
      "Epoch 9/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10561.2984 - val_loss: 10531.9121\n",
      "Epoch 10/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10560.1476 - val_loss: 10530.7393\n",
      "Epoch 11/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10559.0261 - val_loss: 10529.6133\n",
      "Epoch 12/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10557.9469 - val_loss: 10528.5410\n",
      "Epoch 13/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 10556.9178 - val_loss: 10527.5195\n",
      "Epoch 14/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10555.9357 - val_loss: 10526.5508\n",
      "Epoch 15/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10554.9961 - val_loss: 10525.6299\n",
      "Epoch 16/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10554.0997 - val_loss: 10524.7500\n",
      "Epoch 17/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10553.2389 - val_loss: 10523.9072\n",
      "Epoch 18/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10552.4087 - val_loss: 10523.0967\n",
      "Epoch 19/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10551.6087 - val_loss: 10522.3125\n",
      "Epoch 20/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10550.8306 - val_loss: 10521.5527\n",
      "Epoch 21/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10550.0740 - val_loss: 10520.8145\n",
      "Epoch 22/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10549.3377 - val_loss: 10520.0947\n",
      "Epoch 23/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10548.6167 - val_loss: 10519.3906\n",
      "Epoch 24/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 10547.9116 - val_loss: 10518.7012\n",
      "Epoch 25/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10547.2197 - val_loss: 10518.0254\n",
      "Epoch 26/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10546.5411 - val_loss: 10517.3613\n",
      "Epoch 27/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10545.8714 - val_loss: 10516.7080\n",
      "Epoch 28/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10545.2132 - val_loss: 10516.0645\n",
      "Epoch 29/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10544.5648 - val_loss: 10515.4287\n",
      "Epoch 30/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10543.9250 - val_loss: 10514.8008\n",
      "Epoch 31/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10543.2937 - val_loss: 10514.1807\n",
      "Epoch 32/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10542.6694 - val_loss: 10513.5674\n",
      "Epoch 33/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10542.0510 - val_loss: 10512.9619\n",
      "Epoch 34/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10541.4405 - val_loss: 10512.3594\n",
      "Epoch 35/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10540.8360 - val_loss: 10511.7637\n",
      "Epoch 36/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10540.2355 - val_loss: 10511.1738\n",
      "Epoch 37/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10539.6415 - val_loss: 10510.5879\n",
      "Epoch 38/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 10539.0513 - val_loss: 10510.0049\n",
      "Epoch 39/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 10538.4664 - val_loss: 10509.4277\n",
      "Epoch 40/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10537.8858 - val_loss: 10508.8535\n",
      "Epoch 41/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10537.3091 - val_loss: 10508.2832\n",
      "Epoch 42/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10536.7356 - val_loss: 10507.7168\n",
      "Epoch 43/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10536.1651 - val_loss: 10507.1533\n",
      "Epoch 44/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10535.5994 - val_loss: 10506.5938\n",
      "Epoch 45/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10535.0364 - val_loss: 10506.0361\n",
      "Epoch 46/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10534.4763 - val_loss: 10505.4805\n",
      "Epoch 47/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10533.9183 - val_loss: 10504.9287\n",
      "Epoch 48/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10533.3638 - val_loss: 10504.3789\n",
      "Epoch 49/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10532.8119 - val_loss: 10503.8311\n",
      "Epoch 50/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 10532.2614 - val_loss: 10503.2861\n",
      "Epoch 51/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10531.7140 - val_loss: 10502.7432\n",
      "Epoch 52/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10531.1693 - val_loss: 10502.2021\n",
      "Epoch 53/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10530.6259 - val_loss: 10501.6631\n",
      "Epoch 54/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10530.0839 - val_loss: 10501.1250\n",
      "Epoch 55/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10529.5441 - val_loss: 10500.5889\n",
      "Epoch 56/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10529.0061 - val_loss: 10500.0557\n",
      "Epoch 57/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 10528.4702 - val_loss: 10499.5234\n",
      "Epoch 58/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 10527.9350 - val_loss: 10498.9922\n",
      "Epoch 59/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 10527.4024 - val_loss: 10498.4619\n",
      "Epoch 60/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10526.8711 - val_loss: 10497.9346\n",
      "Epoch 61/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10526.3413 - val_loss: 10497.4082\n",
      "Epoch 62/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10525.8129 - val_loss: 10496.8828\n",
      "Epoch 63/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10525.2861 - val_loss: 10496.3584\n",
      "Epoch 64/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10524.7606 - val_loss: 10495.8359\n",
      "Epoch 65/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10524.2364 - val_loss: 10495.3145\n",
      "Epoch 66/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10523.7133 - val_loss: 10494.7949\n",
      "Epoch 67/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10523.1917 - val_loss: 10494.2764\n",
      "Epoch 68/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10522.6711 - val_loss: 10493.7568\n",
      "Epoch 69/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10522.1518 - val_loss: 10493.2393\n",
      "Epoch 70/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10521.6331 - val_loss: 10492.7236\n",
      "Epoch 71/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10521.1158 - val_loss: 10492.2090\n",
      "Epoch 72/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10520.5999 - val_loss: 10491.6953\n",
      "Epoch 73/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10520.0845 - val_loss: 10491.1826\n",
      "Epoch 74/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10519.5699 - val_loss: 10490.6699\n",
      "Epoch 75/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10519.0564 - val_loss: 10490.1592\n",
      "Epoch 76/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10518.5442 - val_loss: 10489.6484\n",
      "Epoch 77/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 10518.0328 - val_loss: 10489.1387\n",
      "Epoch 78/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 10517.5212 - val_loss: 10488.6299\n",
      "Epoch 79/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10517.0116 - val_loss: 10488.1221\n",
      "Epoch 80/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10516.5028 - val_loss: 10487.6152\n",
      "Epoch 81/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10515.9943 - val_loss: 10487.1084\n",
      "Epoch 82/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10515.4868 - val_loss: 10486.6025\n",
      "Epoch 83/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10514.9796 - val_loss: 10486.0977\n",
      "Epoch 84/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10514.4733 - val_loss: 10485.5928\n",
      "Epoch 85/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10513.9682 - val_loss: 10485.0889\n",
      "Epoch 86/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 10513.4631 - val_loss: 10484.5859\n",
      "Epoch 87/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10512.9591 - val_loss: 10484.0840\n",
      "Epoch 88/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10512.4557 - val_loss: 10483.5820\n",
      "Epoch 89/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10511.9529 - val_loss: 10483.0811\n",
      "Epoch 90/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10511.4507 - val_loss: 10482.5791\n",
      "Epoch 91/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10510.9494 - val_loss: 10482.0791\n",
      "Epoch 92/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10510.4482 - val_loss: 10481.5811\n",
      "Epoch 93/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10509.9475 - val_loss: 10481.0811\n",
      "Epoch 94/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10509.4478 - val_loss: 10480.5830\n",
      "Epoch 95/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10508.9479 - val_loss: 10480.0840\n",
      "Epoch 96/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10508.4496 - val_loss: 10479.5869\n",
      "Epoch 97/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10507.9510 - val_loss: 10479.0898\n",
      "Epoch 98/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10507.4535 - val_loss: 10478.5928\n",
      "Epoch 99/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10506.9558 - val_loss: 10478.0977\n",
      "Epoch 100/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10506.4589 - val_loss: 10477.6025\n",
      "Epoch 101/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10505.9626 - val_loss: 10477.1064\n",
      "Epoch 102/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10505.4668 - val_loss: 10476.6113\n",
      "Epoch 103/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10504.9720 - val_loss: 10476.1191\n",
      "Epoch 104/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10504.4762 - val_loss: 10475.6240\n",
      "Epoch 105/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10503.9816 - val_loss: 10475.1309\n",
      "Epoch 106/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10503.4876 - val_loss: 10474.6377\n",
      "Epoch 107/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10502.9941 - val_loss: 10474.1445\n",
      "Epoch 108/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10502.5009 - val_loss: 10473.6514\n",
      "Epoch 109/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10502.0076 - val_loss: 10473.1602\n",
      "Epoch 110/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10501.5154 - val_loss: 10472.6689\n",
      "Epoch 111/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10501.0231 - val_loss: 10472.1787\n",
      "Epoch 112/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10500.5314 - val_loss: 10471.6875\n",
      "Epoch 113/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10500.0399 - val_loss: 10471.1963\n",
      "Epoch 114/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10499.5492 - val_loss: 10470.7070\n",
      "Epoch 115/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10499.0582 - val_loss: 10470.2168\n",
      "Epoch 116/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10498.5680 - val_loss: 10469.7275\n",
      "Epoch 117/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10498.0779 - val_loss: 10469.2383\n",
      "Epoch 118/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10497.5881 - val_loss: 10468.7500\n",
      "Epoch 119/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10497.0987 - val_loss: 10468.2607\n",
      "Epoch 120/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10496.6101 - val_loss: 10467.7725\n",
      "Epoch 121/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10496.1213 - val_loss: 10467.2861\n",
      "Epoch 122/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10495.6327 - val_loss: 10466.7979\n",
      "Epoch 123/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10495.1444 - val_loss: 10466.3105\n",
      "Epoch 124/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10494.6568 - val_loss: 10465.8232\n",
      "Epoch 125/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10494.1691 - val_loss: 10465.3369\n",
      "Epoch 126/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10493.6818 - val_loss: 10464.8496\n",
      "Epoch 127/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10493.1956 - val_loss: 10464.3633\n",
      "Epoch 128/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10492.7083 - val_loss: 10463.8770\n",
      "Epoch 129/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10492.2222 - val_loss: 10463.3926\n",
      "Epoch 130/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 10491.7359 - val_loss: 10462.9072\n",
      "Epoch 131/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10491.2498 - val_loss: 10462.4219\n",
      "Epoch 132/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10490.7640 - val_loss: 10461.9365\n",
      "Epoch 133/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10490.2786 - val_loss: 10461.4521\n",
      "Epoch 134/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10489.7936 - val_loss: 10460.9678\n",
      "Epoch 135/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10489.3090 - val_loss: 10460.4834\n",
      "Epoch 136/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 10488.8239 - val_loss: 10460.0000\n",
      "Epoch 137/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10488.3399 - val_loss: 10459.5156\n",
      "Epoch 138/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 114us/step - loss: 10487.8559 - val_loss: 10459.0322\n",
      "Epoch 139/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 10487.3718 - val_loss: 10458.5488\n",
      "Epoch 140/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10486.8885 - val_loss: 10458.0654\n",
      "Epoch 141/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10486.4051 - val_loss: 10457.5820\n",
      "Epoch 142/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 10485.9218 - val_loss: 10457.1006\n",
      "Epoch 143/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10485.4386 - val_loss: 10456.6172\n",
      "Epoch 144/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10484.9555 - val_loss: 10456.1357\n",
      "Epoch 145/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10484.4733 - val_loss: 10455.6533\n",
      "Epoch 146/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10483.9907 - val_loss: 10455.1719\n",
      "Epoch 147/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10483.5085 - val_loss: 10454.6895\n",
      "Epoch 148/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10483.0263 - val_loss: 10454.2090\n",
      "Epoch 149/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10482.5446 - val_loss: 10453.7275\n",
      "Epoch 150/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10482.0633 - val_loss: 10453.2461\n",
      "Epoch 151/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10481.5818 - val_loss: 10452.7656\n",
      "Epoch 152/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10481.1006 - val_loss: 10452.2852\n",
      "Epoch 153/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10480.6196 - val_loss: 10451.8037\n",
      "Epoch 154/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10480.1386 - val_loss: 10451.3232\n",
      "Epoch 155/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10479.6582 - val_loss: 10450.8438\n",
      "Epoch 156/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10479.1774 - val_loss: 10450.3633\n",
      "Epoch 157/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10478.6973 - val_loss: 10449.8838\n",
      "Epoch 158/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10478.2172 - val_loss: 10449.4043\n",
      "Epoch 159/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10477.7373 - val_loss: 10448.9248\n",
      "Epoch 160/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10477.2570 - val_loss: 10448.4453\n",
      "Epoch 161/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10476.7773 - val_loss: 10447.9658\n",
      "Epoch 162/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10476.2980 - val_loss: 10447.4863\n",
      "Epoch 163/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10475.8188 - val_loss: 10447.0088\n",
      "Epoch 164/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10475.3396 - val_loss: 10446.5293\n",
      "Epoch 165/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10474.8606 - val_loss: 10446.0508\n",
      "Epoch 166/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10474.3819 - val_loss: 10445.5732\n",
      "Epoch 167/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10473.9030 - val_loss: 10445.0947\n",
      "Epoch 168/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10473.4244 - val_loss: 10444.6162\n",
      "Epoch 169/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10472.9463 - val_loss: 10444.1377\n",
      "Epoch 170/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10472.4676 - val_loss: 10443.6611\n",
      "Epoch 171/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10471.9902 - val_loss: 10443.1826\n",
      "Epoch 172/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10471.5119 - val_loss: 10442.7051\n",
      "Epoch 173/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10471.0341 - val_loss: 10442.2275\n",
      "Epoch 174/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10470.5566 - val_loss: 10441.7490\n",
      "Epoch 175/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10470.0788 - val_loss: 10441.2725\n",
      "Epoch 176/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10469.6012 - val_loss: 10440.7969\n",
      "Epoch 177/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10469.1244 - val_loss: 10440.3203\n",
      "Epoch 178/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10468.6471 - val_loss: 10439.8418\n",
      "Epoch 179/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10468.1700 - val_loss: 10439.3652\n",
      "Epoch 180/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 10467.6931 - val_loss: 10438.8906\n",
      "Epoch 181/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 10467.2165 - val_loss: 10438.4131\n",
      "Epoch 182/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 10466.7393 - val_loss: 10437.9365\n",
      "Epoch 183/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 10466.2634 - val_loss: 10437.4600\n",
      "Epoch 184/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10465.7871 - val_loss: 10436.9844\n",
      "Epoch 185/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10465.3105 - val_loss: 10436.5078\n",
      "Epoch 186/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10464.8345 - val_loss: 10436.0322\n",
      "Epoch 187/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10464.3584 - val_loss: 10435.5576\n",
      "Epoch 188/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10463.8828 - val_loss: 10435.0820\n",
      "Epoch 189/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10463.4067 - val_loss: 10434.6055\n",
      "Epoch 190/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10462.9311 - val_loss: 10434.1299\n",
      "Epoch 191/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10462.4557 - val_loss: 10433.6553\n",
      "Epoch 192/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10461.9800 - val_loss: 10433.1807\n",
      "Epoch 193/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10461.5049 - val_loss: 10432.7051\n",
      "Epoch 194/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10461.0292 - val_loss: 10432.2305\n",
      "Epoch 195/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10460.5542 - val_loss: 10431.7549\n",
      "Epoch 196/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10460.0793 - val_loss: 10431.2803\n",
      "Epoch 197/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10459.6047 - val_loss: 10430.8057\n",
      "Epoch 198/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10459.1299 - val_loss: 10430.3311\n",
      "Epoch 199/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10458.6547 - val_loss: 10429.8564\n",
      "Epoch 200/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10458.1802 - val_loss: 10429.3818\n",
      "Epoch 201/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10457.7058 - val_loss: 10428.9082\n",
      "Epoch 202/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10457.2312 - val_loss: 10428.4346\n",
      "Epoch 203/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10456.7571 - val_loss: 10427.9600\n",
      "Epoch 204/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10456.2829 - val_loss: 10427.4863\n",
      "Epoch 205/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10455.8084 - val_loss: 10427.0127\n",
      "Epoch 206/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10455.3349 - val_loss: 10426.5381\n",
      "Epoch 207/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10454.8605 - val_loss: 10426.0645\n",
      "Epoch 208/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10454.3868 - val_loss: 10425.5918\n",
      "Epoch 209/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10453.9132 - val_loss: 10425.1172\n",
      "Epoch 210/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10453.4398 - val_loss: 10424.6445\n",
      "Epoch 211/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10452.9657 - val_loss: 10424.1709\n",
      "Epoch 212/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10452.4920 - val_loss: 10423.6973\n",
      "Epoch 213/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10452.0188 - val_loss: 10423.2236\n",
      "Epoch 214/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10451.5457 - val_loss: 10422.7520\n",
      "Epoch 215/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10451.0724 - val_loss: 10422.2783\n",
      "Epoch 216/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10450.5992 - val_loss: 10421.8047\n",
      "Epoch 217/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10450.1264 - val_loss: 10421.3330\n",
      "Epoch 218/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10449.6535 - val_loss: 10420.8594\n",
      "Epoch 219/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10449.1811 - val_loss: 10420.3877\n",
      "Epoch 220/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10448.7070 - val_loss: 10419.9150\n",
      "Epoch 221/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10448.2348 - val_loss: 10419.4424\n",
      "Epoch 222/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10447.7626 - val_loss: 10418.9688\n",
      "Epoch 223/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10447.2896 - val_loss: 10418.4971\n",
      "Epoch 224/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10446.8174 - val_loss: 10418.0254\n",
      "Epoch 225/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10446.3447 - val_loss: 10417.5537\n",
      "Epoch 226/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10445.8724 - val_loss: 10417.0811\n",
      "Epoch 227/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10445.4000 - val_loss: 10416.6094\n",
      "Epoch 228/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10444.9281 - val_loss: 10416.1367\n",
      "Epoch 229/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10444.4560 - val_loss: 10415.6650\n",
      "Epoch 230/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10443.9840 - val_loss: 10415.1924\n",
      "Epoch 231/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10443.5120 - val_loss: 10414.7207\n",
      "Epoch 232/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10443.0399 - val_loss: 10414.2490\n",
      "Epoch 233/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10442.5682 - val_loss: 10413.7773\n",
      "Epoch 234/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10442.0965 - val_loss: 10413.3066\n",
      "Epoch 235/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10441.6250 - val_loss: 10412.8350\n",
      "Epoch 236/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10441.1532 - val_loss: 10412.3633\n",
      "Epoch 237/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10440.6816 - val_loss: 10411.8926\n",
      "Epoch 238/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10440.2098 - val_loss: 10411.4209\n",
      "Epoch 239/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10439.7387 - val_loss: 10410.9492\n",
      "Epoch 240/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10439.2673 - val_loss: 10410.4775\n",
      "Epoch 241/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10438.7960 - val_loss: 10410.0068\n",
      "Epoch 242/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10438.3250 - val_loss: 10409.5361\n",
      "Epoch 243/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10437.8538 - val_loss: 10409.0654\n",
      "Epoch 244/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10437.3822 - val_loss: 10408.5947\n",
      "Epoch 245/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10436.9114 - val_loss: 10408.1230\n",
      "Epoch 246/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10436.4404 - val_loss: 10407.6514\n",
      "Epoch 247/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10435.9698 - val_loss: 10407.1816\n",
      "Epoch 248/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10435.4990 - val_loss: 10406.7109\n",
      "Epoch 249/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10435.0281 - val_loss: 10406.2412\n",
      "Epoch 250/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10434.5571 - val_loss: 10405.7695\n",
      "Epoch 251/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10434.0873 - val_loss: 10405.2998\n",
      "Epoch 252/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10433.6162 - val_loss: 10404.8291\n",
      "Epoch 253/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10433.1453 - val_loss: 10404.3584\n",
      "Epoch 254/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10432.6746 - val_loss: 10403.8887\n",
      "Epoch 255/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10432.2043 - val_loss: 10403.4180\n",
      "Epoch 256/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10431.7340 - val_loss: 10402.9482\n",
      "Epoch 257/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10431.2638 - val_loss: 10402.4766\n",
      "Epoch 258/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10430.7933 - val_loss: 10402.0078\n",
      "Epoch 259/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10430.3229 - val_loss: 10401.5371\n",
      "Epoch 260/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10429.8530 - val_loss: 10401.0674\n",
      "Epoch 261/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10429.3836 - val_loss: 10400.5977\n",
      "Epoch 262/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10428.9132 - val_loss: 10400.1279\n",
      "Epoch 263/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10428.4433 - val_loss: 10399.6582\n",
      "Epoch 264/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10427.9732 - val_loss: 10399.1885\n",
      "Epoch 265/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10427.5036 - val_loss: 10398.7178\n",
      "Epoch 266/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10427.0335 - val_loss: 10398.2480\n",
      "Epoch 267/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10426.5637 - val_loss: 10397.7783\n",
      "Epoch 268/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10426.0940 - val_loss: 10397.3086\n",
      "Epoch 269/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10425.6243 - val_loss: 10396.8389\n",
      "Epoch 270/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10425.1549 - val_loss: 10396.3701\n",
      "Epoch 271/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10424.6849 - val_loss: 10395.9014\n",
      "Epoch 272/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10424.2156 - val_loss: 10395.4316\n",
      "Epoch 273/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10423.7457 - val_loss: 10394.9629\n",
      "Epoch 274/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10423.2762 - val_loss: 10394.4922\n",
      "Epoch 275/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10422.8074 - val_loss: 10394.0225\n",
      "Epoch 276/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10422.3377 - val_loss: 10393.5537\n",
      "Epoch 277/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10421.8684 - val_loss: 10393.0840\n",
      "Epoch 278/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10421.3994 - val_loss: 10392.6152\n",
      "Epoch 279/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10420.9301 - val_loss: 10392.1465\n",
      "Epoch 280/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10420.4611 - val_loss: 10391.6777\n",
      "Epoch 281/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10419.9914 - val_loss: 10391.2080\n",
      "Epoch 282/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 89us/step - loss: 10419.5224 - val_loss: 10390.7393\n",
      "Epoch 283/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10419.0538 - val_loss: 10390.2695\n",
      "Epoch 284/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10418.5845 - val_loss: 10389.8008\n",
      "Epoch 285/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10418.1155 - val_loss: 10389.3330\n",
      "Epoch 286/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10417.6463 - val_loss: 10388.8643\n",
      "Epoch 287/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10417.1779 - val_loss: 10388.3955\n",
      "Epoch 288/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10416.7091 - val_loss: 10387.9268\n",
      "Epoch 289/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10416.2407 - val_loss: 10387.4580\n",
      "Epoch 290/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10415.7719 - val_loss: 10386.9893\n",
      "Epoch 291/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10415.3033 - val_loss: 10386.5205\n",
      "Epoch 292/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10414.8347 - val_loss: 10386.0518\n",
      "Epoch 293/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10414.3658 - val_loss: 10385.5840\n",
      "Epoch 294/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10413.8971 - val_loss: 10385.1152\n",
      "Epoch 295/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10413.4285 - val_loss: 10384.6465\n",
      "Epoch 296/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10412.9603 - val_loss: 10384.1787\n",
      "Epoch 297/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10412.4915 - val_loss: 10383.7100\n",
      "Epoch 298/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10412.0234 - val_loss: 10383.2412\n",
      "Epoch 299/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10411.5551 - val_loss: 10382.7734\n",
      "Epoch 300/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10411.0867 - val_loss: 10382.3047\n",
      "Epoch 301/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10410.6182 - val_loss: 10381.8369\n",
      "Epoch 302/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10410.1497 - val_loss: 10381.3691\n",
      "Epoch 303/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10409.6816 - val_loss: 10380.9014\n",
      "Epoch 304/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10409.2131 - val_loss: 10380.4326\n",
      "Epoch 305/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10408.7452 - val_loss: 10379.9648\n",
      "Epoch 306/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10408.2775 - val_loss: 10379.4961\n",
      "Epoch 307/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10407.8089 - val_loss: 10379.0293\n",
      "Epoch 308/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10407.3411 - val_loss: 10378.5605\n",
      "Epoch 309/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10406.8734 - val_loss: 10378.0918\n",
      "Epoch 310/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10406.4055 - val_loss: 10377.6240\n",
      "Epoch 311/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10405.9373 - val_loss: 10377.1572\n",
      "Epoch 312/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10405.4697 - val_loss: 10376.6895\n",
      "Epoch 313/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10405.0019 - val_loss: 10376.2217\n",
      "Epoch 314/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10404.5342 - val_loss: 10375.7539\n",
      "Epoch 315/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10404.0664 - val_loss: 10375.2861\n",
      "Epoch 316/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10403.5984 - val_loss: 10374.8184\n",
      "Epoch 317/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10403.1306 - val_loss: 10374.3516\n",
      "Epoch 318/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10402.6624 - val_loss: 10373.8828\n",
      "Epoch 319/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10402.1950 - val_loss: 10373.4160\n",
      "Epoch 320/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10401.7273 - val_loss: 10372.9492\n",
      "Epoch 321/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10401.2602 - val_loss: 10372.4805\n",
      "Epoch 322/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10400.7922 - val_loss: 10372.0117\n",
      "Epoch 323/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10400.3252 - val_loss: 10371.5449\n",
      "Epoch 324/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10399.8573 - val_loss: 10371.0781\n",
      "Epoch 325/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10399.3901 - val_loss: 10370.6104\n",
      "Epoch 326/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10398.9226 - val_loss: 10370.1436\n",
      "Epoch 327/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10398.4550 - val_loss: 10369.6758\n",
      "Epoch 328/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10397.9870 - val_loss: 10369.2080\n",
      "Epoch 329/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10397.5201 - val_loss: 10368.7412\n",
      "Epoch 330/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10397.0529 - val_loss: 10368.2744\n",
      "Epoch 331/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10396.5860 - val_loss: 10367.8076\n",
      "Epoch 332/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10396.1183 - val_loss: 10367.3398\n",
      "Epoch 333/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10395.6514 - val_loss: 10366.8730\n",
      "Epoch 334/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10395.1843 - val_loss: 10366.4053\n",
      "Epoch 335/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10394.7168 - val_loss: 10365.9385\n",
      "Epoch 336/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10394.2498 - val_loss: 10365.4707\n",
      "Epoch 337/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10393.7823 - val_loss: 10365.0039\n",
      "Epoch 338/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10393.3152 - val_loss: 10364.5381\n",
      "Epoch 339/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10392.8483 - val_loss: 10364.0713\n",
      "Epoch 340/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10392.3813 - val_loss: 10363.6025\n",
      "Epoch 341/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10391.9148 - val_loss: 10363.1357\n",
      "Epoch 342/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10391.4477 - val_loss: 10362.6699\n",
      "Epoch 343/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10390.9806 - val_loss: 10362.2031\n",
      "Epoch 344/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10390.5133 - val_loss: 10361.7354\n",
      "Epoch 345/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10390.0461 - val_loss: 10361.2686\n",
      "Epoch 346/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10389.5796 - val_loss: 10360.8027\n",
      "Epoch 347/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10389.1127 - val_loss: 10360.3350\n",
      "Epoch 348/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10388.6464 - val_loss: 10359.8682\n",
      "Epoch 349/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10388.1794 - val_loss: 10359.4023\n",
      "Epoch 350/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10387.7127 - val_loss: 10358.9355\n",
      "Epoch 351/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10387.2458 - val_loss: 10358.4678\n",
      "Epoch 352/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10386.7790 - val_loss: 10358.0020\n",
      "Epoch 353/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10386.3123 - val_loss: 10357.5352\n",
      "Epoch 354/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10385.8455 - val_loss: 10357.0674\n",
      "Epoch 355/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10385.3790 - val_loss: 10356.6016\n",
      "Epoch 356/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10384.9125 - val_loss: 10356.1367\n",
      "Epoch 357/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10384.4463 - val_loss: 10355.6699\n",
      "Epoch 358/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10383.9790 - val_loss: 10355.2031\n",
      "Epoch 359/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10383.5125 - val_loss: 10354.7354\n",
      "Epoch 360/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10383.0458 - val_loss: 10354.2695\n",
      "Epoch 361/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10382.5797 - val_loss: 10353.8027\n",
      "Epoch 362/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10382.1137 - val_loss: 10353.3369\n",
      "Epoch 363/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10381.6474 - val_loss: 10352.8701\n",
      "Epoch 364/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10381.1805 - val_loss: 10352.4043\n",
      "Epoch 365/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10380.7135 - val_loss: 10351.9365\n",
      "Epoch 366/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10380.2475 - val_loss: 10351.4707\n",
      "Epoch 367/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10379.7811 - val_loss: 10351.0049\n",
      "Epoch 368/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10379.3150 - val_loss: 10350.5381\n",
      "Epoch 369/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10378.8488 - val_loss: 10350.0723\n",
      "Epoch 370/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10378.3824 - val_loss: 10349.6064\n",
      "Epoch 371/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10377.9158 - val_loss: 10349.1406\n",
      "Epoch 372/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10377.4498 - val_loss: 10348.6738\n",
      "Epoch 373/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10376.9834 - val_loss: 10348.2070\n",
      "Epoch 374/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10376.5178 - val_loss: 10347.7412\n",
      "Epoch 375/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10376.0512 - val_loss: 10347.2764\n",
      "Epoch 376/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10375.5851 - val_loss: 10346.8086\n",
      "Epoch 377/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10375.1187 - val_loss: 10346.3428\n",
      "Epoch 378/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10374.6530 - val_loss: 10345.8770\n",
      "Epoch 379/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10374.1866 - val_loss: 10345.4111\n",
      "Epoch 380/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10373.7206 - val_loss: 10344.9443\n",
      "Epoch 381/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10373.2545 - val_loss: 10344.4785\n",
      "Epoch 382/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10372.7881 - val_loss: 10344.0117\n",
      "Epoch 383/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10372.3221 - val_loss: 10343.5479\n",
      "Epoch 384/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10371.8561 - val_loss: 10343.0811\n",
      "Epoch 385/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10371.3903 - val_loss: 10342.6152\n",
      "Epoch 386/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10370.9247 - val_loss: 10342.1494\n",
      "Epoch 387/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10370.4584 - val_loss: 10341.6836\n",
      "Epoch 388/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10369.9923 - val_loss: 10341.2168\n",
      "Epoch 389/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10369.5268 - val_loss: 10340.7520\n",
      "Epoch 390/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10369.0610 - val_loss: 10340.2861\n",
      "Epoch 391/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10368.5957 - val_loss: 10339.8193\n",
      "Epoch 392/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10368.1292 - val_loss: 10339.3535\n",
      "Epoch 393/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10367.6629 - val_loss: 10338.8877\n",
      "Epoch 394/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10367.1973 - val_loss: 10338.4229\n",
      "Epoch 395/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10366.7316 - val_loss: 10337.9570\n",
      "Epoch 396/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10366.2663 - val_loss: 10337.4912\n",
      "Epoch 397/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10365.7996 - val_loss: 10337.0254\n",
      "Epoch 398/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10365.3342 - val_loss: 10336.5605\n",
      "Epoch 399/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10364.8684 - val_loss: 10336.0938\n",
      "Epoch 400/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10364.4031 - val_loss: 10335.6289\n",
      "Epoch 401/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10363.9380 - val_loss: 10335.1631\n",
      "Epoch 402/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10363.4717 - val_loss: 10334.6963\n",
      "Epoch 403/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10363.0057 - val_loss: 10334.2314\n",
      "Epoch 404/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10362.5406 - val_loss: 10333.7656\n",
      "Epoch 405/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10362.0755 - val_loss: 10333.3008\n",
      "Epoch 406/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10361.6095 - val_loss: 10332.8350\n",
      "Epoch 407/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10361.1434 - val_loss: 10332.3701\n",
      "Epoch 408/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10360.6779 - val_loss: 10331.9043\n",
      "Epoch 409/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10360.2125 - val_loss: 10331.4385\n",
      "Epoch 410/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10359.7473 - val_loss: 10330.9727\n",
      "Epoch 411/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10359.2818 - val_loss: 10330.5068\n",
      "Epoch 412/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10358.8156 - val_loss: 10330.0420\n",
      "Epoch 413/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10358.3504 - val_loss: 10329.5762\n",
      "Epoch 414/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10357.8851 - val_loss: 10329.1113\n",
      "Epoch 415/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10357.4200 - val_loss: 10328.6465\n",
      "Epoch 416/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10356.9542 - val_loss: 10328.1797\n",
      "Epoch 417/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10356.4882 - val_loss: 10327.7158\n",
      "Epoch 418/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10356.0235 - val_loss: 10327.2490\n",
      "Epoch 419/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10355.5581 - val_loss: 10326.7842\n",
      "Epoch 420/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10355.0930 - val_loss: 10326.3184\n",
      "Epoch 421/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10354.6270 - val_loss: 10325.8535\n",
      "Epoch 422/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10354.1618 - val_loss: 10325.3877\n",
      "Epoch 423/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10353.6970 - val_loss: 10324.9219\n",
      "Epoch 424/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10353.2314 - val_loss: 10324.4570\n",
      "Epoch 425/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10352.7657 - val_loss: 10323.9922\n",
      "Epoch 426/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 87us/step - loss: 10352.3003 - val_loss: 10323.5273\n",
      "Epoch 427/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10351.8351 - val_loss: 10323.0615\n",
      "Epoch 428/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10351.3707 - val_loss: 10322.5967\n",
      "Epoch 429/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10350.9048 - val_loss: 10322.1309\n",
      "Epoch 430/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10350.4393 - val_loss: 10321.6660\n",
      "Epoch 431/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10349.9744 - val_loss: 10321.2002\n",
      "Epoch 432/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10349.5095 - val_loss: 10320.7363\n",
      "Epoch 433/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10349.0445 - val_loss: 10320.2715\n",
      "Epoch 434/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10348.5785 - val_loss: 10319.8066\n",
      "Epoch 435/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10348.1136 - val_loss: 10319.3408\n",
      "Epoch 436/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10347.6495 - val_loss: 10318.8760\n",
      "Epoch 437/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10347.1838 - val_loss: 10318.4102\n",
      "Epoch 438/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10346.7184 - val_loss: 10317.9463\n",
      "Epoch 439/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10346.2532 - val_loss: 10317.4805\n",
      "Epoch 440/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10345.7885 - val_loss: 10317.0156\n",
      "Epoch 441/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10345.3236 - val_loss: 10316.5498\n",
      "Epoch 442/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10344.8578 - val_loss: 10316.0850\n",
      "Epoch 443/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10344.3933 - val_loss: 10315.6201\n",
      "Epoch 444/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10343.9284 - val_loss: 10315.1553\n",
      "Epoch 445/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10343.4638 - val_loss: 10314.6895\n",
      "Epoch 446/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10342.9978 - val_loss: 10314.2246\n",
      "Epoch 447/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10342.5329 - val_loss: 10313.7598\n",
      "Epoch 448/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10342.0684 - val_loss: 10313.2949\n",
      "Epoch 449/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10341.6031 - val_loss: 10312.8291\n",
      "Epoch 450/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10341.1381 - val_loss: 10312.3662\n",
      "Epoch 451/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10340.6729 - val_loss: 10311.9014\n",
      "Epoch 452/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10340.2086 - val_loss: 10311.4355\n",
      "Epoch 453/3000\n",
      "742/742 [==============================] - 0s 83us/step - loss: 10339.7437 - val_loss: 10310.9707\n",
      "Epoch 454/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10339.2780 - val_loss: 10310.5049\n",
      "Epoch 455/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10338.8133 - val_loss: 10310.0400\n",
      "Epoch 456/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10338.3493 - val_loss: 10309.5762\n",
      "Epoch 457/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10337.8846 - val_loss: 10309.1104\n",
      "Epoch 458/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10337.4188 - val_loss: 10308.6465\n",
      "Epoch 459/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10336.9543 - val_loss: 10308.1816\n",
      "Epoch 460/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10336.4896 - val_loss: 10307.7168\n",
      "Epoch 461/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10336.0248 - val_loss: 10307.2520\n",
      "Epoch 462/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10335.5595 - val_loss: 10306.7881\n",
      "Epoch 463/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10335.0946 - val_loss: 10306.3223\n",
      "Epoch 464/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10334.6303 - val_loss: 10305.8574\n",
      "Epoch 465/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10334.1654 - val_loss: 10305.3926\n",
      "Epoch 466/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10333.7002 - val_loss: 10304.9287\n",
      "Epoch 467/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10333.2361 - val_loss: 10304.4639\n",
      "Epoch 468/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10332.7717 - val_loss: 10303.9990\n",
      "Epoch 469/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10332.3064 - val_loss: 10303.5332\n",
      "Epoch 470/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10331.8413 - val_loss: 10303.0693\n",
      "Epoch 471/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10331.3771 - val_loss: 10302.6035\n",
      "Epoch 472/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10330.9129 - val_loss: 10302.1406\n",
      "Epoch 473/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10330.4469 - val_loss: 10301.6748\n",
      "Epoch 474/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10329.9828 - val_loss: 10301.2100\n",
      "Epoch 475/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10329.5187 - val_loss: 10300.7471\n",
      "Epoch 476/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10329.0541 - val_loss: 10300.2812\n",
      "Epoch 477/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10328.5887 - val_loss: 10299.8174\n",
      "Epoch 478/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10328.1240 - val_loss: 10299.3516\n",
      "Epoch 479/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10327.6598 - val_loss: 10298.8877\n",
      "Epoch 480/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10327.1950 - val_loss: 10298.4229\n",
      "Epoch 481/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10326.7304 - val_loss: 10297.9590\n",
      "Epoch 482/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10326.2656 - val_loss: 10297.4941\n",
      "Epoch 483/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10325.8017 - val_loss: 10297.0293\n",
      "Epoch 484/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10325.3365 - val_loss: 10296.5645\n",
      "Epoch 485/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10324.8719 - val_loss: 10296.1006\n",
      "Epoch 486/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10324.4078 - val_loss: 10295.6357\n",
      "Epoch 487/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10323.9435 - val_loss: 10295.1709\n",
      "Epoch 488/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10323.4780 - val_loss: 10294.7070\n",
      "Epoch 489/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10323.0141 - val_loss: 10294.2412\n",
      "Epoch 490/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10322.5499 - val_loss: 10293.7783\n",
      "Epoch 491/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10322.0851 - val_loss: 10293.3125\n",
      "Epoch 492/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10321.6200 - val_loss: 10292.8486\n",
      "Epoch 493/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10321.1561 - val_loss: 10292.3838\n",
      "Epoch 494/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10320.6920 - val_loss: 10291.9189\n",
      "Epoch 495/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10320.2266 - val_loss: 10291.4551\n",
      "Epoch 496/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10319.7623 - val_loss: 10290.9902\n",
      "Epoch 497/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10319.2984 - val_loss: 10290.5273\n",
      "Epoch 498/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10318.8335 - val_loss: 10290.0615\n",
      "Epoch 499/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10318.3687 - val_loss: 10289.5977\n",
      "Epoch 500/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10317.9048 - val_loss: 10289.1338\n",
      "Epoch 501/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10317.4409 - val_loss: 10288.6689\n",
      "Epoch 502/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10316.9755 - val_loss: 10288.2031\n",
      "Epoch 503/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10316.5112 - val_loss: 10287.7393\n",
      "Epoch 504/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10316.0475 - val_loss: 10287.2764\n",
      "Epoch 505/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10315.5831 - val_loss: 10286.8115\n",
      "Epoch 506/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10315.1180 - val_loss: 10286.3467\n",
      "Epoch 507/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10314.6543 - val_loss: 10285.8818\n",
      "Epoch 508/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10314.1904 - val_loss: 10285.4180\n",
      "Epoch 509/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10313.7253 - val_loss: 10284.9531\n",
      "Epoch 510/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10313.2610 - val_loss: 10284.4883\n",
      "Epoch 511/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10312.7970 - val_loss: 10284.0264\n",
      "Epoch 512/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10312.3331 - val_loss: 10283.5605\n",
      "Epoch 513/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10311.8676 - val_loss: 10283.0967\n",
      "Epoch 514/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10311.4042 - val_loss: 10282.6318\n",
      "Epoch 515/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10310.9398 - val_loss: 10282.1680\n",
      "Epoch 516/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10310.4748 - val_loss: 10281.7031\n",
      "Epoch 517/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10310.0107 - val_loss: 10281.2383\n",
      "Epoch 518/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10309.5472 - val_loss: 10280.7754\n",
      "Epoch 519/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10309.0825 - val_loss: 10280.3105\n",
      "Epoch 520/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10308.6179 - val_loss: 10279.8477\n",
      "Epoch 521/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10308.1539 - val_loss: 10279.3828\n",
      "Epoch 522/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10307.6901 - val_loss: 10278.9180\n",
      "Epoch 523/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10307.2249 - val_loss: 10278.4541\n",
      "Epoch 524/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10306.7613 - val_loss: 10277.9893\n",
      "Epoch 525/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10306.2975 - val_loss: 10277.5264\n",
      "Epoch 526/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10305.8325 - val_loss: 10277.0615\n",
      "Epoch 527/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10305.3683 - val_loss: 10276.5977\n",
      "Epoch 528/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10304.9049 - val_loss: 10276.1338\n",
      "Epoch 529/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10304.4404 - val_loss: 10275.6680\n",
      "Epoch 530/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10303.9757 - val_loss: 10275.2051\n",
      "Epoch 531/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10303.5121 - val_loss: 10274.7402\n",
      "Epoch 532/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10303.0486 - val_loss: 10274.2773\n",
      "Epoch 533/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10302.5832 - val_loss: 10273.8125\n",
      "Epoch 534/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10302.1195 - val_loss: 10273.3486\n",
      "Epoch 535/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10301.6559 - val_loss: 10272.8848\n",
      "Epoch 536/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10301.1912 - val_loss: 10272.4189\n",
      "Epoch 537/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10300.7270 - val_loss: 10271.9570\n",
      "Epoch 538/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10300.2633 - val_loss: 10271.4922\n",
      "Epoch 539/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10299.7992 - val_loss: 10271.0273\n",
      "Epoch 540/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10299.3347 - val_loss: 10270.5645\n",
      "Epoch 541/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10298.8710 - val_loss: 10270.0986\n",
      "Epoch 542/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10298.4068 - val_loss: 10269.6357\n",
      "Epoch 543/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10297.9423 - val_loss: 10269.1709\n",
      "Epoch 544/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10297.4781 - val_loss: 10268.7070\n",
      "Epoch 545/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10297.0148 - val_loss: 10268.2432\n",
      "Epoch 546/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10296.5504 - val_loss: 10267.7793\n",
      "Epoch 547/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10296.0858 - val_loss: 10267.3154\n",
      "Epoch 548/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10295.6224 - val_loss: 10266.8525\n",
      "Epoch 549/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10295.1581 - val_loss: 10266.3867\n",
      "Epoch 550/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10294.6940 - val_loss: 10265.9229\n",
      "Epoch 551/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10294.2303 - val_loss: 10265.4590\n",
      "Epoch 552/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10293.7666 - val_loss: 10264.9951\n",
      "Epoch 553/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10293.3015 - val_loss: 10264.5322\n",
      "Epoch 554/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10292.8381 - val_loss: 10264.0674\n",
      "Epoch 555/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10292.3745 - val_loss: 10263.6025\n",
      "Epoch 556/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10291.9096 - val_loss: 10263.1396\n",
      "Epoch 557/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10291.4456 - val_loss: 10262.6748\n",
      "Epoch 558/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10290.9827 - val_loss: 10262.2119\n",
      "Epoch 559/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10290.5180 - val_loss: 10261.7471\n",
      "Epoch 560/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10290.0536 - val_loss: 10261.2832\n",
      "Epoch 561/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10289.5905 - val_loss: 10260.8203\n",
      "Epoch 562/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10289.1267 - val_loss: 10260.3545\n",
      "Epoch 563/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10288.6615 - val_loss: 10259.8916\n",
      "Epoch 564/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10288.1983 - val_loss: 10259.4277\n",
      "Epoch 565/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10287.7345 - val_loss: 10258.9639\n",
      "Epoch 566/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10287.2699 - val_loss: 10258.5010\n",
      "Epoch 567/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10286.8066 - val_loss: 10258.0361\n",
      "Epoch 568/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10286.3433 - val_loss: 10257.5713\n",
      "Epoch 569/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10285.8781 - val_loss: 10257.1084\n",
      "Epoch 570/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 89us/step - loss: 10285.4148 - val_loss: 10256.6426\n",
      "Epoch 571/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10284.9512 - val_loss: 10256.1797\n",
      "Epoch 572/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10284.4866 - val_loss: 10255.7168\n",
      "Epoch 573/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10284.0231 - val_loss: 10255.2520\n",
      "Epoch 574/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10283.5596 - val_loss: 10254.7900\n",
      "Epoch 575/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10283.0951 - val_loss: 10254.3242\n",
      "Epoch 576/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10282.6312 - val_loss: 10253.8604\n",
      "Epoch 577/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10282.1677 - val_loss: 10253.3975\n",
      "Epoch 578/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10281.7037 - val_loss: 10252.9326\n",
      "Epoch 579/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10281.2391 - val_loss: 10252.4697\n",
      "Epoch 580/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10280.7759 - val_loss: 10252.0049\n",
      "Epoch 581/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10280.3119 - val_loss: 10251.5410\n",
      "Epoch 582/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10279.8482 - val_loss: 10251.0781\n",
      "Epoch 583/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10279.3842 - val_loss: 10250.6143\n",
      "Epoch 584/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10278.9207 - val_loss: 10250.1484\n",
      "Epoch 585/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10278.4563 - val_loss: 10249.6855\n",
      "Epoch 586/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10277.9926 - val_loss: 10249.2227\n",
      "Epoch 587/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10277.5295 - val_loss: 10248.7588\n",
      "Epoch 588/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10277.0645 - val_loss: 10248.2939\n",
      "Epoch 589/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10276.6015 - val_loss: 10247.8301\n",
      "Epoch 590/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10276.1378 - val_loss: 10247.3672\n",
      "Epoch 591/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10275.6734 - val_loss: 10246.9033\n",
      "Epoch 592/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10275.2094 - val_loss: 10246.4395\n",
      "Epoch 593/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10274.7464 - val_loss: 10245.9756\n",
      "Epoch 594/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10274.2821 - val_loss: 10245.5117\n",
      "Epoch 595/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10273.8181 - val_loss: 10245.0469\n",
      "Epoch 596/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10273.3552 - val_loss: 10244.5850\n",
      "Epoch 597/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10272.8908 - val_loss: 10244.1201\n",
      "Epoch 598/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10272.4270 - val_loss: 10243.6572\n",
      "Epoch 599/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10271.9639 - val_loss: 10243.1934\n",
      "Epoch 600/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10271.4992 - val_loss: 10242.7285\n",
      "Epoch 601/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10271.0356 - val_loss: 10242.2656\n",
      "Epoch 602/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10270.5725 - val_loss: 10241.8018\n",
      "Epoch 603/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10270.1085 - val_loss: 10241.3379\n",
      "Epoch 604/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10269.6446 - val_loss: 10240.8750\n",
      "Epoch 605/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10269.1813 - val_loss: 10240.4111\n",
      "Epoch 606/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10268.7171 - val_loss: 10239.9463\n",
      "Epoch 607/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10268.2534 - val_loss: 10239.4844\n",
      "Epoch 608/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10267.7904 - val_loss: 10239.0195\n",
      "Epoch 609/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10267.3264 - val_loss: 10238.5547\n",
      "Epoch 610/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10266.8621 - val_loss: 10238.0918\n",
      "Epoch 611/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10266.3987 - val_loss: 10237.6299\n",
      "Epoch 612/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10265.9351 - val_loss: 10237.1650\n",
      "Epoch 613/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10265.4707 - val_loss: 10236.7012\n",
      "Epoch 614/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10265.0074 - val_loss: 10236.2383\n",
      "Epoch 615/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10264.5443 - val_loss: 10235.7725\n",
      "Epoch 616/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10264.0797 - val_loss: 10235.3105\n",
      "Epoch 617/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10263.6164 - val_loss: 10234.8457\n",
      "Epoch 618/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10263.1535 - val_loss: 10234.3828\n",
      "Epoch 619/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10262.6887 - val_loss: 10233.9199\n",
      "Epoch 620/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10262.2256 - val_loss: 10233.4551\n",
      "Epoch 621/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10261.7622 - val_loss: 10232.9912\n",
      "Epoch 622/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10261.2977 - val_loss: 10232.5283\n",
      "Epoch 623/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10260.8347 - val_loss: 10232.0645\n",
      "Epoch 624/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10260.3714 - val_loss: 10231.6006\n",
      "Epoch 625/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10259.9072 - val_loss: 10231.1377\n",
      "Epoch 626/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10259.4438 - val_loss: 10230.6738\n",
      "Epoch 627/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10258.9805 - val_loss: 10230.2100\n",
      "Epoch 628/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10258.5161 - val_loss: 10229.7461\n",
      "Epoch 629/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10258.0527 - val_loss: 10229.2832\n",
      "Epoch 630/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10257.5895 - val_loss: 10228.8193\n",
      "Epoch 631/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10257.1251 - val_loss: 10228.3555\n",
      "Epoch 632/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10256.6620 - val_loss: 10227.8906\n",
      "Epoch 633/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10256.1989 - val_loss: 10227.4287\n",
      "Epoch 634/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10255.7342 - val_loss: 10226.9639\n",
      "Epoch 635/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10255.2709 - val_loss: 10226.5010\n",
      "Epoch 636/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10254.8085 - val_loss: 10226.0381\n",
      "Epoch 637/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10254.3431 - val_loss: 10225.5742\n",
      "Epoch 638/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10253.8802 - val_loss: 10225.1094\n",
      "Epoch 639/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10253.4170 - val_loss: 10224.6465\n",
      "Epoch 640/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10252.9525 - val_loss: 10224.1826\n",
      "Epoch 641/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10252.4894 - val_loss: 10223.7197\n",
      "Epoch 642/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10252.0260 - val_loss: 10223.2549\n",
      "Epoch 643/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10251.5620 - val_loss: 10222.7930\n",
      "Epoch 644/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10251.0982 - val_loss: 10222.3281\n",
      "Epoch 645/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10250.6355 - val_loss: 10221.8643\n",
      "Epoch 646/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10250.1708 - val_loss: 10221.4014\n",
      "Epoch 647/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10249.7078 - val_loss: 10220.9375\n",
      "Epoch 648/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10249.2446 - val_loss: 10220.4736\n",
      "Epoch 649/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10248.7801 - val_loss: 10220.0117\n",
      "Epoch 650/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10248.3174 - val_loss: 10219.5469\n",
      "Epoch 651/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10247.8543 - val_loss: 10219.0830\n",
      "Epoch 652/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10247.3900 - val_loss: 10218.6201\n",
      "Epoch 653/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10246.9264 - val_loss: 10218.1572\n",
      "Epoch 654/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10246.4631 - val_loss: 10217.6924\n",
      "Epoch 655/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10245.9991 - val_loss: 10217.2305\n",
      "Epoch 656/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 10245.5356 - val_loss: 10216.7656\n",
      "Epoch 657/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 10245.0731 - val_loss: 10216.3018\n",
      "Epoch 658/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10244.6083 - val_loss: 10215.8389\n",
      "Epoch 659/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10244.1450 - val_loss: 10215.3750\n",
      "Epoch 660/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10243.6820 - val_loss: 10214.9111\n",
      "Epoch 661/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10243.2175 - val_loss: 10214.4492\n",
      "Epoch 662/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10242.7547 - val_loss: 10213.9844\n",
      "Epoch 663/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10242.2914 - val_loss: 10213.5205\n",
      "Epoch 664/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10241.8274 - val_loss: 10213.0576\n",
      "Epoch 665/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10241.3642 - val_loss: 10212.5938\n",
      "Epoch 666/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10240.9016 - val_loss: 10212.1299\n",
      "Epoch 667/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10240.4364 - val_loss: 10211.6680\n",
      "Epoch 668/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10239.9736 - val_loss: 10211.2031\n",
      "Epoch 669/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10239.5107 - val_loss: 10210.7402\n",
      "Epoch 670/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10239.0460 - val_loss: 10210.2764\n",
      "Epoch 671/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10238.5836 - val_loss: 10209.8145\n",
      "Epoch 672/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10238.1200 - val_loss: 10209.3506\n",
      "Epoch 673/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10237.6557 - val_loss: 10208.8867\n",
      "Epoch 674/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10237.1932 - val_loss: 10208.4229\n",
      "Epoch 675/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10236.7297 - val_loss: 10207.9600\n",
      "Epoch 676/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10236.2653 - val_loss: 10207.4961\n",
      "Epoch 677/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10235.8027 - val_loss: 10207.0332\n",
      "Epoch 678/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10235.3393 - val_loss: 10206.5693\n",
      "Epoch 679/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10234.8750 - val_loss: 10206.1064\n",
      "Epoch 680/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10234.4128 - val_loss: 10205.6426\n",
      "Epoch 681/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10233.9489 - val_loss: 10205.1787\n",
      "Epoch 682/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10233.4849 - val_loss: 10204.7158\n",
      "Epoch 683/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10233.0228 - val_loss: 10204.2529\n",
      "Epoch 684/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10232.5586 - val_loss: 10203.7891\n",
      "Epoch 685/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10232.0946 - val_loss: 10203.3252\n",
      "Epoch 686/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10231.6319 - val_loss: 10202.8633\n",
      "Epoch 687/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10231.1686 - val_loss: 10202.3984\n",
      "Epoch 688/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10230.7048 - val_loss: 10201.9346\n",
      "Epoch 689/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10230.2421 - val_loss: 10201.4727\n",
      "Epoch 690/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10229.7779 - val_loss: 10201.0088\n",
      "Epoch 691/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10229.3143 - val_loss: 10200.5449\n",
      "Epoch 692/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10228.8518 - val_loss: 10200.0820\n",
      "Epoch 693/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10228.3874 - val_loss: 10199.6191\n",
      "Epoch 694/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10227.9243 - val_loss: 10199.1543\n",
      "Epoch 695/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10227.4616 - val_loss: 10198.6924\n",
      "Epoch 696/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10226.9972 - val_loss: 10198.2285\n",
      "Epoch 697/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10226.5342 - val_loss: 10197.7646\n",
      "Epoch 698/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10226.0715 - val_loss: 10197.3008\n",
      "Epoch 699/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10225.6069 - val_loss: 10196.8389\n",
      "Epoch 700/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10225.1439 - val_loss: 10196.3730\n",
      "Epoch 701/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10224.6812 - val_loss: 10195.9111\n",
      "Epoch 702/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10224.2172 - val_loss: 10195.4482\n",
      "Epoch 703/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10223.7541 - val_loss: 10194.9844\n",
      "Epoch 704/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10223.2909 - val_loss: 10194.5205\n",
      "Epoch 705/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10222.8266 - val_loss: 10194.0576\n",
      "Epoch 706/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10222.3641 - val_loss: 10193.5938\n",
      "Epoch 707/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10221.9009 - val_loss: 10193.1299\n",
      "Epoch 708/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10221.4366 - val_loss: 10192.6680\n",
      "Epoch 709/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10220.9742 - val_loss: 10192.2051\n",
      "Epoch 710/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10220.5102 - val_loss: 10191.7402\n",
      "Epoch 711/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10220.0462 - val_loss: 10191.2764\n",
      "Epoch 712/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10219.5839 - val_loss: 10190.8145\n",
      "Epoch 713/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10219.1199 - val_loss: 10190.3506\n",
      "Epoch 714/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 90us/step - loss: 10218.6565 - val_loss: 10189.8867\n",
      "Epoch 715/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 10218.1941 - val_loss: 10189.4238\n",
      "Epoch 716/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10217.7300 - val_loss: 10188.9600\n",
      "Epoch 717/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10217.2659 - val_loss: 10188.4951\n",
      "Epoch 718/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10216.8039 - val_loss: 10188.0332\n",
      "Epoch 719/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10216.3391 - val_loss: 10187.5703\n",
      "Epoch 720/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10215.8759 - val_loss: 10187.1064\n",
      "Epoch 721/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10215.4136 - val_loss: 10186.6426\n",
      "Epoch 722/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10214.9490 - val_loss: 10186.1797\n",
      "Epoch 723/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10214.4861 - val_loss: 10185.7158\n",
      "Epoch 724/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10214.0232 - val_loss: 10185.2529\n",
      "Epoch 725/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10213.5589 - val_loss: 10184.7900\n",
      "Epoch 726/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10213.0961 - val_loss: 10184.3262\n",
      "Epoch 727/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10212.6331 - val_loss: 10183.8633\n",
      "Epoch 728/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10212.1690 - val_loss: 10183.4004\n",
      "Epoch 729/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10211.7063 - val_loss: 10182.9355\n",
      "Epoch 730/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10211.2426 - val_loss: 10182.4727\n",
      "Epoch 731/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10210.7788 - val_loss: 10182.0098\n",
      "Epoch 732/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10210.3164 - val_loss: 10181.5469\n",
      "Epoch 733/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10209.8531 - val_loss: 10181.0820\n",
      "Epoch 734/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10209.3883 - val_loss: 10180.6191\n",
      "Epoch 735/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10208.9259 - val_loss: 10180.1562\n",
      "Epoch 736/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10208.4624 - val_loss: 10179.6924\n",
      "Epoch 737/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10207.9983 - val_loss: 10179.2285\n",
      "Epoch 738/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10207.5360 - val_loss: 10178.7676\n",
      "Epoch 739/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10207.0723 - val_loss: 10178.3027\n",
      "Epoch 740/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10206.6089 - val_loss: 10177.8389\n",
      "Epoch 741/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10206.1461 - val_loss: 10177.3770\n",
      "Epoch 742/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10205.6822 - val_loss: 10176.9131\n",
      "Epoch 743/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10205.2189 - val_loss: 10176.4492\n",
      "Epoch 744/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10204.7557 - val_loss: 10175.9863\n",
      "Epoch 745/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10204.2920 - val_loss: 10175.5225\n",
      "Epoch 746/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10203.8290 - val_loss: 10175.0586\n",
      "Epoch 747/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10203.3664 - val_loss: 10174.5957\n",
      "Epoch 748/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10202.9020 - val_loss: 10174.1338\n",
      "Epoch 749/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10202.4392 - val_loss: 10173.6689\n",
      "Epoch 750/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10201.9763 - val_loss: 10173.2070\n",
      "Epoch 751/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10201.5120 - val_loss: 10172.7441\n",
      "Epoch 752/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10201.0491 - val_loss: 10172.2793\n",
      "Epoch 753/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10200.5863 - val_loss: 10171.8154\n",
      "Epoch 754/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10200.1220 - val_loss: 10171.3535\n",
      "Epoch 755/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10199.6596 - val_loss: 10170.8906\n",
      "Epoch 756/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10199.1960 - val_loss: 10170.4258\n",
      "Epoch 757/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10198.7324 - val_loss: 10169.9629\n",
      "Epoch 758/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10198.2697 - val_loss: 10169.5010\n",
      "Epoch 759/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10197.8062 - val_loss: 10169.0361\n",
      "Epoch 760/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10197.3421 - val_loss: 10168.5732\n",
      "Epoch 761/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10196.8803 - val_loss: 10168.1104\n",
      "Epoch 762/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10196.4163 - val_loss: 10167.6475\n",
      "Epoch 763/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10195.9530 - val_loss: 10167.1826\n",
      "Epoch 764/3000\n",
      "742/742 [==============================] - 0s 85us/step - loss: 10195.4905 - val_loss: 10166.7207\n",
      "Epoch 765/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10195.0262 - val_loss: 10166.2578\n",
      "Epoch 766/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10194.5636 - val_loss: 10165.7939\n",
      "Epoch 767/3000\n",
      "742/742 [==============================] - 0s 83us/step - loss: 10194.1008 - val_loss: 10165.3301\n",
      "Epoch 768/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10193.6368 - val_loss: 10164.8682\n",
      "Epoch 769/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10193.1738 - val_loss: 10164.4043\n",
      "Epoch 770/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10192.7110 - val_loss: 10163.9404\n",
      "Epoch 771/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10192.2472 - val_loss: 10163.4775\n",
      "Epoch 772/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10191.7844 - val_loss: 10163.0156\n",
      "Epoch 773/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10191.3211 - val_loss: 10162.5508\n",
      "Epoch 774/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10190.8573 - val_loss: 10162.0879\n",
      "Epoch 775/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10190.3946 - val_loss: 10161.6260\n",
      "Epoch 776/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10189.9310 - val_loss: 10161.1611\n",
      "Epoch 777/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10189.4680 - val_loss: 10160.6992\n",
      "Epoch 778/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10189.0056 - val_loss: 10160.2354\n",
      "Epoch 779/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10188.5414 - val_loss: 10159.7725\n",
      "Epoch 780/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10188.0784 - val_loss: 10159.3086\n",
      "Epoch 781/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10187.6163 - val_loss: 10158.8457\n",
      "Epoch 782/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10187.1520 - val_loss: 10158.3838\n",
      "Epoch 783/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10186.6892 - val_loss: 10157.9209\n",
      "Epoch 784/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10186.2264 - val_loss: 10157.4561\n",
      "Epoch 785/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10185.7623 - val_loss: 10156.9932\n",
      "Epoch 786/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10185.2997 - val_loss: 10156.5303\n",
      "Epoch 787/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10184.8364 - val_loss: 10156.0674\n",
      "Epoch 788/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10184.3727 - val_loss: 10155.6035\n",
      "Epoch 789/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10183.9103 - val_loss: 10155.1406\n",
      "Epoch 790/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10183.4461 - val_loss: 10154.6768\n",
      "Epoch 791/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10182.9837 - val_loss: 10154.2139\n",
      "Epoch 792/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10182.5211 - val_loss: 10153.7510\n",
      "Epoch 793/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10182.0571 - val_loss: 10153.2891\n",
      "Epoch 794/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10181.5939 - val_loss: 10152.8242\n",
      "Epoch 795/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10181.1317 - val_loss: 10152.3613\n",
      "Epoch 796/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10180.6678 - val_loss: 10151.8984\n",
      "Epoch 797/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10180.2049 - val_loss: 10151.4355\n",
      "Epoch 798/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10179.7420 - val_loss: 10150.9707\n",
      "Epoch 799/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10179.2782 - val_loss: 10150.5088\n",
      "Epoch 800/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10178.8159 - val_loss: 10150.0469\n",
      "Epoch 801/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10178.3520 - val_loss: 10149.5820\n",
      "Epoch 802/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10177.8887 - val_loss: 10149.1191\n",
      "Epoch 803/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10177.4261 - val_loss: 10148.6562\n",
      "Epoch 804/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10176.9621 - val_loss: 10148.1934\n",
      "Epoch 805/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10176.4991 - val_loss: 10147.7295\n",
      "Epoch 806/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10176.0366 - val_loss: 10147.2666\n",
      "Epoch 807/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10175.5725 - val_loss: 10146.8047\n",
      "Epoch 808/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10175.1101 - val_loss: 10146.3398\n",
      "Epoch 809/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10174.6465 - val_loss: 10145.8770\n",
      "Epoch 810/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10174.1832 - val_loss: 10145.4150\n",
      "Epoch 811/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10173.7206 - val_loss: 10144.9521\n",
      "Epoch 812/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10173.2576 - val_loss: 10144.4883\n",
      "Epoch 813/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10172.7935 - val_loss: 10144.0244\n",
      "Epoch 814/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10172.3309 - val_loss: 10143.5625\n",
      "Epoch 815/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10171.8677 - val_loss: 10143.0977\n",
      "Epoch 816/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10171.4045 - val_loss: 10142.6348\n",
      "Epoch 817/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10170.9418 - val_loss: 10142.1719\n",
      "Epoch 818/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10170.4780 - val_loss: 10141.7100\n",
      "Epoch 819/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10170.0149 - val_loss: 10141.2451\n",
      "Epoch 820/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 10169.5522 - val_loss: 10140.7812\n",
      "Epoch 821/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10169.0883 - val_loss: 10140.3213\n",
      "Epoch 822/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10168.6255 - val_loss: 10139.8564\n",
      "Epoch 823/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10168.1630 - val_loss: 10139.3926\n",
      "Epoch 824/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10167.6987 - val_loss: 10138.9297\n",
      "Epoch 825/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10167.2361 - val_loss: 10138.4668\n",
      "Epoch 826/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 10166.7726 - val_loss: 10138.0029\n",
      "Epoch 827/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10166.3097 - val_loss: 10137.5400\n",
      "Epoch 828/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10165.8474 - val_loss: 10137.0781\n",
      "Epoch 829/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10165.3834 - val_loss: 10136.6133\n",
      "Epoch 830/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10164.9202 - val_loss: 10136.1504\n",
      "Epoch 831/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 10164.4578 - val_loss: 10135.6875\n",
      "Epoch 832/3000\n",
      "742/742 [==============================] - 0s 107us/step - loss: 10163.9933 - val_loss: 10135.2256\n",
      "Epoch 833/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10163.5308 - val_loss: 10134.7617\n",
      "Epoch 834/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10163.0681 - val_loss: 10134.2969\n",
      "Epoch 835/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10162.6041 - val_loss: 10133.8359\n",
      "Epoch 836/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10162.1416 - val_loss: 10133.3721\n",
      "Epoch 837/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10161.6785 - val_loss: 10132.9082\n",
      "Epoch 838/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 10161.2143 - val_loss: 10132.4463\n",
      "Epoch 839/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10160.7520 - val_loss: 10131.9834\n",
      "Epoch 840/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10160.2886 - val_loss: 10131.5195\n",
      "Epoch 841/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10159.8248 - val_loss: 10131.0547\n",
      "Epoch 842/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10159.3631 - val_loss: 10130.5938\n",
      "Epoch 843/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 10158.8988 - val_loss: 10130.1299\n",
      "Epoch 844/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10158.4359 - val_loss: 10129.6660\n",
      "Epoch 845/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10157.9734 - val_loss: 10129.2031\n",
      "Epoch 846/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10157.5093 - val_loss: 10128.7412\n",
      "Epoch 847/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 10157.0466 - val_loss: 10128.2773\n",
      "Epoch 848/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10156.5839 - val_loss: 10127.8125\n",
      "Epoch 849/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10156.1202 - val_loss: 10127.3525\n",
      "Epoch 850/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 10155.6569 - val_loss: 10126.8887\n",
      "Epoch 851/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10155.1941 - val_loss: 10126.4238\n",
      "Epoch 852/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10154.7302 - val_loss: 10125.9609\n",
      "Epoch 853/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10154.2682 - val_loss: 10125.4980\n",
      "Epoch 854/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10153.8042 - val_loss: 10125.0342\n",
      "Epoch 855/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10153.3411 - val_loss: 10124.5713\n",
      "Epoch 856/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10152.8786 - val_loss: 10124.1094\n",
      "Epoch 857/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10152.4141 - val_loss: 10123.6465\n",
      "Epoch 858/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 122us/step - loss: 10151.9516 - val_loss: 10123.1816\n",
      "Epoch 859/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10151.4893 - val_loss: 10122.7188\n",
      "Epoch 860/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10151.0250 - val_loss: 10122.2559\n",
      "Epoch 861/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10150.5623 - val_loss: 10121.7939\n",
      "Epoch 862/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10150.0994 - val_loss: 10121.3281\n",
      "Epoch 863/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10149.6352 - val_loss: 10120.8662\n",
      "Epoch 864/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10149.1732 - val_loss: 10120.4043\n",
      "Epoch 865/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10148.7096 - val_loss: 10119.9395\n",
      "Epoch 866/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10148.2460 - val_loss: 10119.4775\n",
      "Epoch 867/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10147.7842 - val_loss: 10119.0146\n",
      "Epoch 868/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10147.3197 - val_loss: 10118.5508\n",
      "Epoch 869/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10146.8569 - val_loss: 10118.0869\n",
      "Epoch 870/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10146.3947 - val_loss: 10117.6240\n",
      "Epoch 871/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 10145.9303 - val_loss: 10117.1611\n",
      "Epoch 872/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10145.4677 - val_loss: 10116.6973\n",
      "Epoch 873/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10145.0049 - val_loss: 10116.2344\n",
      "Epoch 874/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10144.5412 - val_loss: 10115.7725\n",
      "Epoch 875/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10144.0783 - val_loss: 10115.3086\n",
      "Epoch 876/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10143.6153 - val_loss: 10114.8447\n",
      "Epoch 877/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10143.1516 - val_loss: 10114.3828\n",
      "Epoch 878/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10142.6891 - val_loss: 10113.9199\n",
      "Epoch 879/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 10142.2254 - val_loss: 10113.4561\n",
      "Epoch 880/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 10141.7621 - val_loss: 10112.9922\n",
      "Epoch 881/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10141.3000 - val_loss: 10112.5303\n",
      "Epoch 882/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 10140.8354 - val_loss: 10112.0654\n",
      "Epoch 883/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10140.3724 - val_loss: 10111.6025\n",
      "Epoch 884/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10139.9101 - val_loss: 10111.1406\n",
      "Epoch 885/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10139.4459 - val_loss: 10110.6777\n",
      "Epoch 886/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 10138.9836 - val_loss: 10110.2139\n",
      "Epoch 887/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10138.5209 - val_loss: 10109.7500\n",
      "Epoch 888/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 10138.0566 - val_loss: 10109.2881\n",
      "Epoch 889/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 10137.5942 - val_loss: 10108.8252\n",
      "Epoch 890/3000\n",
      "742/742 [==============================] - 0s 104us/step - loss: 10137.1307 - val_loss: 10108.3613\n",
      "Epoch 891/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10136.6671 - val_loss: 10107.8975\n",
      "Epoch 892/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10136.2052 - val_loss: 10107.4355\n",
      "Epoch 893/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10135.7412 - val_loss: 10106.9707\n",
      "Epoch 894/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10135.2782 - val_loss: 10106.5088\n",
      "Epoch 895/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10134.8158 - val_loss: 10106.0469\n",
      "Epoch 896/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10134.3511 - val_loss: 10105.5830\n",
      "Epoch 897/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10133.8885 - val_loss: 10105.1191\n",
      "Epoch 898/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10133.4261 - val_loss: 10104.6553\n",
      "Epoch 899/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10132.9620 - val_loss: 10104.1934\n",
      "Epoch 900/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10132.4991 - val_loss: 10103.7305\n",
      "Epoch 901/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10132.0368 - val_loss: 10103.2656\n",
      "Epoch 902/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10131.5722 - val_loss: 10102.8037\n",
      "Epoch 903/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10131.1100 - val_loss: 10102.3408\n",
      "Epoch 904/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 10130.6467 - val_loss: 10101.8770\n",
      "Epoch 905/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 10130.1830 - val_loss: 10101.4141\n",
      "Epoch 906/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 10129.7211 - val_loss: 10100.9512\n",
      "Epoch 907/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 10129.2568 - val_loss: 10100.4873\n",
      "Epoch 908/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 10128.7937 - val_loss: 10100.0234\n",
      "Epoch 909/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10128.3313 - val_loss: 10099.5615\n",
      "Epoch 910/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10127.8676 - val_loss: 10099.0986\n",
      "Epoch 911/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10127.4041 - val_loss: 10098.6348\n",
      "Epoch 912/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10126.9417 - val_loss: 10098.1719\n",
      "Epoch 913/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 10126.4772 - val_loss: 10097.7090\n",
      "Epoch 914/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10126.0148 - val_loss: 10097.2461\n",
      "Epoch 915/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10125.5521 - val_loss: 10096.7812\n",
      "Epoch 916/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10125.0884 - val_loss: 10096.3203\n",
      "Epoch 917/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 10124.6261 - val_loss: 10095.8564\n",
      "Epoch 918/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10124.1624 - val_loss: 10095.3926\n",
      "Epoch 919/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10123.6987 - val_loss: 10094.9287\n",
      "Epoch 920/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10123.2366 - val_loss: 10094.4668\n",
      "Epoch 921/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 10122.7724 - val_loss: 10094.0029\n",
      "Epoch 922/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10122.3090 - val_loss: 10093.5400\n",
      "Epoch 923/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10121.8470 - val_loss: 10093.0781\n",
      "Epoch 924/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10121.3829 - val_loss: 10092.6143\n",
      "Epoch 925/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10120.9206 - val_loss: 10092.1514\n",
      "Epoch 926/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10120.4577 - val_loss: 10091.6865\n",
      "Epoch 927/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10119.9934 - val_loss: 10091.2236\n",
      "Epoch 928/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10119.5312 - val_loss: 10090.7617\n",
      "Epoch 929/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10119.0678 - val_loss: 10090.2979\n",
      "Epoch 930/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 90us/step - loss: 10118.6042 - val_loss: 10089.8350\n",
      "Epoch 931/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10118.1422 - val_loss: 10089.3721\n",
      "Epoch 932/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10117.6780 - val_loss: 10088.9082\n",
      "Epoch 933/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 10117.2146 - val_loss: 10088.4453\n",
      "Epoch 934/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10116.7527 - val_loss: 10087.9834\n",
      "Epoch 935/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10116.2881 - val_loss: 10087.5195\n",
      "Epoch 936/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10115.8258 - val_loss: 10087.0557\n",
      "Epoch 937/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10115.3628 - val_loss: 10086.5918\n",
      "Epoch 938/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10114.8987 - val_loss: 10086.1299\n",
      "Epoch 939/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10114.4362 - val_loss: 10085.6680\n",
      "Epoch 940/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10113.9732 - val_loss: 10085.2031\n",
      "Epoch 941/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10113.5094 - val_loss: 10084.7412\n",
      "Epoch 942/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10113.0469 - val_loss: 10084.2773\n",
      "Epoch 943/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10112.5836 - val_loss: 10083.8135\n",
      "Epoch 944/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10112.1196 - val_loss: 10083.3506\n",
      "Epoch 945/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10111.6581 - val_loss: 10082.8877\n",
      "Epoch 946/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10111.1936 - val_loss: 10082.4248\n",
      "Epoch 947/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10110.7306 - val_loss: 10081.9609\n",
      "Epoch 948/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10110.2683 - val_loss: 10081.4980\n",
      "Epoch 949/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10109.8043 - val_loss: 10081.0361\n",
      "Epoch 950/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10109.3412 - val_loss: 10080.5713\n",
      "Epoch 951/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10108.8791 - val_loss: 10080.1084\n",
      "Epoch 952/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10108.4150 - val_loss: 10079.6455\n",
      "Epoch 953/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10107.9525 - val_loss: 10079.1826\n",
      "Epoch 954/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 10107.4895 - val_loss: 10078.7188\n",
      "Epoch 955/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10107.0252 - val_loss: 10078.2559\n",
      "Epoch 956/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10106.5630 - val_loss: 10077.7939\n",
      "Epoch 957/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10106.0992 - val_loss: 10077.3301\n",
      "Epoch 958/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10105.6367 - val_loss: 10076.8662\n",
      "Epoch 959/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10105.1742 - val_loss: 10076.4043\n",
      "Epoch 960/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10104.7096 - val_loss: 10075.9404\n",
      "Epoch 961/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10104.2470 - val_loss: 10075.4775\n",
      "Epoch 962/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10103.7841 - val_loss: 10075.0146\n",
      "Epoch 963/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10103.3203 - val_loss: 10074.5518\n",
      "Epoch 964/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10102.8578 - val_loss: 10074.0889\n",
      "Epoch 965/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10102.3951 - val_loss: 10073.6240\n",
      "Epoch 966/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10101.9310 - val_loss: 10073.1621\n",
      "Epoch 967/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10101.4686 - val_loss: 10072.6992\n",
      "Epoch 968/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10101.0054 - val_loss: 10072.2354\n",
      "Epoch 969/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10100.5419 - val_loss: 10071.7725\n",
      "Epoch 970/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10100.0797 - val_loss: 10071.3096\n",
      "Epoch 971/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10099.6157 - val_loss: 10070.8457\n",
      "Epoch 972/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10099.1526 - val_loss: 10070.3838\n",
      "Epoch 973/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10098.6905 - val_loss: 10069.9209\n",
      "Epoch 974/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10098.2262 - val_loss: 10069.4580\n",
      "Epoch 975/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10097.7635 - val_loss: 10068.9951\n",
      "Epoch 976/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10097.3010 - val_loss: 10068.5303\n",
      "Epoch 977/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 10096.8371 - val_loss: 10068.0674\n",
      "Epoch 978/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10096.3751 - val_loss: 10067.6055\n",
      "Epoch 979/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10095.9114 - val_loss: 10067.1406\n",
      "Epoch 980/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10095.4483 - val_loss: 10066.6787\n",
      "Epoch 981/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10094.9862 - val_loss: 10066.2158\n",
      "Epoch 982/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10094.5217 - val_loss: 10065.7529\n",
      "Epoch 983/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10094.0593 - val_loss: 10065.2900\n",
      "Epoch 984/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 10093.5966 - val_loss: 10064.8271\n",
      "Epoch 985/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10093.1327 - val_loss: 10064.3643\n",
      "Epoch 986/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10092.6697 - val_loss: 10063.9014\n",
      "Epoch 987/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 10092.2073 - val_loss: 10063.4365\n",
      "Epoch 988/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10091.7432 - val_loss: 10062.9746\n",
      "Epoch 989/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10091.2814 - val_loss: 10062.5127\n",
      "Epoch 990/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10090.8178 - val_loss: 10062.0488\n",
      "Epoch 991/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10090.3549 - val_loss: 10061.5850\n",
      "Epoch 992/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 10089.8929 - val_loss: 10061.1230\n",
      "Epoch 993/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10089.4288 - val_loss: 10060.6602\n",
      "Epoch 994/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 10088.9656 - val_loss: 10060.1973\n",
      "Epoch 995/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10088.5034 - val_loss: 10059.7334\n",
      "Epoch 996/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 10088.0392 - val_loss: 10059.2705\n",
      "Epoch 997/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10087.5771 - val_loss: 10058.8076\n",
      "Epoch 998/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 10087.1142 - val_loss: 10058.3438\n",
      "Epoch 999/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 10086.6509 - val_loss: 10057.8809\n",
      "Epoch 1000/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10086.1887 - val_loss: 10057.4189\n",
      "Epoch 1001/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 10085.7249 - val_loss: 10056.9570\n",
      "Epoch 1002/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10085.2624 - val_loss: 10056.4922\n",
      "Epoch 1003/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10084.7997 - val_loss: 10056.0293\n",
      "Epoch 1004/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 10084.3356 - val_loss: 10055.5674\n",
      "Epoch 1005/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 10083.8737 - val_loss: 10055.1055\n",
      "Epoch 1006/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 10083.4107 - val_loss: 10054.6406\n",
      "Epoch 1007/3000\n",
      "742/742 [==============================] - 0s 177us/step - loss: 10082.9475 - val_loss: 10054.1787\n",
      "Epoch 1008/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 10082.4853 - val_loss: 10053.7158\n",
      "Epoch 1009/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10082.0213 - val_loss: 10053.2529\n",
      "Epoch 1010/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10081.5588 - val_loss: 10052.7900\n",
      "Epoch 1011/3000\n",
      "742/742 [==============================] - 0s 159us/step - loss: 10081.0966 - val_loss: 10052.3262\n",
      "Epoch 1012/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10080.6320 - val_loss: 10051.8643\n",
      "Epoch 1013/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10080.1700 - val_loss: 10051.4014\n",
      "Epoch 1014/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10079.7074 - val_loss: 10050.9355\n",
      "Epoch 1015/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 10079.2436 - val_loss: 10050.4736\n",
      "Epoch 1016/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 10078.7818 - val_loss: 10050.0127\n",
      "Epoch 1017/3000\n",
      "742/742 [==============================] - 0s 157us/step - loss: 10078.3179 - val_loss: 10049.5488\n",
      "Epoch 1018/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 10077.8547 - val_loss: 10049.0850\n",
      "Epoch 1019/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 10077.3929 - val_loss: 10048.6221\n",
      "Epoch 1020/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 10076.9289 - val_loss: 10048.1611\n",
      "Epoch 1021/3000\n",
      "742/742 [==============================] - 0s 152us/step - loss: 10076.4663 - val_loss: 10047.6973\n",
      "Epoch 1022/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 10076.0042 - val_loss: 10047.2334\n",
      "Epoch 1023/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 10075.5401 - val_loss: 10046.7705\n",
      "Epoch 1024/3000\n",
      "742/742 [==============================] - 0s 156us/step - loss: 10075.0782 - val_loss: 10046.3096\n",
      "Epoch 1025/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10074.6146 - val_loss: 10045.8457\n",
      "Epoch 1026/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 10074.1511 - val_loss: 10045.3809\n",
      "Epoch 1027/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 10073.6893 - val_loss: 10044.9189\n",
      "Epoch 1028/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 10073.2257 - val_loss: 10044.4580\n",
      "Epoch 1029/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10072.7631 - val_loss: 10043.9941\n",
      "Epoch 1030/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10072.3006 - val_loss: 10043.5293\n",
      "Epoch 1031/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10071.8366 - val_loss: 10043.0674\n",
      "Epoch 1032/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10071.3750 - val_loss: 10042.6055\n",
      "Epoch 1033/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10070.9111 - val_loss: 10042.1406\n",
      "Epoch 1034/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 10070.4482 - val_loss: 10041.6787\n",
      "Epoch 1035/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10069.9860 - val_loss: 10041.2158\n",
      "Epoch 1036/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10069.5219 - val_loss: 10040.7539\n",
      "Epoch 1037/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10069.0592 - val_loss: 10040.2900\n",
      "Epoch 1038/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10068.5966 - val_loss: 10039.8271\n",
      "Epoch 1039/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10068.1332 - val_loss: 10039.3643\n",
      "Epoch 1040/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10067.6708 - val_loss: 10038.9014\n",
      "Epoch 1041/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10067.2074 - val_loss: 10038.4375\n",
      "Epoch 1042/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 10066.7446 - val_loss: 10037.9756\n",
      "Epoch 1043/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10066.2826 - val_loss: 10037.5127\n",
      "Epoch 1044/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10065.8181 - val_loss: 10037.0508\n",
      "Epoch 1045/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10065.3558 - val_loss: 10036.5859\n",
      "Epoch 1046/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10064.8934 - val_loss: 10036.1230\n",
      "Epoch 1047/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10064.4297 - val_loss: 10035.6611\n",
      "Epoch 1048/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10063.9675 - val_loss: 10035.1992\n",
      "Epoch 1049/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 10063.5046 - val_loss: 10034.7344\n",
      "Epoch 1050/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10063.0410 - val_loss: 10034.2715\n",
      "Epoch 1051/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10062.5787 - val_loss: 10033.8096\n",
      "Epoch 1052/3000\n",
      "742/742 [==============================] - ETA: 0s - loss: 10066.426 - 0s 128us/step - loss: 10062.1148 - val_loss: 10033.3467\n",
      "Epoch 1053/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10061.6522 - val_loss: 10032.8838\n",
      "Epoch 1054/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10061.1900 - val_loss: 10032.4199\n",
      "Epoch 1055/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10060.7264 - val_loss: 10031.9580\n",
      "Epoch 1056/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10060.2643 - val_loss: 10031.4951\n",
      "Epoch 1057/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10059.8008 - val_loss: 10031.0303\n",
      "Epoch 1058/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10059.3373 - val_loss: 10030.5674\n",
      "Epoch 1059/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 10058.8757 - val_loss: 10030.1064\n",
      "Epoch 1060/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10058.4115 - val_loss: 10029.6445\n",
      "Epoch 1061/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10057.9494 - val_loss: 10029.1787\n",
      "Epoch 1062/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10057.4868 - val_loss: 10028.7168\n",
      "Epoch 1063/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10057.0227 - val_loss: 10028.2539\n",
      "Epoch 1064/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10056.5607 - val_loss: 10027.7920\n",
      "Epoch 1065/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 10056.0976 - val_loss: 10027.3281\n",
      "Epoch 1066/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10055.6338 - val_loss: 10026.8652\n",
      "Epoch 1067/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10055.1724 - val_loss: 10026.4023\n",
      "Epoch 1068/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10054.7082 - val_loss: 10025.9395\n",
      "Epoch 1069/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10054.2458 - val_loss: 10025.4756\n",
      "Epoch 1070/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10053.7831 - val_loss: 10025.0127\n",
      "Epoch 1071/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10053.3192 - val_loss: 10024.5508\n",
      "Epoch 1072/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10052.8567 - val_loss: 10024.0879\n",
      "Epoch 1073/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 122us/step - loss: 10052.3942 - val_loss: 10023.6230\n",
      "Epoch 1074/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10051.9305 - val_loss: 10023.1611\n",
      "Epoch 1075/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10051.4685 - val_loss: 10022.6992\n",
      "Epoch 1076/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10051.0046 - val_loss: 10022.2354\n",
      "Epoch 1077/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10050.5418 - val_loss: 10021.7725\n",
      "Epoch 1078/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10050.0800 - val_loss: 10021.3096\n",
      "Epoch 1079/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10049.6159 - val_loss: 10020.8477\n",
      "Epoch 1080/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10049.1534 - val_loss: 10020.3848\n",
      "Epoch 1081/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10048.6910 - val_loss: 10019.9199\n",
      "Epoch 1082/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10048.2271 - val_loss: 10019.4580\n",
      "Epoch 1083/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10047.7652 - val_loss: 10018.9961\n",
      "Epoch 1084/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10047.3015 - val_loss: 10018.5312\n",
      "Epoch 1085/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10046.8384 - val_loss: 10018.0674\n",
      "Epoch 1086/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10046.3766 - val_loss: 10017.6064\n",
      "Epoch 1087/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 10045.9124 - val_loss: 10017.1445\n",
      "Epoch 1088/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 10045.4500 - val_loss: 10016.6816\n",
      "Epoch 1089/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10044.9878 - val_loss: 10016.2168\n",
      "Epoch 1090/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 10044.5239 - val_loss: 10015.7549\n",
      "Epoch 1091/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10044.0616 - val_loss: 10015.2930\n",
      "Epoch 1092/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10043.5983 - val_loss: 10014.8281\n",
      "Epoch 1093/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10043.1354 - val_loss: 10014.3652\n",
      "Epoch 1094/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10042.6730 - val_loss: 10013.9033\n",
      "Epoch 1095/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10042.2084 - val_loss: 10013.4404\n",
      "Epoch 1096/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10041.7465 - val_loss: 10012.9785\n",
      "Epoch 1097/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10041.2841 - val_loss: 10012.5137\n",
      "Epoch 1098/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10040.8203 - val_loss: 10012.0508\n",
      "Epoch 1099/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10040.3575 - val_loss: 10011.5889\n",
      "Epoch 1100/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10039.8947 - val_loss: 10011.1250\n",
      "Epoch 1101/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10039.4316 - val_loss: 10010.6611\n",
      "Epoch 1102/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 10038.9694 - val_loss: 10010.1992\n",
      "Epoch 1103/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10038.5054 - val_loss: 10009.7383\n",
      "Epoch 1104/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 10038.0427 - val_loss: 10009.2734\n",
      "Epoch 1105/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 10037.5805 - val_loss: 10008.8105\n",
      "Epoch 1106/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10037.1164 - val_loss: 10008.3477\n",
      "Epoch 1107/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10036.6544 - val_loss: 10007.8857\n",
      "Epoch 1108/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10036.1911 - val_loss: 10007.4219\n",
      "Epoch 1109/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10035.7276 - val_loss: 10006.9590\n",
      "Epoch 1110/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10035.2661 - val_loss: 10006.4961\n",
      "Epoch 1111/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10034.8016 - val_loss: 10006.0332\n",
      "Epoch 1112/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10034.3396 - val_loss: 10005.5703\n",
      "Epoch 1113/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10033.8772 - val_loss: 10005.1064\n",
      "Epoch 1114/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10033.4132 - val_loss: 10004.6445\n",
      "Epoch 1115/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10032.9510 - val_loss: 10004.1816\n",
      "Epoch 1116/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10032.4879 - val_loss: 10003.7178\n",
      "Epoch 1117/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10032.0244 - val_loss: 10003.2549\n",
      "Epoch 1118/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10031.5629 - val_loss: 10002.7930\n",
      "Epoch 1119/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10031.0987 - val_loss: 10002.3301\n",
      "Epoch 1120/3000\n",
      "742/742 [==============================] - ETA: 0s - loss: 10027.522 - 0s 120us/step - loss: 10030.6359 - val_loss: 10001.8662\n",
      "Epoch 1121/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10030.1740 - val_loss: 10001.4033\n",
      "Epoch 1122/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10029.7101 - val_loss: 10000.9414\n",
      "Epoch 1123/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10029.2474 - val_loss: 10000.4785\n",
      "Epoch 1124/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10028.7846 - val_loss: 10000.0146\n",
      "Epoch 1125/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10028.3210 - val_loss: 9999.5518\n",
      "Epoch 1126/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 10027.8596 - val_loss: 9999.0898\n",
      "Epoch 1127/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 10027.3951 - val_loss: 9998.6260\n",
      "Epoch 1128/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10026.9321 - val_loss: 9998.1631\n",
      "Epoch 1129/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10026.4705 - val_loss: 9997.7002\n",
      "Epoch 1130/3000\n",
      "742/742 [==============================] - 0s 177us/step - loss: 10026.0064 - val_loss: 9997.2383\n",
      "Epoch 1131/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 10025.5438 - val_loss: 9996.7754\n",
      "Epoch 1132/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 10025.0812 - val_loss: 9996.3105\n",
      "Epoch 1133/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10024.6175 - val_loss: 9995.8486\n",
      "Epoch 1134/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10024.1558 - val_loss: 9995.3867\n",
      "Epoch 1135/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10023.6918 - val_loss: 9994.9219\n",
      "Epoch 1136/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10023.2290 - val_loss: 9994.4590\n",
      "Epoch 1137/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10022.7669 - val_loss: 9993.9961\n",
      "Epoch 1138/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10022.3030 - val_loss: 9993.5342\n",
      "Epoch 1139/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 10021.8403 - val_loss: 9993.0713\n",
      "Epoch 1140/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 10021.3777 - val_loss: 9992.6074\n",
      "Epoch 1141/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 10020.9140 - val_loss: 9992.1445\n",
      "Epoch 1142/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10020.4524 - val_loss: 9991.6826\n",
      "Epoch 1143/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 10019.9885 - val_loss: 9991.2188\n",
      "Epoch 1144/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 10019.5256 - val_loss: 9990.7549\n",
      "Epoch 1145/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 10019.0636 - val_loss: 9990.2930\n",
      "Epoch 1146/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10018.5995 - val_loss: 9989.8320\n",
      "Epoch 1147/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10018.1373 - val_loss: 9989.3691\n",
      "Epoch 1148/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10017.6748 - val_loss: 9988.9033\n",
      "Epoch 1149/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10017.2107 - val_loss: 9988.4414\n",
      "Epoch 1150/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 10016.7485 - val_loss: 9987.9805\n",
      "Epoch 1151/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10016.2852 - val_loss: 9987.5156\n",
      "Epoch 1152/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10015.8223 - val_loss: 9987.0527\n",
      "Epoch 1153/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10015.3604 - val_loss: 9986.5898\n",
      "Epoch 1154/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10014.8960 - val_loss: 9986.1279\n",
      "Epoch 1155/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10014.4335 - val_loss: 9985.6660\n",
      "Epoch 1156/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10013.9716 - val_loss: 9985.2002\n",
      "Epoch 1157/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10013.5073 - val_loss: 9984.7383\n",
      "Epoch 1158/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10013.0453 - val_loss: 9984.2764\n",
      "Epoch 1159/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10012.5823 - val_loss: 9983.8115\n",
      "Epoch 1160/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 10012.1185 - val_loss: 9983.3486\n",
      "Epoch 1161/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10011.6568 - val_loss: 9982.8867\n",
      "Epoch 1162/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 10011.1924 - val_loss: 9982.4248\n",
      "Epoch 1163/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10010.7303 - val_loss: 9981.9600\n",
      "Epoch 1164/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10010.2672 - val_loss: 9981.4971\n",
      "Epoch 1165/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10009.8035 - val_loss: 9981.0342\n",
      "Epoch 1166/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10009.3417 - val_loss: 9980.5723\n",
      "Epoch 1167/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10008.8788 - val_loss: 9980.1084\n",
      "Epoch 1168/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10008.4151 - val_loss: 9979.6455\n",
      "Epoch 1169/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10007.9533 - val_loss: 9979.1826\n",
      "Epoch 1170/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10007.4890 - val_loss: 9978.7207\n",
      "Epoch 1171/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10007.0268 - val_loss: 9978.2559\n",
      "Epoch 1172/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 10006.5645 - val_loss: 9977.7939\n",
      "Epoch 1173/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10006.1004 - val_loss: 9977.3320\n",
      "Epoch 1174/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 10005.6378 - val_loss: 9976.8691\n",
      "Epoch 1175/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10005.1751 - val_loss: 9976.4043\n",
      "Epoch 1176/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 10004.7113 - val_loss: 9975.9424\n",
      "Epoch 1177/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 10004.2501 - val_loss: 9975.4805\n",
      "Epoch 1178/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 10003.7858 - val_loss: 9975.0176\n",
      "Epoch 1179/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 10003.3234 - val_loss: 9974.5537\n",
      "Epoch 1180/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 10002.8606 - val_loss: 9974.0908\n",
      "Epoch 1181/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 10002.3971 - val_loss: 9973.6279\n",
      "Epoch 1182/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 10001.9349 - val_loss: 9973.1660\n",
      "Epoch 1183/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10001.4720 - val_loss: 9972.7021\n",
      "Epoch 1184/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 10001.0085 - val_loss: 9972.2393\n",
      "Epoch 1185/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 10000.5467 - val_loss: 9971.7764\n",
      "Epoch 1186/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 10000.0824 - val_loss: 9971.3145\n",
      "Epoch 1187/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9999.6202 - val_loss: 9970.8496\n",
      "Epoch 1188/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9999.1576 - val_loss: 9970.3877\n",
      "Epoch 1189/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9998.6934 - val_loss: 9969.9258\n",
      "Epoch 1190/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9998.2310 - val_loss: 9969.4629\n",
      "Epoch 1191/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9997.7686 - val_loss: 9968.9980\n",
      "Epoch 1192/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9997.3050 - val_loss: 9968.5352\n",
      "Epoch 1193/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9996.8421 - val_loss: 9968.0742\n",
      "Epoch 1194/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9996.3785 - val_loss: 9967.6094\n",
      "Epoch 1195/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9995.9159 - val_loss: 9967.1455\n",
      "Epoch 1196/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9995.4542 - val_loss: 9966.6826\n",
      "Epoch 1197/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9994.9897 - val_loss: 9966.2217\n",
      "Epoch 1198/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9994.5276 - val_loss: 9965.7588\n",
      "Epoch 1199/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9994.0648 - val_loss: 9965.2939\n",
      "Epoch 1200/3000\n",
      "742/742 [==============================] - ETA: 0s - loss: 10006.036 - 0s 117us/step - loss: 9993.6009 - val_loss: 9964.8320\n",
      "Epoch 1201/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9993.1392 - val_loss: 9964.3701\n",
      "Epoch 1202/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9992.6756 - val_loss: 9963.9053\n",
      "Epoch 1203/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9992.2126 - val_loss: 9963.4424\n",
      "Epoch 1204/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9991.7503 - val_loss: 9962.9805\n",
      "Epoch 1205/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9991.2865 - val_loss: 9962.5195\n",
      "Epoch 1206/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9990.8243 - val_loss: 9962.0547\n",
      "Epoch 1207/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9990.3618 - val_loss: 9961.5908\n",
      "Epoch 1208/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9989.8979 - val_loss: 9961.1279\n",
      "Epoch 1209/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9989.4358 - val_loss: 9960.6680\n",
      "Epoch 1210/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9988.9723 - val_loss: 9960.2031\n",
      "Epoch 1211/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9988.5092 - val_loss: 9959.7393\n",
      "Epoch 1212/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9988.0474 - val_loss: 9959.2764\n",
      "Epoch 1213/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9987.5833 - val_loss: 9958.8154\n",
      "Epoch 1214/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9987.1208 - val_loss: 9958.3525\n",
      "Epoch 1215/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9986.6584 - val_loss: 9957.8877\n",
      "Epoch 1216/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 106us/step - loss: 9986.1946 - val_loss: 9957.4258\n",
      "Epoch 1217/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9985.7330 - val_loss: 9956.9639\n",
      "Epoch 1218/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9985.2692 - val_loss: 9956.4980\n",
      "Epoch 1219/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9984.8052 - val_loss: 9956.0361\n",
      "Epoch 1220/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9984.3441 - val_loss: 9955.5742\n",
      "Epoch 1221/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9983.8796 - val_loss: 9955.1133\n",
      "Epoch 1222/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9983.4173 - val_loss: 9954.6475\n",
      "Epoch 1223/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9982.9547 - val_loss: 9954.1846\n",
      "Epoch 1224/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9982.4907 - val_loss: 9953.7217\n",
      "Epoch 1225/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 9982.0286 - val_loss: 9953.2588\n",
      "Epoch 1226/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9981.5655 - val_loss: 9952.7959\n",
      "Epoch 1227/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9981.1023 - val_loss: 9952.3330\n",
      "Epoch 1228/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9980.6403 - val_loss: 9951.8701\n",
      "Epoch 1229/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9980.1761 - val_loss: 9951.4082\n",
      "Epoch 1230/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9979.7139 - val_loss: 9950.9434\n",
      "Epoch 1231/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9979.2515 - val_loss: 9950.4814\n",
      "Epoch 1232/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9978.7879 - val_loss: 9950.0195\n",
      "Epoch 1233/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9978.3250 - val_loss: 9949.5566\n",
      "Epoch 1234/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9977.8625 - val_loss: 9949.0918\n",
      "Epoch 1235/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9977.3985 - val_loss: 9948.6299\n",
      "Epoch 1236/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9976.9369 - val_loss: 9948.1680\n",
      "Epoch 1237/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9976.4728 - val_loss: 9947.7051\n",
      "Epoch 1238/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9976.0104 - val_loss: 9947.2412\n",
      "Epoch 1239/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9975.5482 - val_loss: 9946.7783\n",
      "Epoch 1240/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9975.0841 - val_loss: 9946.3154\n",
      "Epoch 1241/3000\n",
      "742/742 [==============================] - 0s 139us/step - loss: 9974.6223 - val_loss: 9945.8535\n",
      "Epoch 1242/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9974.1589 - val_loss: 9945.3896\n",
      "Epoch 1243/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9973.6956 - val_loss: 9944.9268\n",
      "Epoch 1244/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9973.2339 - val_loss: 9944.4639\n",
      "Epoch 1245/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9972.7696 - val_loss: 9944.0020\n",
      "Epoch 1246/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9972.3074 - val_loss: 9943.5371\n",
      "Epoch 1247/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9971.8449 - val_loss: 9943.0742\n",
      "Epoch 1248/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9971.3811 - val_loss: 9942.6133\n",
      "Epoch 1249/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9970.9186 - val_loss: 9942.1494\n",
      "Epoch 1250/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9970.4560 - val_loss: 9941.6846\n",
      "Epoch 1251/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 9969.9915 - val_loss: 9941.2217\n",
      "Epoch 1252/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 9969.5298 - val_loss: 9940.7617\n",
      "Epoch 1253/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9969.0661 - val_loss: 9940.2969\n",
      "Epoch 1254/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9968.6030 - val_loss: 9939.8330\n",
      "Epoch 1255/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9968.1412 - val_loss: 9939.3701\n",
      "Epoch 1256/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9967.6770 - val_loss: 9938.9092\n",
      "Epoch 1257/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9967.2149 - val_loss: 9938.4453\n",
      "Epoch 1258/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9966.7522 - val_loss: 9937.9814\n",
      "Epoch 1259/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9966.2885 - val_loss: 9937.5195\n",
      "Epoch 1260/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9965.8267 - val_loss: 9937.0576\n",
      "Epoch 1261/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9965.3624 - val_loss: 9936.5918\n",
      "Epoch 1262/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9964.8998 - val_loss: 9936.1299\n",
      "Epoch 1263/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9964.4381 - val_loss: 9935.6680\n",
      "Epoch 1264/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9963.9738 - val_loss: 9935.2070\n",
      "Epoch 1265/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9963.5117 - val_loss: 9934.7422\n",
      "Epoch 1266/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9963.0493 - val_loss: 9934.2783\n",
      "Epoch 1267/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9962.5853 - val_loss: 9933.8154\n",
      "Epoch 1268/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9962.1227 - val_loss: 9933.3555\n",
      "Epoch 1269/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9961.6595 - val_loss: 9932.8896\n",
      "Epoch 1270/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9961.1965 - val_loss: 9932.4268\n",
      "Epoch 1271/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9960.7349 - val_loss: 9931.9639\n",
      "Epoch 1272/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9960.2704 - val_loss: 9931.5029\n",
      "Epoch 1273/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9959.8083 - val_loss: 9931.0391\n",
      "Epoch 1274/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9959.3459 - val_loss: 9930.5752\n",
      "Epoch 1275/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9958.8816 - val_loss: 9930.1133\n",
      "Epoch 1276/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9958.4193 - val_loss: 9929.6504\n",
      "Epoch 1277/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9957.9561 - val_loss: 9929.1855\n",
      "Epoch 1278/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9957.4928 - val_loss: 9928.7236\n",
      "Epoch 1279/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9957.0306 - val_loss: 9928.2617\n",
      "Epoch 1280/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9956.5665 - val_loss: 9927.7988\n",
      "Epoch 1281/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9956.1040 - val_loss: 9927.3350\n",
      "Epoch 1282/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9955.6417 - val_loss: 9926.8721\n",
      "Epoch 1283/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9955.1777 - val_loss: 9926.4092\n",
      "Epoch 1284/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9954.7159 - val_loss: 9925.9463\n",
      "Epoch 1285/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9954.2527 - val_loss: 9925.4814\n",
      "Epoch 1286/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9953.7887 - val_loss: 9925.0195\n",
      "Epoch 1287/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9953.3275 - val_loss: 9924.5576\n",
      "Epoch 1288/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9952.8625 - val_loss: 9924.0928\n",
      "Epoch 1289/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9952.3998 - val_loss: 9923.6299\n",
      "Epoch 1290/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9951.9382 - val_loss: 9923.1680\n",
      "Epoch 1291/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9951.4736 - val_loss: 9922.7070\n",
      "Epoch 1292/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9951.0114 - val_loss: 9922.2422\n",
      "Epoch 1293/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9950.5489 - val_loss: 9921.7783\n",
      "Epoch 1294/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9950.0844 - val_loss: 9921.3154\n",
      "Epoch 1295/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9949.6223 - val_loss: 9920.8535\n",
      "Epoch 1296/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9949.1590 - val_loss: 9920.3896\n",
      "Epoch 1297/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9948.6954 - val_loss: 9919.9258\n",
      "Epoch 1298/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9948.2333 - val_loss: 9919.4639\n",
      "Epoch 1299/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9947.7694 - val_loss: 9919.0000\n",
      "Epoch 1300/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9947.3062 - val_loss: 9918.5361\n",
      "Epoch 1301/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9946.8439 - val_loss: 9918.0742\n",
      "Epoch 1302/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9946.3801 - val_loss: 9917.6133\n",
      "Epoch 1303/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9945.9173 - val_loss: 9917.1484\n",
      "Epoch 1304/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9945.4549 - val_loss: 9916.6846\n",
      "Epoch 1305/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9944.9904 - val_loss: 9916.2217\n",
      "Epoch 1306/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9944.5281 - val_loss: 9915.7588\n",
      "Epoch 1307/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9944.0652 - val_loss: 9915.2959\n",
      "Epoch 1308/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9943.6018 - val_loss: 9914.8320\n",
      "Epoch 1309/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9943.1390 - val_loss: 9914.3701\n",
      "Epoch 1310/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9942.6751 - val_loss: 9913.9043\n",
      "Epoch 1311/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9942.2120 - val_loss: 9913.4424\n",
      "Epoch 1312/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9941.7499 - val_loss: 9912.9805\n",
      "Epoch 1313/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9941.2855 - val_loss: 9912.5176\n",
      "Epoch 1314/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9940.8227 - val_loss: 9912.0537\n",
      "Epoch 1315/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9940.3603 - val_loss: 9911.5898\n",
      "Epoch 1316/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9939.8964 - val_loss: 9911.1270\n",
      "Epoch 1317/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9939.4338 - val_loss: 9910.6641\n",
      "Epoch 1318/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9938.9707 - val_loss: 9910.2002\n",
      "Epoch 1319/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9938.5064 - val_loss: 9909.7383\n",
      "Epoch 1320/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9938.0447 - val_loss: 9909.2754\n",
      "Epoch 1321/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9937.5810 - val_loss: 9908.8105\n",
      "Epoch 1322/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9937.1169 - val_loss: 9908.3477\n",
      "Epoch 1323/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 9936.6551 - val_loss: 9907.8857\n",
      "Epoch 1324/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9936.1913 - val_loss: 9907.4219\n",
      "Epoch 1325/3000\n",
      "742/742 [==============================] - 0s 147us/step - loss: 9935.7278 - val_loss: 9906.9580\n",
      "Epoch 1326/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9935.2656 - val_loss: 9906.4951\n",
      "Epoch 1327/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9934.8014 - val_loss: 9906.0332\n",
      "Epoch 1328/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9934.3388 - val_loss: 9905.5684\n",
      "Epoch 1329/3000\n",
      "742/742 [==============================] - 0s 139us/step - loss: 9933.8760 - val_loss: 9905.1055\n",
      "Epoch 1330/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9933.4122 - val_loss: 9904.6436\n",
      "Epoch 1331/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9932.9500 - val_loss: 9904.1816\n",
      "Epoch 1332/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9932.4864 - val_loss: 9903.7158\n",
      "Epoch 1333/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9932.0224 - val_loss: 9903.2529\n",
      "Epoch 1334/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9931.5602 - val_loss: 9902.7910\n",
      "Epoch 1335/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9931.0969 - val_loss: 9902.3271\n",
      "Epoch 1336/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9930.6338 - val_loss: 9901.8643\n",
      "Epoch 1337/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9930.1711 - val_loss: 9901.4014\n",
      "Epoch 1338/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9929.7067 - val_loss: 9900.9395\n",
      "Epoch 1339/3000\n",
      "742/742 [==============================] - 0s 143us/step - loss: 9929.2441 - val_loss: 9900.4736\n",
      "Epoch 1340/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9928.7818 - val_loss: 9900.0117\n",
      "Epoch 1341/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9928.3173 - val_loss: 9899.5498\n",
      "Epoch 1342/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9927.8547 - val_loss: 9899.0859\n",
      "Epoch 1343/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9927.3924 - val_loss: 9898.6211\n",
      "Epoch 1344/3000\n",
      "742/742 [==============================] - 0s 145us/step - loss: 9926.9277 - val_loss: 9898.1592\n",
      "Epoch 1345/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9926.4659 - val_loss: 9897.6963\n",
      "Epoch 1346/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9926.0024 - val_loss: 9897.2334\n",
      "Epoch 1347/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9925.5389 - val_loss: 9896.7695\n",
      "Epoch 1348/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9925.0764 - val_loss: 9896.3076\n",
      "Epoch 1349/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9924.6124 - val_loss: 9895.8418\n",
      "Epoch 1350/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9924.1491 - val_loss: 9895.3799\n",
      "Epoch 1351/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9923.6871 - val_loss: 9894.9180\n",
      "Epoch 1352/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9923.2227 - val_loss: 9894.4551\n",
      "Epoch 1353/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9922.7602 - val_loss: 9893.9912\n",
      "Epoch 1354/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9922.2978 - val_loss: 9893.5273\n",
      "Epoch 1355/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9921.8334 - val_loss: 9893.0645\n",
      "Epoch 1356/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9921.3708 - val_loss: 9892.6016\n",
      "Epoch 1357/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9920.9083 - val_loss: 9892.1377\n",
      "Epoch 1358/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9920.4441 - val_loss: 9891.6758\n",
      "Epoch 1359/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9919.9817 - val_loss: 9891.2119\n",
      "Epoch 1360/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 128us/step - loss: 9919.5186 - val_loss: 9890.7480\n",
      "Epoch 1361/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9919.0543 - val_loss: 9890.2842\n",
      "Epoch 1362/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9918.5925 - val_loss: 9889.8232\n",
      "Epoch 1363/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9918.1279 - val_loss: 9889.3584\n",
      "Epoch 1364/3000\n",
      "742/742 [==============================] - 0s 149us/step - loss: 9917.6650 - val_loss: 9888.8955\n",
      "Epoch 1365/3000\n",
      "742/742 [==============================] - 0s 152us/step - loss: 9917.2031 - val_loss: 9888.4326\n",
      "Epoch 1366/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9916.7388 - val_loss: 9887.9707\n",
      "Epoch 1367/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9916.2757 - val_loss: 9887.5059\n",
      "Epoch 1368/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9915.8134 - val_loss: 9887.0430\n",
      "Epoch 1369/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9915.3494 - val_loss: 9886.5811\n",
      "Epoch 1370/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9914.8872 - val_loss: 9886.1182\n",
      "Epoch 1371/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9914.4236 - val_loss: 9885.6533\n",
      "Epoch 1372/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9913.9596 - val_loss: 9885.1904\n",
      "Epoch 1373/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9913.4980 - val_loss: 9884.7285\n",
      "Epoch 1374/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9913.0341 - val_loss: 9884.2646\n",
      "Epoch 1375/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9912.5705 - val_loss: 9883.8008\n",
      "Epoch 1376/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9912.1083 - val_loss: 9883.3389\n",
      "Epoch 1377/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9911.6441 - val_loss: 9882.8760\n",
      "Epoch 1378/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9911.1815 - val_loss: 9882.4111\n",
      "Epoch 1379/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9910.7186 - val_loss: 9881.9482\n",
      "Epoch 1380/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9910.2543 - val_loss: 9881.4863\n",
      "Epoch 1381/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9909.7922 - val_loss: 9881.0234\n",
      "Epoch 1382/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9909.3297 - val_loss: 9880.5586\n",
      "Epoch 1383/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9908.8651 - val_loss: 9880.0957\n",
      "Epoch 1384/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9908.4029 - val_loss: 9879.6338\n",
      "Epoch 1385/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9907.9400 - val_loss: 9879.1699\n",
      "Epoch 1386/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9907.4764 - val_loss: 9878.7070\n",
      "Epoch 1387/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9907.0137 - val_loss: 9878.2441\n",
      "Epoch 1388/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9906.5498 - val_loss: 9877.7793\n",
      "Epoch 1389/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9906.0868 - val_loss: 9877.3174\n",
      "Epoch 1390/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9905.6246 - val_loss: 9876.8545\n",
      "Epoch 1391/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9905.1599 - val_loss: 9876.3916\n",
      "Epoch 1392/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9904.6970 - val_loss: 9875.9287\n",
      "Epoch 1393/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9904.2350 - val_loss: 9875.4639\n",
      "Epoch 1394/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9903.7706 - val_loss: 9875.0020\n",
      "Epoch 1395/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9903.3081 - val_loss: 9874.5391\n",
      "Epoch 1396/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9902.8456 - val_loss: 9874.0752\n",
      "Epoch 1397/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9902.3817 - val_loss: 9873.6123\n",
      "Epoch 1398/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9901.9182 - val_loss: 9873.1494\n",
      "Epoch 1399/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9901.4554 - val_loss: 9872.6855\n",
      "Epoch 1400/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9900.9917 - val_loss: 9872.2217\n",
      "Epoch 1401/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9900.5301 - val_loss: 9871.7607\n",
      "Epoch 1402/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9900.0654 - val_loss: 9871.2959\n",
      "Epoch 1403/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9899.6025 - val_loss: 9870.8330\n",
      "Epoch 1404/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9899.1406 - val_loss: 9870.3701\n",
      "Epoch 1405/3000\n",
      "742/742 [==============================] - 0s 151us/step - loss: 9898.6760 - val_loss: 9869.9082\n",
      "Epoch 1406/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9898.2134 - val_loss: 9869.4434\n",
      "Epoch 1407/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9897.7508 - val_loss: 9868.9805\n",
      "Epoch 1408/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9897.2869 - val_loss: 9868.5186\n",
      "Epoch 1409/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9896.8244 - val_loss: 9868.0547\n",
      "Epoch 1410/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9896.3613 - val_loss: 9867.5908\n",
      "Epoch 1411/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9895.8969 - val_loss: 9867.1279\n",
      "Epoch 1412/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9895.4352 - val_loss: 9866.6650\n",
      "Epoch 1413/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9894.9716 - val_loss: 9866.2012\n",
      "Epoch 1414/3000\n",
      "742/742 [==============================] - 0s 151us/step - loss: 9894.5081 - val_loss: 9865.7383\n",
      "Epoch 1415/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9894.0458 - val_loss: 9865.2764\n",
      "Epoch 1416/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9893.5815 - val_loss: 9864.8135\n",
      "Epoch 1417/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9893.1183 - val_loss: 9864.3486\n",
      "Epoch 1418/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9892.6563 - val_loss: 9863.8857\n",
      "Epoch 1419/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9892.1920 - val_loss: 9863.4238\n",
      "Epoch 1420/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9891.7294 - val_loss: 9862.9600\n",
      "Epoch 1421/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9891.2673 - val_loss: 9862.4951\n",
      "Epoch 1422/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9890.8028 - val_loss: 9862.0332\n",
      "Epoch 1423/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9890.3402 - val_loss: 9861.5713\n",
      "Epoch 1424/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9889.8773 - val_loss: 9861.1064\n",
      "Epoch 1425/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9889.4134 - val_loss: 9860.6445\n",
      "Epoch 1426/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9888.9508 - val_loss: 9860.1816\n",
      "Epoch 1427/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9888.4873 - val_loss: 9859.7168\n",
      "Epoch 1428/3000\n",
      "742/742 [==============================] - ETA: 0s - loss: 9895.47 - 0s 126us/step - loss: 9888.0238 - val_loss: 9859.2529\n",
      "Epoch 1429/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9887.5611 - val_loss: 9858.7920\n",
      "Epoch 1430/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9887.0975 - val_loss: 9858.3291\n",
      "Epoch 1431/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9886.6346 - val_loss: 9857.8643\n",
      "Epoch 1432/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9886.1725 - val_loss: 9857.4014\n",
      "Epoch 1433/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9885.7077 - val_loss: 9856.9395\n",
      "Epoch 1434/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9885.2453 - val_loss: 9856.4756\n",
      "Epoch 1435/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9884.7828 - val_loss: 9856.0117\n",
      "Epoch 1436/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9884.3185 - val_loss: 9855.5498\n",
      "Epoch 1437/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9883.8558 - val_loss: 9855.0869\n",
      "Epoch 1438/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9883.3928 - val_loss: 9854.6221\n",
      "Epoch 1439/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9882.9292 - val_loss: 9854.1592\n",
      "Epoch 1440/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9882.4667 - val_loss: 9853.6963\n",
      "Epoch 1441/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9882.0028 - val_loss: 9853.2334\n",
      "Epoch 1442/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9881.5400 - val_loss: 9852.7695\n",
      "Epoch 1443/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9881.0776 - val_loss: 9852.3076\n",
      "Epoch 1444/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9880.6136 - val_loss: 9851.8447\n",
      "Epoch 1445/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9880.1506 - val_loss: 9851.3799\n",
      "Epoch 1446/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9879.6880 - val_loss: 9850.9180\n",
      "Epoch 1447/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9879.2241 - val_loss: 9850.4561\n",
      "Epoch 1448/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9878.7612 - val_loss: 9849.9912\n",
      "Epoch 1449/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9878.2985 - val_loss: 9849.5273\n",
      "Epoch 1450/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9877.8342 - val_loss: 9849.0645\n",
      "Epoch 1451/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9877.3724 - val_loss: 9848.6025\n",
      "Epoch 1452/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9876.9084 - val_loss: 9848.1387\n",
      "Epoch 1453/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9876.4449 - val_loss: 9847.6758\n",
      "Epoch 1454/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9875.9829 - val_loss: 9847.2139\n",
      "Epoch 1455/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9875.5191 - val_loss: 9846.7510\n",
      "Epoch 1456/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9875.0560 - val_loss: 9846.2852\n",
      "Epoch 1457/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9874.5935 - val_loss: 9845.8232\n",
      "Epoch 1458/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9874.1294 - val_loss: 9845.3613\n",
      "Epoch 1459/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9873.6669 - val_loss: 9844.8965\n",
      "Epoch 1460/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9873.2041 - val_loss: 9844.4326\n",
      "Epoch 1461/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9872.7400 - val_loss: 9843.9707\n",
      "Epoch 1462/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9872.2775 - val_loss: 9843.5088\n",
      "Epoch 1463/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9871.8146 - val_loss: 9843.0439\n",
      "Epoch 1464/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9871.3504 - val_loss: 9842.5811\n",
      "Epoch 1465/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9870.8882 - val_loss: 9842.1191\n",
      "Epoch 1466/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9870.4248 - val_loss: 9841.6543\n",
      "Epoch 1467/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9869.9612 - val_loss: 9841.1904\n",
      "Epoch 1468/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9869.4987 - val_loss: 9840.7285\n",
      "Epoch 1469/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9869.0346 - val_loss: 9840.2666\n",
      "Epoch 1470/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9868.5720 - val_loss: 9839.8018\n",
      "Epoch 1471/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9868.1099 - val_loss: 9839.3389\n",
      "Epoch 1472/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9867.6450 - val_loss: 9838.8770\n",
      "Epoch 1473/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9867.1822 - val_loss: 9838.4131\n",
      "Epoch 1474/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9866.7200 - val_loss: 9837.9492\n",
      "Epoch 1475/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9866.2560 - val_loss: 9837.4873\n",
      "Epoch 1476/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9865.7927 - val_loss: 9837.0244\n",
      "Epoch 1477/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9865.3302 - val_loss: 9836.5596\n",
      "Epoch 1478/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9864.8661 - val_loss: 9836.0957\n",
      "Epoch 1479/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9864.4040 - val_loss: 9835.6338\n",
      "Epoch 1480/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9863.9403 - val_loss: 9835.1709\n",
      "Epoch 1481/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9863.4774 - val_loss: 9834.7070\n",
      "Epoch 1482/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9863.0151 - val_loss: 9834.2451\n",
      "Epoch 1483/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9862.5502 - val_loss: 9833.7822\n",
      "Epoch 1484/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9862.0880 - val_loss: 9833.3174\n",
      "Epoch 1485/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9861.6256 - val_loss: 9832.8555\n",
      "Epoch 1486/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9861.1609 - val_loss: 9832.3926\n",
      "Epoch 1487/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9860.6982 - val_loss: 9831.9287\n",
      "Epoch 1488/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9860.2360 - val_loss: 9831.4648\n",
      "Epoch 1489/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9859.7720 - val_loss: 9831.0020\n",
      "Epoch 1490/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9859.3094 - val_loss: 9830.5400\n",
      "Epoch 1491/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9858.8462 - val_loss: 9830.0752\n",
      "Epoch 1492/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9858.3820 - val_loss: 9829.6133\n",
      "Epoch 1493/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9857.9203 - val_loss: 9829.1514\n",
      "Epoch 1494/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9857.4565 - val_loss: 9828.6855\n",
      "Epoch 1495/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9856.9927 - val_loss: 9828.2227\n",
      "Epoch 1496/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9856.5308 - val_loss: 9827.7607\n",
      "Epoch 1497/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9856.0668 - val_loss: 9827.2988\n",
      "Epoch 1498/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9855.6035 - val_loss: 9826.8340\n",
      "Epoch 1499/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9855.1417 - val_loss: 9826.3701\n",
      "Epoch 1500/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9854.6775 - val_loss: 9825.9082\n",
      "Epoch 1501/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9854.2153 - val_loss: 9825.4463\n",
      "Epoch 1502/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9853.7517 - val_loss: 9824.9805\n",
      "Epoch 1503/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9853.2876 - val_loss: 9824.5186\n",
      "Epoch 1504/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 112us/step - loss: 9852.8259 - val_loss: 9824.0566\n",
      "Epoch 1505/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9852.3621 - val_loss: 9823.5918\n",
      "Epoch 1506/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9851.8983 - val_loss: 9823.1279\n",
      "Epoch 1507/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9851.4359 - val_loss: 9822.6660\n",
      "Epoch 1508/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9850.9723 - val_loss: 9822.2041\n",
      "Epoch 1509/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9850.5095 - val_loss: 9821.7383\n",
      "Epoch 1510/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9850.0466 - val_loss: 9821.2764\n",
      "Epoch 1511/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9849.5825 - val_loss: 9820.8145\n",
      "Epoch 1512/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9849.1194 - val_loss: 9820.3506\n",
      "Epoch 1513/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9848.6574 - val_loss: 9819.8867\n",
      "Epoch 1514/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9848.1932 - val_loss: 9819.4248\n",
      "Epoch 1515/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9847.7304 - val_loss: 9818.9619\n",
      "Epoch 1516/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9847.2681 - val_loss: 9818.4971\n",
      "Epoch 1517/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9846.8038 - val_loss: 9818.0332\n",
      "Epoch 1518/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9846.3413 - val_loss: 9817.5713\n",
      "Epoch 1519/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9845.8779 - val_loss: 9817.1074\n",
      "Epoch 1520/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9845.4146 - val_loss: 9816.6445\n",
      "Epoch 1521/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9844.9525 - val_loss: 9816.1826\n",
      "Epoch 1522/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9844.4877 - val_loss: 9815.7197\n",
      "Epoch 1523/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9844.0253 - val_loss: 9815.2549\n",
      "Epoch 1524/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9843.5630 - val_loss: 9814.7930\n",
      "Epoch 1525/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9843.0984 - val_loss: 9814.3301\n",
      "Epoch 1526/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9842.6358 - val_loss: 9813.8662\n",
      "Epoch 1527/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9842.1733 - val_loss: 9813.4023\n",
      "Epoch 1528/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9841.7090 - val_loss: 9812.9395\n",
      "Epoch 1529/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9841.2465 - val_loss: 9812.4775\n",
      "Epoch 1530/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9840.7834 - val_loss: 9812.0127\n",
      "Epoch 1531/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9840.3194 - val_loss: 9811.5508\n",
      "Epoch 1532/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9839.8578 - val_loss: 9811.0889\n",
      "Epoch 1533/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9839.3939 - val_loss: 9810.6230\n",
      "Epoch 1534/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9838.9303 - val_loss: 9810.1602\n",
      "Epoch 1535/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9838.4682 - val_loss: 9809.6982\n",
      "Epoch 1536/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9838.0039 - val_loss: 9809.2354\n",
      "Epoch 1537/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9837.5412 - val_loss: 9808.7705\n",
      "Epoch 1538/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9837.0787 - val_loss: 9808.3076\n",
      "Epoch 1539/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9836.6149 - val_loss: 9807.8457\n",
      "Epoch 1540/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9836.1517 - val_loss: 9807.3818\n",
      "Epoch 1541/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9835.6891 - val_loss: 9806.9180\n",
      "Epoch 1542/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9835.2250 - val_loss: 9806.4561\n",
      "Epoch 1543/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9834.7632 - val_loss: 9805.9941\n",
      "Epoch 1544/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9834.2993 - val_loss: 9805.5293\n",
      "Epoch 1545/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9833.8352 - val_loss: 9805.0654\n",
      "Epoch 1546/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9833.3735 - val_loss: 9804.6035\n",
      "Epoch 1547/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9832.9097 - val_loss: 9804.1406\n",
      "Epoch 1548/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9832.4466 - val_loss: 9803.6758\n",
      "Epoch 1549/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9831.9838 - val_loss: 9803.2139\n",
      "Epoch 1550/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9831.5197 - val_loss: 9802.7520\n",
      "Epoch 1551/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9831.0569 - val_loss: 9802.2881\n",
      "Epoch 1552/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9830.5950 - val_loss: 9801.8242\n",
      "Epoch 1553/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9830.1304 - val_loss: 9801.3623\n",
      "Epoch 1554/3000\n",
      "742/742 [==============================] - 0s 139us/step - loss: 9829.6678 - val_loss: 9800.8984\n",
      "Epoch 1555/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9829.2053 - val_loss: 9800.4336\n",
      "Epoch 1556/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9828.7408 - val_loss: 9799.9707\n",
      "Epoch 1557/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9828.2785 - val_loss: 9799.5088\n",
      "Epoch 1558/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 9827.8153 - val_loss: 9799.0449\n",
      "Epoch 1559/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9827.3519 - val_loss: 9798.5820\n",
      "Epoch 1560/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9826.8895 - val_loss: 9798.1201\n",
      "Epoch 1561/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9826.4252 - val_loss: 9797.6562\n",
      "Epoch 1562/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 9825.9629 - val_loss: 9797.1924\n",
      "Epoch 1563/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9825.5003 - val_loss: 9796.7305\n",
      "Epoch 1564/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9825.0360 - val_loss: 9796.2676\n",
      "Epoch 1565/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9824.5728 - val_loss: 9795.8037\n",
      "Epoch 1566/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9824.1107 - val_loss: 9795.3389\n",
      "Epoch 1567/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9823.6468 - val_loss: 9794.8770\n",
      "Epoch 1568/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9823.1840 - val_loss: 9794.4141\n",
      "Epoch 1569/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9822.7210 - val_loss: 9793.9502\n",
      "Epoch 1570/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9822.2569 - val_loss: 9793.4883\n",
      "Epoch 1571/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9821.7948 - val_loss: 9793.0264\n",
      "Epoch 1572/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9821.3309 - val_loss: 9792.5605\n",
      "Epoch 1573/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9820.8677 - val_loss: 9792.0977\n",
      "Epoch 1574/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9820.4057 - val_loss: 9791.6357\n",
      "Epoch 1575/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9819.9410 - val_loss: 9791.1729\n",
      "Epoch 1576/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9819.4779 - val_loss: 9790.7080\n",
      "Epoch 1577/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9819.0162 - val_loss: 9790.2451\n",
      "Epoch 1578/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9818.5523 - val_loss: 9789.7832\n",
      "Epoch 1579/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9818.0893 - val_loss: 9789.3193\n",
      "Epoch 1580/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9817.6264 - val_loss: 9788.8555\n",
      "Epoch 1581/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9817.1627 - val_loss: 9788.3936\n",
      "Epoch 1582/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9816.7003 - val_loss: 9787.9316\n",
      "Epoch 1583/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9816.2368 - val_loss: 9787.4658\n",
      "Epoch 1584/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9815.7729 - val_loss: 9787.0029\n",
      "Epoch 1585/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9815.3110 - val_loss: 9786.5410\n",
      "Epoch 1586/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9814.8470 - val_loss: 9786.0771\n",
      "Epoch 1587/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9814.3838 - val_loss: 9785.6133\n",
      "Epoch 1588/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9813.9214 - val_loss: 9785.1514\n",
      "Epoch 1589/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9813.4574 - val_loss: 9784.6895\n",
      "Epoch 1590/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9812.9940 - val_loss: 9784.2236\n",
      "Epoch 1591/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9812.5320 - val_loss: 9783.7617\n",
      "Epoch 1592/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9812.0680 - val_loss: 9783.2998\n",
      "Epoch 1593/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9811.6050 - val_loss: 9782.8359\n",
      "Epoch 1594/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9811.1426 - val_loss: 9782.3711\n",
      "Epoch 1595/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9810.6781 - val_loss: 9781.9082\n",
      "Epoch 1596/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9810.2160 - val_loss: 9781.4463\n",
      "Epoch 1597/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9809.7529 - val_loss: 9780.9824\n",
      "Epoch 1598/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9809.2892 - val_loss: 9780.5195\n",
      "Epoch 1599/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9808.8266 - val_loss: 9780.0576\n",
      "Epoch 1600/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9808.3627 - val_loss: 9779.5938\n",
      "Epoch 1601/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9807.9002 - val_loss: 9779.1299\n",
      "Epoch 1602/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9807.4378 - val_loss: 9778.6670\n",
      "Epoch 1603/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9806.9730 - val_loss: 9778.2051\n",
      "Epoch 1604/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9806.5102 - val_loss: 9777.7412\n",
      "Epoch 1605/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9806.0483 - val_loss: 9777.2764\n",
      "Epoch 1606/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9805.5842 - val_loss: 9776.8145\n",
      "Epoch 1607/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9805.1215 - val_loss: 9776.3516\n",
      "Epoch 1608/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9804.6582 - val_loss: 9775.8877\n",
      "Epoch 1609/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9804.1946 - val_loss: 9775.4258\n",
      "Epoch 1610/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9803.7320 - val_loss: 9774.9629\n",
      "Epoch 1611/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9803.2685 - val_loss: 9774.4980\n",
      "Epoch 1612/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9802.8050 - val_loss: 9774.0342\n",
      "Epoch 1613/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9802.3432 - val_loss: 9773.5732\n",
      "Epoch 1614/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9801.8783 - val_loss: 9773.1094\n",
      "Epoch 1615/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9801.4156 - val_loss: 9772.6455\n",
      "Epoch 1616/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9800.9537 - val_loss: 9772.1826\n",
      "Epoch 1617/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9800.4893 - val_loss: 9771.7207\n",
      "Epoch 1618/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9800.0262 - val_loss: 9771.2559\n",
      "Epoch 1619/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9799.5640 - val_loss: 9770.7930\n",
      "Epoch 1620/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9799.1000 - val_loss: 9770.3311\n",
      "Epoch 1621/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9798.6377 - val_loss: 9769.8691\n",
      "Epoch 1622/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9798.1741 - val_loss: 9769.4033\n",
      "Epoch 1623/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9797.7102 - val_loss: 9768.9404\n",
      "Epoch 1624/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9797.2485 - val_loss: 9768.4785\n",
      "Epoch 1625/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9796.7845 - val_loss: 9768.0137\n",
      "Epoch 1626/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9796.3210 - val_loss: 9767.5508\n",
      "Epoch 1627/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9795.8588 - val_loss: 9767.0889\n",
      "Epoch 1628/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9795.3948 - val_loss: 9766.6270\n",
      "Epoch 1629/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9794.9315 - val_loss: 9766.1611\n",
      "Epoch 1630/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9794.4693 - val_loss: 9765.6992\n",
      "Epoch 1631/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9794.0052 - val_loss: 9765.2363\n",
      "Epoch 1632/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9793.5426 - val_loss: 9764.7734\n",
      "Epoch 1633/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9793.0799 - val_loss: 9764.3076\n",
      "Epoch 1634/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9792.6158 - val_loss: 9763.8457\n",
      "Epoch 1635/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9792.1536 - val_loss: 9763.3838\n",
      "Epoch 1636/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9791.6902 - val_loss: 9762.9199\n",
      "Epoch 1637/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9791.2265 - val_loss: 9762.4570\n",
      "Epoch 1638/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9790.7642 - val_loss: 9761.9941\n",
      "Epoch 1639/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9790.3002 - val_loss: 9761.5312\n",
      "Epoch 1640/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9789.8376 - val_loss: 9761.0674\n",
      "Epoch 1641/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9789.3751 - val_loss: 9760.6045\n",
      "Epoch 1642/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9788.9105 - val_loss: 9760.1426\n",
      "Epoch 1643/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9788.4480 - val_loss: 9759.6787\n",
      "Epoch 1644/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9787.9855 - val_loss: 9759.2139\n",
      "Epoch 1645/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9787.5216 - val_loss: 9758.7520\n",
      "Epoch 1646/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 9787.0588 - val_loss: 9758.2891\n",
      "Epoch 1647/3000\n",
      "742/742 [==============================] - 0s 139us/step - loss: 9786.5957 - val_loss: 9757.8252\n",
      "Epoch 1648/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 138us/step - loss: 9786.1316 - val_loss: 9757.3633\n",
      "Epoch 1649/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9785.6697 - val_loss: 9756.9004\n",
      "Epoch 1650/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9785.2057 - val_loss: 9756.4355\n",
      "Epoch 1651/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9784.7425 - val_loss: 9755.9717\n",
      "Epoch 1652/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9784.2807 - val_loss: 9755.5088\n",
      "Epoch 1653/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9783.8159 - val_loss: 9755.0469\n",
      "Epoch 1654/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9783.3532 - val_loss: 9754.5830\n",
      "Epoch 1655/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9782.8910 - val_loss: 9754.1201\n",
      "Epoch 1656/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9782.4267 - val_loss: 9753.6582\n",
      "Epoch 1657/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9781.9635 - val_loss: 9753.1934\n",
      "Epoch 1658/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9781.5014 - val_loss: 9752.7305\n",
      "Epoch 1659/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9781.0376 - val_loss: 9752.2686\n",
      "Epoch 1660/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9780.5749 - val_loss: 9751.8047\n",
      "Epoch 1661/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9780.1113 - val_loss: 9751.3408\n",
      "Epoch 1662/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9779.6474 - val_loss: 9750.8779\n",
      "Epoch 1663/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9779.1858 - val_loss: 9750.4150\n",
      "Epoch 1664/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9778.7214 - val_loss: 9749.9512\n",
      "Epoch 1665/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9778.2582 - val_loss: 9749.4883\n",
      "Epoch 1666/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9777.7964 - val_loss: 9749.0264\n",
      "Epoch 1667/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9777.3320 - val_loss: 9748.5625\n",
      "Epoch 1668/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9776.8689 - val_loss: 9748.0986\n",
      "Epoch 1669/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9776.4068 - val_loss: 9747.6367\n",
      "Epoch 1670/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9775.9428 - val_loss: 9747.1738\n",
      "Epoch 1671/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9775.4798 - val_loss: 9746.7109\n",
      "Epoch 1672/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9775.0171 - val_loss: 9746.2451\n",
      "Epoch 1673/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9774.5529 - val_loss: 9745.7832\n",
      "Epoch 1674/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9774.0908 - val_loss: 9745.3213\n",
      "Epoch 1675/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9773.6279 - val_loss: 9744.8574\n",
      "Epoch 1676/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9773.1638 - val_loss: 9744.3945\n",
      "Epoch 1677/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9772.7016 - val_loss: 9743.9316\n",
      "Epoch 1678/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9772.2376 - val_loss: 9743.4688\n",
      "Epoch 1679/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9771.7744 - val_loss: 9743.0039\n",
      "Epoch 1680/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9771.3120 - val_loss: 9742.5420\n",
      "Epoch 1681/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9770.8477 - val_loss: 9742.0801\n",
      "Epoch 1682/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9770.3855 - val_loss: 9741.6162\n",
      "Epoch 1683/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9769.9228 - val_loss: 9741.1514\n",
      "Epoch 1684/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9769.4585 - val_loss: 9740.6895\n",
      "Epoch 1685/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9768.9961 - val_loss: 9740.2266\n",
      "Epoch 1686/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9768.5333 - val_loss: 9739.7627\n",
      "Epoch 1687/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9768.0689 - val_loss: 9739.2998\n",
      "Epoch 1688/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9767.6066 - val_loss: 9738.8379\n",
      "Epoch 1689/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9767.1432 - val_loss: 9738.3730\n",
      "Epoch 1690/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9766.6800 - val_loss: 9737.9092\n",
      "Epoch 1691/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9766.2179 - val_loss: 9737.4463\n",
      "Epoch 1692/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9765.7533 - val_loss: 9736.9844\n",
      "Epoch 1693/3000\n",
      "742/742 [==============================] - 0s 107us/step - loss: 9765.2904 - val_loss: 9736.5205\n",
      "Epoch 1694/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9764.8286 - val_loss: 9736.0576\n",
      "Epoch 1695/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9764.3643 - val_loss: 9735.5957\n",
      "Epoch 1696/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9763.9007 - val_loss: 9735.1309\n",
      "Epoch 1697/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9763.4386 - val_loss: 9734.6680\n",
      "Epoch 1698/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9762.9744 - val_loss: 9734.2051\n",
      "Epoch 1699/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9762.5119 - val_loss: 9733.7422\n",
      "Epoch 1700/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9762.0488 - val_loss: 9733.2783\n",
      "Epoch 1701/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9761.5850 - val_loss: 9732.8154\n",
      "Epoch 1702/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9761.1231 - val_loss: 9732.3525\n",
      "Epoch 1703/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9760.6592 - val_loss: 9731.8887\n",
      "Epoch 1704/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9760.1956 - val_loss: 9731.4258\n",
      "Epoch 1705/3000\n",
      "742/742 [==============================] - 0s 164us/step - loss: 9759.7336 - val_loss: 9730.9639\n",
      "Epoch 1706/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9759.2692 - val_loss: 9730.5000\n",
      "Epoch 1707/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9758.8059 - val_loss: 9730.0361\n",
      "Epoch 1708/3000\n",
      "742/742 [==============================] - 0s 177us/step - loss: 9758.3443 - val_loss: 9729.5742\n",
      "Epoch 1709/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9757.8804 - val_loss: 9729.1113\n",
      "Epoch 1710/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9757.4173 - val_loss: 9728.6475\n",
      "Epoch 1711/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9756.9546 - val_loss: 9728.1826\n",
      "Epoch 1712/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9756.4904 - val_loss: 9727.7207\n",
      "Epoch 1713/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9756.0283 - val_loss: 9727.2588\n",
      "Epoch 1714/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9755.5650 - val_loss: 9726.7939\n",
      "Epoch 1715/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9755.1009 - val_loss: 9726.3320\n",
      "Epoch 1716/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9754.6391 - val_loss: 9725.8691\n",
      "Epoch 1717/3000\n",
      "742/742 [==============================] - 0s 152us/step - loss: 9754.1751 - val_loss: 9725.4062\n",
      "Epoch 1718/3000\n",
      "742/742 [==============================] - 0s 188us/step - loss: 9753.7119 - val_loss: 9724.9404\n",
      "Epoch 1719/3000\n",
      "742/742 [==============================] - 0s 159us/step - loss: 9753.2495 - val_loss: 9724.4795\n",
      "Epoch 1720/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9752.7852 - val_loss: 9724.0176\n",
      "Epoch 1721/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9752.3223 - val_loss: 9723.5537\n",
      "Epoch 1722/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9751.8602 - val_loss: 9723.0889\n",
      "Epoch 1723/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9751.3957 - val_loss: 9722.6270\n",
      "Epoch 1724/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9750.9335 - val_loss: 9722.1641\n",
      "Epoch 1725/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9750.4704 - val_loss: 9721.6992\n",
      "Epoch 1726/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9750.0063 - val_loss: 9721.2373\n",
      "Epoch 1727/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9749.5442 - val_loss: 9720.7754\n",
      "Epoch 1728/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9749.0807 - val_loss: 9720.3105\n",
      "Epoch 1729/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9748.6170 - val_loss: 9719.8467\n",
      "Epoch 1730/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9748.1550 - val_loss: 9719.3838\n",
      "Epoch 1731/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9747.6905 - val_loss: 9718.9219\n",
      "Epoch 1732/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9747.2279 - val_loss: 9718.4580\n",
      "Epoch 1733/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9746.7660 - val_loss: 9717.9951\n",
      "Epoch 1734/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9746.3012 - val_loss: 9717.5332\n",
      "Epoch 1735/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9745.8382 - val_loss: 9717.0684\n",
      "Epoch 1736/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9745.3762 - val_loss: 9716.6055\n",
      "Epoch 1737/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9744.9118 - val_loss: 9716.1436\n",
      "Epoch 1738/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9744.4496 - val_loss: 9715.6797\n",
      "Epoch 1739/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9743.9863 - val_loss: 9715.2158\n",
      "Epoch 1740/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9743.5222 - val_loss: 9714.7529\n",
      "Epoch 1741/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9743.0602 - val_loss: 9714.2900\n",
      "Epoch 1742/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9742.5966 - val_loss: 9713.8262\n",
      "Epoch 1743/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9742.1328 - val_loss: 9713.3633\n",
      "Epoch 1744/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9741.6714 - val_loss: 9712.9014\n",
      "Epoch 1745/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9741.2067 - val_loss: 9712.4375\n",
      "Epoch 1746/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9740.7437 - val_loss: 9711.9736\n",
      "Epoch 1747/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9740.2818 - val_loss: 9711.5117\n",
      "Epoch 1748/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9739.8177 - val_loss: 9711.0488\n",
      "Epoch 1749/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9739.3546 - val_loss: 9710.5850\n",
      "Epoch 1750/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9738.8920 - val_loss: 9710.1201\n",
      "Epoch 1751/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9738.4277 - val_loss: 9709.6582\n",
      "Epoch 1752/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9737.9660 - val_loss: 9709.1953\n",
      "Epoch 1753/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9737.5020 - val_loss: 9708.7314\n",
      "Epoch 1754/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9737.0383 - val_loss: 9708.2695\n",
      "Epoch 1755/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9736.5763 - val_loss: 9707.8066\n",
      "Epoch 1756/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9736.1123 - val_loss: 9707.3418\n",
      "Epoch 1757/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9735.6488 - val_loss: 9706.8779\n",
      "Epoch 1758/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9735.1870 - val_loss: 9706.4160\n",
      "Epoch 1759/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9734.7227 - val_loss: 9705.9551\n",
      "Epoch 1760/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9734.2599 - val_loss: 9705.4893\n",
      "Epoch 1761/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9733.7975 - val_loss: 9705.0264\n",
      "Epoch 1762/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9733.3331 - val_loss: 9704.5645\n",
      "Epoch 1763/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9732.8707 - val_loss: 9704.1016\n",
      "Epoch 1764/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9732.4080 - val_loss: 9703.6367\n",
      "Epoch 1765/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9731.9435 - val_loss: 9703.1748\n",
      "Epoch 1766/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9731.4816 - val_loss: 9702.7129\n",
      "Epoch 1767/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9731.0184 - val_loss: 9702.2480\n",
      "Epoch 1768/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9730.5542 - val_loss: 9701.7842\n",
      "Epoch 1769/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9730.0921 - val_loss: 9701.3213\n",
      "Epoch 1770/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9729.6281 - val_loss: 9700.8594\n",
      "Epoch 1771/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9729.1655 - val_loss: 9700.3945\n",
      "Epoch 1772/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9728.7033 - val_loss: 9699.9326\n",
      "Epoch 1773/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9728.2382 - val_loss: 9699.4707\n",
      "Epoch 1774/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9727.7758 - val_loss: 9699.0059\n",
      "Epoch 1775/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9727.3137 - val_loss: 9698.5430\n",
      "Epoch 1776/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9726.8489 - val_loss: 9698.0801\n",
      "Epoch 1777/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9726.3864 - val_loss: 9697.6172\n",
      "Epoch 1778/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9725.9239 - val_loss: 9697.1533\n",
      "Epoch 1779/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9725.4598 - val_loss: 9696.6904\n",
      "Epoch 1780/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9724.9977 - val_loss: 9696.2275\n",
      "Epoch 1781/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9724.5340 - val_loss: 9695.7637\n",
      "Epoch 1782/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9724.0704 - val_loss: 9695.3008\n",
      "Epoch 1783/3000\n",
      "742/742 [==============================] - 0s 88us/step - loss: 9723.6084 - val_loss: 9694.8379\n",
      "Epoch 1784/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9723.1440 - val_loss: 9694.3750\n",
      "Epoch 1785/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9722.6810 - val_loss: 9693.9111\n",
      "Epoch 1786/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9722.2192 - val_loss: 9693.4492\n",
      "Epoch 1787/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9721.7551 - val_loss: 9692.9863\n",
      "Epoch 1788/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9721.2920 - val_loss: 9692.5225\n",
      "Epoch 1789/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9720.8294 - val_loss: 9692.0576\n",
      "Epoch 1790/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9720.3653 - val_loss: 9691.5957\n",
      "Epoch 1791/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9719.9031 - val_loss: 9691.1328\n",
      "Epoch 1792/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9719.4394 - val_loss: 9690.6689\n",
      "Epoch 1793/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 103us/step - loss: 9718.9760 - val_loss: 9690.2070\n",
      "Epoch 1794/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9718.5141 - val_loss: 9689.7441\n",
      "Epoch 1795/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9718.0495 - val_loss: 9689.2793\n",
      "Epoch 1796/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9717.5860 - val_loss: 9688.8154\n",
      "Epoch 1797/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9717.1244 - val_loss: 9688.3535\n",
      "Epoch 1798/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9716.6599 - val_loss: 9687.8926\n",
      "Epoch 1799/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9716.1971 - val_loss: 9687.4268\n",
      "Epoch 1800/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9715.7346 - val_loss: 9686.9639\n",
      "Epoch 1801/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9715.2707 - val_loss: 9686.5020\n",
      "Epoch 1802/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9714.8083 - val_loss: 9686.0391\n",
      "Epoch 1803/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9714.3455 - val_loss: 9685.5742\n",
      "Epoch 1804/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9713.8810 - val_loss: 9685.1113\n",
      "Epoch 1805/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9713.4191 - val_loss: 9684.6504\n",
      "Epoch 1806/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9712.9554 - val_loss: 9684.1855\n",
      "Epoch 1807/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9712.4914 - val_loss: 9683.7217\n",
      "Epoch 1808/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9712.0296 - val_loss: 9683.2588\n",
      "Epoch 1809/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9711.5655 - val_loss: 9682.7969\n",
      "Epoch 1810/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9711.1026 - val_loss: 9682.3320\n",
      "Epoch 1811/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9710.6401 - val_loss: 9681.8691\n",
      "Epoch 1812/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9710.1758 - val_loss: 9681.4082\n",
      "Epoch 1813/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9709.7132 - val_loss: 9680.9434\n",
      "Epoch 1814/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9709.2511 - val_loss: 9680.4805\n",
      "Epoch 1815/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9708.7863 - val_loss: 9680.0176\n",
      "Epoch 1816/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9708.3240 - val_loss: 9679.5547\n",
      "Epoch 1817/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9707.8611 - val_loss: 9679.0908\n",
      "Epoch 1818/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9707.3973 - val_loss: 9678.6270\n",
      "Epoch 1819/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9706.9347 - val_loss: 9678.1650\n",
      "Epoch 1820/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9706.4714 - val_loss: 9677.7012\n",
      "Epoch 1821/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9706.0078 - val_loss: 9677.2383\n",
      "Epoch 1822/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9705.5460 - val_loss: 9676.7754\n",
      "Epoch 1823/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9705.0814 - val_loss: 9676.3125\n",
      "Epoch 1824/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9704.6186 - val_loss: 9675.8486\n",
      "Epoch 1825/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9704.1568 - val_loss: 9675.3867\n",
      "Epoch 1826/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9703.6922 - val_loss: 9674.9238\n",
      "Epoch 1827/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9703.2288 - val_loss: 9674.4600\n",
      "Epoch 1828/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9702.7670 - val_loss: 9673.9951\n",
      "Epoch 1829/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9702.3023 - val_loss: 9673.5332\n",
      "Epoch 1830/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9701.8399 - val_loss: 9673.0703\n",
      "Epoch 1831/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9701.3769 - val_loss: 9672.6064\n",
      "Epoch 1832/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9700.9130 - val_loss: 9672.1445\n",
      "Epoch 1833/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9700.4513 - val_loss: 9671.6816\n",
      "Epoch 1834/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9699.9871 - val_loss: 9671.2168\n",
      "Epoch 1835/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9699.5236 - val_loss: 9670.7529\n",
      "Epoch 1836/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9699.0620 - val_loss: 9670.2910\n",
      "Epoch 1837/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9698.5974 - val_loss: 9669.8301\n",
      "Epoch 1838/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9698.1346 - val_loss: 9669.3643\n",
      "Epoch 1839/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9697.6723 - val_loss: 9668.9014\n",
      "Epoch 1840/3000\n",
      "742/742 [==============================] - ETA: 0s - loss: 9691.62 - 0s 125us/step - loss: 9697.2081 - val_loss: 9668.4395\n",
      "Epoch 1841/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9696.7456 - val_loss: 9667.9746\n",
      "Epoch 1842/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9696.2826 - val_loss: 9667.5117\n",
      "Epoch 1843/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9695.8185 - val_loss: 9667.0488\n",
      "Epoch 1844/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9695.3566 - val_loss: 9666.5879\n",
      "Epoch 1845/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9694.8928 - val_loss: 9666.1230\n",
      "Epoch 1846/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9694.4288 - val_loss: 9665.6592\n",
      "Epoch 1847/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9693.9672 - val_loss: 9665.1963\n",
      "Epoch 1848/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9693.5031 - val_loss: 9664.7344\n",
      "Epoch 1849/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9693.0399 - val_loss: 9664.2695\n",
      "Epoch 1850/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9692.5776 - val_loss: 9663.8066\n",
      "Epoch 1851/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9692.1131 - val_loss: 9663.3457\n",
      "Epoch 1852/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9691.6507 - val_loss: 9662.8809\n",
      "Epoch 1853/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9691.1884 - val_loss: 9662.4180\n",
      "Epoch 1854/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9690.7237 - val_loss: 9661.9551\n",
      "Epoch 1855/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9690.2613 - val_loss: 9661.4922\n",
      "Epoch 1856/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9689.7986 - val_loss: 9661.0283\n",
      "Epoch 1857/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9689.3347 - val_loss: 9660.5654\n",
      "Epoch 1858/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9688.8727 - val_loss: 9660.1025\n",
      "Epoch 1859/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9688.4086 - val_loss: 9659.6377\n",
      "Epoch 1860/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9687.9454 - val_loss: 9659.1758\n",
      "Epoch 1861/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9687.4829 - val_loss: 9658.7129\n",
      "Epoch 1862/3000\n",
      "742/742 [==============================] - 0s 131us/step - loss: 9687.0187 - val_loss: 9658.2500\n",
      "Epoch 1863/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9686.5561 - val_loss: 9657.7842\n",
      "Epoch 1864/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9686.0940 - val_loss: 9657.3242\n",
      "Epoch 1865/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9685.6292 - val_loss: 9656.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1866/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9685.1667 - val_loss: 9656.3975\n",
      "Epoch 1867/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9684.7044 - val_loss: 9655.9326\n",
      "Epoch 1868/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9684.2396 - val_loss: 9655.4707\n",
      "Epoch 1869/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9683.7769 - val_loss: 9655.0078\n",
      "Epoch 1870/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9683.3143 - val_loss: 9654.5439\n",
      "Epoch 1871/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9682.8504 - val_loss: 9654.0801\n",
      "Epoch 1872/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9682.3883 - val_loss: 9653.6191\n",
      "Epoch 1873/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9681.9244 - val_loss: 9653.1543\n",
      "Epoch 1874/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9681.4609 - val_loss: 9652.6904\n",
      "Epoch 1875/3000\n",
      "742/742 [==============================] - 0s 107us/step - loss: 9680.9995 - val_loss: 9652.2285\n",
      "Epoch 1876/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9680.5347 - val_loss: 9651.7676\n",
      "Epoch 1877/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9680.0720 - val_loss: 9651.3018\n",
      "Epoch 1878/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 9679.6096 - val_loss: 9650.8389\n",
      "Epoch 1879/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9679.1455 - val_loss: 9650.3770\n",
      "Epoch 1880/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 9678.6826 - val_loss: 9649.9121\n",
      "Epoch 1881/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9678.2200 - val_loss: 9649.4492\n",
      "Epoch 1882/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9677.7561 - val_loss: 9648.9863\n",
      "Epoch 1883/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9677.2936 - val_loss: 9648.5254\n",
      "Epoch 1884/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9676.8302 - val_loss: 9648.0596\n",
      "Epoch 1885/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9676.3664 - val_loss: 9647.5967\n",
      "Epoch 1886/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9675.9047 - val_loss: 9647.1338\n",
      "Epoch 1887/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 9675.4404 - val_loss: 9646.6719\n",
      "Epoch 1888/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9674.9772 - val_loss: 9646.2070\n",
      "Epoch 1889/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9674.5149 - val_loss: 9645.7441\n",
      "Epoch 1890/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9674.0506 - val_loss: 9645.2832\n",
      "Epoch 1891/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9673.5882 - val_loss: 9644.8184\n",
      "Epoch 1892/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9673.1259 - val_loss: 9644.3555\n",
      "Epoch 1893/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9672.6613 - val_loss: 9643.8926\n",
      "Epoch 1894/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9672.1987 - val_loss: 9643.4297\n",
      "Epoch 1895/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9671.7362 - val_loss: 9642.9658\n",
      "Epoch 1896/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9671.2722 - val_loss: 9642.5020\n",
      "Epoch 1897/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9670.8096 - val_loss: 9642.0400\n",
      "Epoch 1898/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9670.3463 - val_loss: 9641.5752\n",
      "Epoch 1899/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9669.8827 - val_loss: 9641.1133\n",
      "Epoch 1900/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9669.4200 - val_loss: 9640.6504\n",
      "Epoch 1901/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9668.9562 - val_loss: 9640.1875\n",
      "Epoch 1902/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9668.4935 - val_loss: 9639.7217\n",
      "Epoch 1903/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9668.0313 - val_loss: 9639.2598\n",
      "Epoch 1904/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9667.5666 - val_loss: 9638.7988\n",
      "Epoch 1905/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9667.1037 - val_loss: 9638.3350\n",
      "Epoch 1906/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9666.6418 - val_loss: 9637.8701\n",
      "Epoch 1907/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9666.1769 - val_loss: 9637.4082\n",
      "Epoch 1908/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9665.7145 - val_loss: 9636.9453\n",
      "Epoch 1909/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9665.2515 - val_loss: 9636.4814\n",
      "Epoch 1910/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9664.7881 - val_loss: 9636.0176\n",
      "Epoch 1911/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9664.3257 - val_loss: 9635.5566\n",
      "Epoch 1912/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9663.8617 - val_loss: 9635.0918\n",
      "Epoch 1913/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9663.3984 - val_loss: 9634.6279\n",
      "Epoch 1914/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9662.9366 - val_loss: 9634.1660\n",
      "Epoch 1915/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9662.4718 - val_loss: 9633.7051\n",
      "Epoch 1916/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9662.0093 - val_loss: 9633.2393\n",
      "Epoch 1917/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9661.5473 - val_loss: 9632.7764\n",
      "Epoch 1918/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9661.0824 - val_loss: 9632.3145\n",
      "Epoch 1919/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9660.6197 - val_loss: 9631.8496\n",
      "Epoch 1920/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9660.1574 - val_loss: 9631.3867\n",
      "Epoch 1921/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9659.6931 - val_loss: 9630.9238\n",
      "Epoch 1922/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9659.2309 - val_loss: 9630.4609\n",
      "Epoch 1923/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9658.7673 - val_loss: 9629.9971\n",
      "Epoch 1924/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9658.3035 - val_loss: 9629.5342\n",
      "Epoch 1925/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9657.8421 - val_loss: 9629.0713\n",
      "Epoch 1926/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9657.3779 - val_loss: 9628.6094\n",
      "Epoch 1927/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9656.9147 - val_loss: 9628.1445\n",
      "Epoch 1928/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9656.4525 - val_loss: 9627.6816\n",
      "Epoch 1929/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9655.9881 - val_loss: 9627.2207\n",
      "Epoch 1930/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9655.5256 - val_loss: 9626.7549\n",
      "Epoch 1931/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9655.0628 - val_loss: 9626.2930\n",
      "Epoch 1932/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9654.5988 - val_loss: 9625.8301\n",
      "Epoch 1933/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9654.1364 - val_loss: 9625.3672\n",
      "Epoch 1934/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9653.6735 - val_loss: 9624.9014\n",
      "Epoch 1935/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9653.2091 - val_loss: 9624.4395\n",
      "Epoch 1936/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9652.7470 - val_loss: 9623.9775\n",
      "Epoch 1937/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9652.2833 - val_loss: 9623.5127\n",
      "Epoch 1938/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9651.8199 - val_loss: 9623.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1939/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9651.3576 - val_loss: 9622.5879\n",
      "Epoch 1940/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9650.8937 - val_loss: 9622.1250\n",
      "Epoch 1941/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9650.4306 - val_loss: 9621.6592\n",
      "Epoch 1942/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9649.9685 - val_loss: 9621.1963\n",
      "Epoch 1943/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9649.5037 - val_loss: 9620.7363\n",
      "Epoch 1944/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9649.0412 - val_loss: 9620.2725\n",
      "Epoch 1945/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9648.5791 - val_loss: 9619.8076\n",
      "Epoch 1946/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9648.1142 - val_loss: 9619.3457\n",
      "Epoch 1947/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9647.6520 - val_loss: 9618.8828\n",
      "Epoch 1948/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9647.1893 - val_loss: 9618.4189\n",
      "Epoch 1949/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9646.7250 - val_loss: 9617.9551\n",
      "Epoch 1950/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9646.2635 - val_loss: 9617.4941\n",
      "Epoch 1951/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9645.7993 - val_loss: 9617.0283\n",
      "Epoch 1952/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9645.3358 - val_loss: 9616.5654\n",
      "Epoch 1953/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9644.8739 - val_loss: 9616.1035\n",
      "Epoch 1954/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9644.4093 - val_loss: 9615.6406\n",
      "Epoch 1955/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9643.9469 - val_loss: 9615.1768\n",
      "Epoch 1956/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9643.4845 - val_loss: 9614.7139\n",
      "Epoch 1957/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9643.0198 - val_loss: 9614.2520\n",
      "Epoch 1958/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9642.5570 - val_loss: 9613.7871\n",
      "Epoch 1959/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9642.0950 - val_loss: 9613.3242\n",
      "Epoch 1960/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9641.6309 - val_loss: 9612.8613\n",
      "Epoch 1961/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9641.1682 - val_loss: 9612.3984\n",
      "Epoch 1962/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9640.7049 - val_loss: 9611.9346\n",
      "Epoch 1963/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9640.2410 - val_loss: 9611.4717\n",
      "Epoch 1964/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9639.7797 - val_loss: 9611.0088\n",
      "Epoch 1965/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9639.3154 - val_loss: 9610.5449\n",
      "Epoch 1966/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9638.8519 - val_loss: 9610.0820\n",
      "Epoch 1967/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9638.3896 - val_loss: 9609.6191\n",
      "Epoch 1968/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9637.9256 - val_loss: 9609.1562\n",
      "Epoch 1969/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9637.4626 - val_loss: 9608.6924\n",
      "Epoch 1970/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9637.0002 - val_loss: 9608.2305\n",
      "Epoch 1971/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9636.5361 - val_loss: 9607.7676\n",
      "Epoch 1972/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9636.0730 - val_loss: 9607.3047\n",
      "Epoch 1973/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9635.6105 - val_loss: 9606.8389\n",
      "Epoch 1974/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9635.1466 - val_loss: 9606.3770\n",
      "Epoch 1975/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9634.6847 - val_loss: 9605.9150\n",
      "Epoch 1976/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9634.2208 - val_loss: 9605.4502\n",
      "Epoch 1977/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9633.7567 - val_loss: 9604.9883\n",
      "Epoch 1978/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9633.2951 - val_loss: 9604.5254\n",
      "Epoch 1979/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9632.8312 - val_loss: 9604.0625\n",
      "Epoch 1980/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9632.3682 - val_loss: 9603.5967\n",
      "Epoch 1981/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9631.9058 - val_loss: 9603.1338\n",
      "Epoch 1982/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9631.4414 - val_loss: 9602.6738\n",
      "Epoch 1983/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9630.9788 - val_loss: 9602.2100\n",
      "Epoch 1984/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9630.5166 - val_loss: 9601.7451\n",
      "Epoch 1985/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9630.0517 - val_loss: 9601.2832\n",
      "Epoch 1986/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9629.5894 - val_loss: 9600.8203\n",
      "Epoch 1987/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9629.1264 - val_loss: 9600.3564\n",
      "Epoch 1988/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9628.6624 - val_loss: 9599.8926\n",
      "Epoch 1989/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9628.2005 - val_loss: 9599.4316\n",
      "Epoch 1990/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9627.7367 - val_loss: 9598.9658\n",
      "Epoch 1991/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9627.2731 - val_loss: 9598.5029\n",
      "Epoch 1992/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9626.8112 - val_loss: 9598.0410\n",
      "Epoch 1993/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9626.3466 - val_loss: 9597.5781\n",
      "Epoch 1994/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9625.8842 - val_loss: 9597.1143\n",
      "Epoch 1995/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9625.4222 - val_loss: 9596.6514\n",
      "Epoch 1996/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9624.9571 - val_loss: 9596.1895\n",
      "Epoch 1997/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9624.4946 - val_loss: 9595.7246\n",
      "Epoch 1998/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9624.0323 - val_loss: 9595.2617\n",
      "Epoch 1999/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9623.5682 - val_loss: 9594.7988\n",
      "Epoch 2000/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9623.1060 - val_loss: 9594.3359\n",
      "Epoch 2001/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9622.6425 - val_loss: 9593.8721\n",
      "Epoch 2002/3000\n",
      "742/742 [==============================] - 0s 161us/step - loss: 9622.1783 - val_loss: 9593.4092\n",
      "Epoch 2003/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9621.7169 - val_loss: 9592.9463\n",
      "Epoch 2004/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9621.2527 - val_loss: 9592.4824\n",
      "Epoch 2005/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9620.7892 - val_loss: 9592.0195\n",
      "Epoch 2006/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9620.3274 - val_loss: 9591.5566\n",
      "Epoch 2007/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9619.8628 - val_loss: 9591.0938\n",
      "Epoch 2008/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9619.3998 - val_loss: 9590.6299\n",
      "Epoch 2009/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9618.9378 - val_loss: 9590.1680\n",
      "Epoch 2010/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9618.4736 - val_loss: 9589.7051\n",
      "Epoch 2011/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9618.0104 - val_loss: 9589.2412\n",
      "Epoch 2012/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 101us/step - loss: 9617.5483 - val_loss: 9588.7764\n",
      "Epoch 2013/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9617.0841 - val_loss: 9588.3145\n",
      "Epoch 2014/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9616.6222 - val_loss: 9587.8525\n",
      "Epoch 2015/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9616.1583 - val_loss: 9587.3877\n",
      "Epoch 2016/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9615.6942 - val_loss: 9586.9258\n",
      "Epoch 2017/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9615.2324 - val_loss: 9586.4629\n",
      "Epoch 2018/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9614.7687 - val_loss: 9585.9980\n",
      "Epoch 2019/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9614.3053 - val_loss: 9585.5342\n",
      "Epoch 2020/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9613.8432 - val_loss: 9585.0713\n",
      "Epoch 2021/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9613.3786 - val_loss: 9584.6113\n",
      "Epoch 2022/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9612.9161 - val_loss: 9584.1475\n",
      "Epoch 2023/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9612.4544 - val_loss: 9583.6826\n",
      "Epoch 2024/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9611.9893 - val_loss: 9583.2207\n",
      "Epoch 2025/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9611.5269 - val_loss: 9582.7578\n",
      "Epoch 2026/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9611.0640 - val_loss: 9582.2930\n",
      "Epoch 2027/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9610.5999 - val_loss: 9581.8301\n",
      "Epoch 2028/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9610.1379 - val_loss: 9581.3691\n",
      "Epoch 2029/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9609.6742 - val_loss: 9580.9033\n",
      "Epoch 2030/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9609.2104 - val_loss: 9580.4404\n",
      "Epoch 2031/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9608.7482 - val_loss: 9579.9785\n",
      "Epoch 2032/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9608.2842 - val_loss: 9579.5156\n",
      "Epoch 2033/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9607.8216 - val_loss: 9579.0518\n",
      "Epoch 2034/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9607.3593 - val_loss: 9578.5889\n",
      "Epoch 2035/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9606.8946 - val_loss: 9578.1270\n",
      "Epoch 2036/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9606.4319 - val_loss: 9577.6621\n",
      "Epoch 2037/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9605.9699 - val_loss: 9577.1992\n",
      "Epoch 2038/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9605.5058 - val_loss: 9576.7363\n",
      "Epoch 2039/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9605.0428 - val_loss: 9576.2734\n",
      "Epoch 2040/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9604.5799 - val_loss: 9575.8096\n",
      "Epoch 2041/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9604.1160 - val_loss: 9575.3467\n",
      "Epoch 2042/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9603.6539 - val_loss: 9574.8838\n",
      "Epoch 2043/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9603.1900 - val_loss: 9574.4199\n",
      "Epoch 2044/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9602.7266 - val_loss: 9573.9570\n",
      "Epoch 2045/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9602.2649 - val_loss: 9573.4941\n",
      "Epoch 2046/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9601.8002 - val_loss: 9573.0312\n",
      "Epoch 2047/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9601.3372 - val_loss: 9572.5674\n",
      "Epoch 2048/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9600.8753 - val_loss: 9572.1055\n",
      "Epoch 2049/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9600.4112 - val_loss: 9571.6426\n",
      "Epoch 2050/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9599.9476 - val_loss: 9571.1787\n",
      "Epoch 2051/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9599.4856 - val_loss: 9570.7139\n",
      "Epoch 2052/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9599.0214 - val_loss: 9570.2520\n",
      "Epoch 2053/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9598.5594 - val_loss: 9569.7900\n",
      "Epoch 2054/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9598.0958 - val_loss: 9569.3252\n",
      "Epoch 2055/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9597.6316 - val_loss: 9568.8633\n",
      "Epoch 2056/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9597.1700 - val_loss: 9568.4004\n",
      "Epoch 2057/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9596.7056 - val_loss: 9567.9355\n",
      "Epoch 2058/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9596.2424 - val_loss: 9567.4717\n",
      "Epoch 2059/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9595.7806 - val_loss: 9567.0088\n",
      "Epoch 2060/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9595.3162 - val_loss: 9566.5488\n",
      "Epoch 2061/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9594.8532 - val_loss: 9566.0850\n",
      "Epoch 2062/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9594.3916 - val_loss: 9565.6201\n",
      "Epoch 2063/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9593.9267 - val_loss: 9565.1582\n",
      "Epoch 2064/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9593.4644 - val_loss: 9564.6953\n",
      "Epoch 2065/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9593.0012 - val_loss: 9564.2305\n",
      "Epoch 2066/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9592.5371 - val_loss: 9563.7676\n",
      "Epoch 2067/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9592.0753 - val_loss: 9563.3066\n",
      "Epoch 2068/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9591.6118 - val_loss: 9562.8408\n",
      "Epoch 2069/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9591.1479 - val_loss: 9562.3779\n",
      "Epoch 2070/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9590.6856 - val_loss: 9561.9160\n",
      "Epoch 2071/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9590.2217 - val_loss: 9561.4531\n",
      "Epoch 2072/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9589.7592 - val_loss: 9560.9893\n",
      "Epoch 2073/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9589.2965 - val_loss: 9560.5254\n",
      "Epoch 2074/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9588.8318 - val_loss: 9560.0645\n",
      "Epoch 2075/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9588.3694 - val_loss: 9559.5996\n",
      "Epoch 2076/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9587.9073 - val_loss: 9559.1367\n",
      "Epoch 2077/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 9587.4432 - val_loss: 9558.6738\n",
      "Epoch 2078/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9586.9800 - val_loss: 9558.2109\n",
      "Epoch 2079/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9586.5174 - val_loss: 9557.7471\n",
      "Epoch 2080/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9586.0535 - val_loss: 9557.2842\n",
      "Epoch 2081/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9585.5910 - val_loss: 9556.8213\n",
      "Epoch 2082/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9585.1274 - val_loss: 9556.3574\n",
      "Epoch 2083/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9584.6640 - val_loss: 9555.8945\n",
      "Epoch 2084/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9584.2020 - val_loss: 9555.4316\n",
      "Epoch 2085/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9583.7373 - val_loss: 9554.9688\n",
      "Epoch 2086/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9583.2747 - val_loss: 9554.5049\n",
      "Epoch 2087/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9582.8125 - val_loss: 9554.0430\n",
      "Epoch 2088/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9582.3479 - val_loss: 9553.5801\n",
      "Epoch 2089/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9581.8854 - val_loss: 9553.1162\n",
      "Epoch 2090/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9581.4232 - val_loss: 9552.6514\n",
      "Epoch 2091/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9580.9589 - val_loss: 9552.1895\n",
      "Epoch 2092/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9580.4968 - val_loss: 9551.7275\n",
      "Epoch 2093/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9580.0331 - val_loss: 9551.2627\n",
      "Epoch 2094/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9579.5692 - val_loss: 9550.8008\n",
      "Epoch 2095/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9579.1074 - val_loss: 9550.3379\n",
      "Epoch 2096/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9578.6434 - val_loss: 9549.8730\n",
      "Epoch 2097/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9578.1799 - val_loss: 9549.4092\n",
      "Epoch 2098/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9577.7179 - val_loss: 9548.9463\n",
      "Epoch 2099/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9577.2537 - val_loss: 9548.4863\n",
      "Epoch 2100/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9576.7908 - val_loss: 9548.0205\n",
      "Epoch 2101/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9576.3286 - val_loss: 9547.5576\n",
      "Epoch 2102/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9575.8641 - val_loss: 9547.0957\n",
      "Epoch 2103/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9575.4014 - val_loss: 9546.6328\n",
      "Epoch 2104/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9574.9388 - val_loss: 9546.1680\n",
      "Epoch 2105/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9574.4746 - val_loss: 9545.7051\n",
      "Epoch 2106/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9574.0124 - val_loss: 9545.2441\n",
      "Epoch 2107/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9573.5493 - val_loss: 9544.7783\n",
      "Epoch 2108/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9573.0854 - val_loss: 9544.3154\n",
      "Epoch 2109/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9572.6233 - val_loss: 9543.8535\n",
      "Epoch 2110/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9572.1592 - val_loss: 9543.3906\n",
      "Epoch 2111/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9571.6965 - val_loss: 9542.9268\n",
      "Epoch 2112/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9571.2338 - val_loss: 9542.4629\n",
      "Epoch 2113/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9570.7693 - val_loss: 9542.0020\n",
      "Epoch 2114/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9570.3069 - val_loss: 9541.5371\n",
      "Epoch 2115/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9569.8448 - val_loss: 9541.0742\n",
      "Epoch 2116/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9569.3804 - val_loss: 9540.6113\n",
      "Epoch 2117/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9568.9174 - val_loss: 9540.1484\n",
      "Epoch 2118/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9568.4548 - val_loss: 9539.6826\n",
      "Epoch 2119/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9567.9903 - val_loss: 9539.2207\n",
      "Epoch 2120/3000\n",
      "742/742 [==============================] - 0s 146us/step - loss: 9567.5281 - val_loss: 9538.7588\n",
      "Epoch 2121/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9567.0651 - val_loss: 9538.2939\n",
      "Epoch 2122/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9566.6013 - val_loss: 9537.8320\n",
      "Epoch 2123/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9566.1393 - val_loss: 9537.3691\n",
      "Epoch 2124/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9565.6748 - val_loss: 9536.9062\n",
      "Epoch 2125/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9565.2120 - val_loss: 9536.4424\n",
      "Epoch 2126/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9564.7502 - val_loss: 9535.9785\n",
      "Epoch 2127/3000\n",
      "742/742 [==============================] - 0s 148us/step - loss: 9564.2851 - val_loss: 9535.5176\n",
      "Epoch 2128/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9563.8225 - val_loss: 9535.0537\n",
      "Epoch 2129/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9563.3603 - val_loss: 9534.5889\n",
      "Epoch 2130/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9562.8964 - val_loss: 9534.1270\n",
      "Epoch 2131/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 9562.4340 - val_loss: 9533.6650\n",
      "Epoch 2132/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9561.9706 - val_loss: 9533.2002\n",
      "Epoch 2133/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9561.5065 - val_loss: 9532.7383\n",
      "Epoch 2134/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9561.0449 - val_loss: 9532.2754\n",
      "Epoch 2135/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9560.5805 - val_loss: 9531.8105\n",
      "Epoch 2136/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9560.1172 - val_loss: 9531.3467\n",
      "Epoch 2137/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9559.6556 - val_loss: 9530.8838\n",
      "Epoch 2138/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9559.1912 - val_loss: 9530.4219\n",
      "Epoch 2139/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9558.7279 - val_loss: 9529.9580\n",
      "Epoch 2140/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9558.2659 - val_loss: 9529.4951\n",
      "Epoch 2141/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9557.8017 - val_loss: 9529.0332\n",
      "Epoch 2142/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9557.3389 - val_loss: 9528.5703\n",
      "Epoch 2143/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9556.8761 - val_loss: 9528.1055\n",
      "Epoch 2144/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9556.4121 - val_loss: 9527.6426\n",
      "Epoch 2145/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9555.9500 - val_loss: 9527.1816\n",
      "Epoch 2146/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9555.4867 - val_loss: 9526.7158\n",
      "Epoch 2147/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9555.0229 - val_loss: 9526.2529\n",
      "Epoch 2148/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9554.5608 - val_loss: 9525.7900\n",
      "Epoch 2149/3000\n",
      "742/742 [==============================] - 0s 145us/step - loss: 9554.0966 - val_loss: 9525.3281\n",
      "Epoch 2150/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9553.6337 - val_loss: 9524.8633\n",
      "Epoch 2151/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 9553.1708 - val_loss: 9524.4004\n",
      "Epoch 2152/3000\n",
      "742/742 [==============================] - 0s 159us/step - loss: 9552.7068 - val_loss: 9523.9395\n",
      "Epoch 2153/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9552.2438 - val_loss: 9523.4746\n",
      "Epoch 2154/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9551.7820 - val_loss: 9523.0117\n",
      "Epoch 2155/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9551.3174 - val_loss: 9522.5488\n",
      "Epoch 2156/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9550.8548 - val_loss: 9522.0859\n",
      "Epoch 2157/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9550.3920 - val_loss: 9521.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2158/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9549.9279 - val_loss: 9521.1582\n",
      "Epoch 2159/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9549.4658 - val_loss: 9520.6963\n",
      "Epoch 2160/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9549.0025 - val_loss: 9520.2314\n",
      "Epoch 2161/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9548.5390 - val_loss: 9519.7695\n",
      "Epoch 2162/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9548.0769 - val_loss: 9519.3066\n",
      "Epoch 2163/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9547.6123 - val_loss: 9518.8438\n",
      "Epoch 2164/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9547.1497 - val_loss: 9518.3799\n",
      "Epoch 2165/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9546.6875 - val_loss: 9517.9160\n",
      "Epoch 2166/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9546.2227 - val_loss: 9517.4551\n",
      "Epoch 2167/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9545.7600 - val_loss: 9516.9912\n",
      "Epoch 2168/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9545.2980 - val_loss: 9516.5264\n",
      "Epoch 2169/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9544.8337 - val_loss: 9516.0645\n",
      "Epoch 2170/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9544.3712 - val_loss: 9515.6016\n",
      "Epoch 2171/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9543.9080 - val_loss: 9515.1377\n",
      "Epoch 2172/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9543.4441 - val_loss: 9514.6758\n",
      "Epoch 2173/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9542.9824 - val_loss: 9514.2129\n",
      "Epoch 2174/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9542.5180 - val_loss: 9513.7471\n",
      "Epoch 2175/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9542.0546 - val_loss: 9513.2842\n",
      "Epoch 2176/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9541.5929 - val_loss: 9512.8213\n",
      "Epoch 2177/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9541.1283 - val_loss: 9512.3594\n",
      "Epoch 2178/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9540.6654 - val_loss: 9511.8955\n",
      "Epoch 2179/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9540.2033 - val_loss: 9511.4326\n",
      "Epoch 2180/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9539.7391 - val_loss: 9510.9707\n",
      "Epoch 2181/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9539.2763 - val_loss: 9510.5059\n",
      "Epoch 2182/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9538.8138 - val_loss: 9510.0430\n",
      "Epoch 2183/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9538.3495 - val_loss: 9509.5801\n",
      "Epoch 2184/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9537.8874 - val_loss: 9509.1191\n",
      "Epoch 2185/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9537.4240 - val_loss: 9508.6533\n",
      "Epoch 2186/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9536.9601 - val_loss: 9508.1904\n",
      "Epoch 2187/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9536.4980 - val_loss: 9507.7275\n",
      "Epoch 2188/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9536.0341 - val_loss: 9507.2627\n",
      "Epoch 2189/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9535.5705 - val_loss: 9506.8008\n",
      "Epoch 2190/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9535.1086 - val_loss: 9506.3379\n",
      "Epoch 2191/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9534.6443 - val_loss: 9505.8770\n",
      "Epoch 2192/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9534.1814 - val_loss: 9505.4121\n",
      "Epoch 2193/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9533.7195 - val_loss: 9504.9492\n",
      "Epoch 2194/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9533.2549 - val_loss: 9504.4863\n",
      "Epoch 2195/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9532.7923 - val_loss: 9504.0234\n",
      "Epoch 2196/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9532.3294 - val_loss: 9503.5576\n",
      "Epoch 2197/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9531.8651 - val_loss: 9503.0957\n",
      "Epoch 2198/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 9531.4033 - val_loss: 9502.6338\n",
      "Epoch 2199/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9530.9399 - val_loss: 9502.1689\n",
      "Epoch 2200/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9530.4758 - val_loss: 9501.7070\n",
      "Epoch 2201/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9530.0143 - val_loss: 9501.2441\n",
      "Epoch 2202/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9529.5499 - val_loss: 9500.7812\n",
      "Epoch 2203/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9529.0866 - val_loss: 9500.3174\n",
      "Epoch 2204/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9528.6249 - val_loss: 9499.8535\n",
      "Epoch 2205/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9528.1602 - val_loss: 9499.3926\n",
      "Epoch 2206/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9527.6975 - val_loss: 9498.9287\n",
      "Epoch 2207/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9527.2354 - val_loss: 9498.4639\n",
      "Epoch 2208/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9526.7704 - val_loss: 9498.0020\n",
      "Epoch 2209/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9526.3086 - val_loss: 9497.5391\n",
      "Epoch 2210/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9525.8453 - val_loss: 9497.0752\n",
      "Epoch 2211/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9525.3817 - val_loss: 9496.6113\n",
      "Epoch 2212/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9524.9190 - val_loss: 9496.1504\n",
      "Epoch 2213/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9524.4555 - val_loss: 9495.6846\n",
      "Epoch 2214/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9523.9921 - val_loss: 9495.2217\n",
      "Epoch 2215/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9523.5303 - val_loss: 9494.7588\n",
      "Epoch 2216/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9523.0655 - val_loss: 9494.2969\n",
      "Epoch 2217/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9522.6033 - val_loss: 9493.8330\n",
      "Epoch 2218/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9522.1407 - val_loss: 9493.3701\n",
      "Epoch 2219/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9521.6767 - val_loss: 9492.9082\n",
      "Epoch 2220/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9521.2137 - val_loss: 9492.4434\n",
      "Epoch 2221/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9520.7511 - val_loss: 9491.9805\n",
      "Epoch 2222/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9520.2870 - val_loss: 9491.5176\n",
      "Epoch 2223/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9519.8251 - val_loss: 9491.0547\n",
      "Epoch 2224/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9519.3611 - val_loss: 9490.5908\n",
      "Epoch 2225/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9518.8973 - val_loss: 9490.1279\n",
      "Epoch 2226/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9518.4358 - val_loss: 9489.6650\n",
      "Epoch 2227/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9517.9712 - val_loss: 9489.2002\n",
      "Epoch 2228/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9517.5079 - val_loss: 9488.7383\n",
      "Epoch 2229/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9517.0459 - val_loss: 9488.2754\n",
      "Epoch 2230/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9516.5819 - val_loss: 9487.8145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2231/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9516.1189 - val_loss: 9487.3496\n",
      "Epoch 2232/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9515.6569 - val_loss: 9486.8867\n",
      "Epoch 2233/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9515.1924 - val_loss: 9486.4238\n",
      "Epoch 2234/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9514.7297 - val_loss: 9485.9600\n",
      "Epoch 2235/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9514.2669 - val_loss: 9485.4951\n",
      "Epoch 2236/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9513.8026 - val_loss: 9485.0332\n",
      "Epoch 2237/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9513.3408 - val_loss: 9484.5713\n",
      "Epoch 2238/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9512.8769 - val_loss: 9484.1064\n",
      "Epoch 2239/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9512.4133 - val_loss: 9483.6445\n",
      "Epoch 2240/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9511.9515 - val_loss: 9483.1816\n",
      "Epoch 2241/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9511.4873 - val_loss: 9482.7188\n",
      "Epoch 2242/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9511.0244 - val_loss: 9482.2529\n",
      "Epoch 2243/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9510.5624 - val_loss: 9481.7910\n",
      "Epoch 2244/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9510.0976 - val_loss: 9481.3301\n",
      "Epoch 2245/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9509.6350 - val_loss: 9480.8662\n",
      "Epoch 2246/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9509.1728 - val_loss: 9480.4014\n",
      "Epoch 2247/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9508.7080 - val_loss: 9479.9395\n",
      "Epoch 2248/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9508.2456 - val_loss: 9479.4766\n",
      "Epoch 2249/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9507.7828 - val_loss: 9479.0127\n",
      "Epoch 2250/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9507.3192 - val_loss: 9478.5488\n",
      "Epoch 2251/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9506.8563 - val_loss: 9478.0879\n",
      "Epoch 2252/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9506.3930 - val_loss: 9477.6221\n",
      "Epoch 2253/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9505.9296 - val_loss: 9477.1592\n",
      "Epoch 2254/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9505.4678 - val_loss: 9476.6963\n",
      "Epoch 2255/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9505.0031 - val_loss: 9476.2344\n",
      "Epoch 2256/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 9504.5403 - val_loss: 9475.7705\n",
      "Epoch 2257/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9504.0780 - val_loss: 9475.3076\n",
      "Epoch 2258/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9503.6137 - val_loss: 9474.8457\n",
      "Epoch 2259/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9503.1510 - val_loss: 9474.3809\n",
      "Epoch 2260/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9502.6884 - val_loss: 9473.9180\n",
      "Epoch 2261/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9502.2239 - val_loss: 9473.4551\n",
      "Epoch 2262/3000\n",
      "742/742 [==============================] - 0s 155us/step - loss: 9501.7620 - val_loss: 9472.9922\n",
      "Epoch 2263/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9501.2986 - val_loss: 9472.5283\n",
      "Epoch 2264/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9500.8348 - val_loss: 9472.0654\n",
      "Epoch 2265/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9500.3730 - val_loss: 9471.6025\n",
      "Epoch 2266/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9499.9088 - val_loss: 9471.1377\n",
      "Epoch 2267/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9499.4452 - val_loss: 9470.6758\n",
      "Epoch 2268/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9498.9834 - val_loss: 9470.2129\n",
      "Epoch 2269/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9498.5189 - val_loss: 9469.7510\n",
      "Epoch 2270/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9498.0559 - val_loss: 9469.2861\n",
      "Epoch 2271/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9497.5937 - val_loss: 9468.8242\n",
      "Epoch 2272/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9497.1299 - val_loss: 9468.3613\n",
      "Epoch 2273/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9496.6670 - val_loss: 9467.8975\n",
      "Epoch 2274/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9496.2046 - val_loss: 9467.4326\n",
      "Epoch 2275/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9495.7400 - val_loss: 9466.9707\n",
      "Epoch 2276/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9495.2781 - val_loss: 9466.5088\n",
      "Epoch 2277/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9494.8146 - val_loss: 9466.0439\n",
      "Epoch 2278/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9494.3505 - val_loss: 9465.5820\n",
      "Epoch 2279/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9493.8888 - val_loss: 9465.1191\n",
      "Epoch 2280/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9493.4248 - val_loss: 9464.6562\n",
      "Epoch 2281/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9492.9619 - val_loss: 9464.1904\n",
      "Epoch 2282/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9492.4991 - val_loss: 9463.7285\n",
      "Epoch 2283/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9492.0349 - val_loss: 9463.2666\n",
      "Epoch 2284/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9491.5724 - val_loss: 9462.8037\n",
      "Epoch 2285/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9491.1101 - val_loss: 9462.3389\n",
      "Epoch 2286/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9490.6455 - val_loss: 9461.8770\n",
      "Epoch 2287/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9490.1833 - val_loss: 9461.4141\n",
      "Epoch 2288/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9489.7203 - val_loss: 9460.9502\n",
      "Epoch 2289/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9489.2566 - val_loss: 9460.4863\n",
      "Epoch 2290/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9488.7937 - val_loss: 9460.0254\n",
      "Epoch 2291/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9488.3304 - val_loss: 9459.5596\n",
      "Epoch 2292/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9487.8671 - val_loss: 9459.0967\n",
      "Epoch 2293/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9487.4045 - val_loss: 9458.6338\n",
      "Epoch 2294/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9486.9404 - val_loss: 9458.1719\n",
      "Epoch 2295/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9486.4779 - val_loss: 9457.7080\n",
      "Epoch 2296/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9486.0155 - val_loss: 9457.2441\n",
      "Epoch 2297/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9485.5508 - val_loss: 9456.7822\n",
      "Epoch 2298/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9485.0882 - val_loss: 9456.3184\n",
      "Epoch 2299/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9484.6259 - val_loss: 9455.8555\n",
      "Epoch 2300/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9484.1613 - val_loss: 9455.3926\n",
      "Epoch 2301/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9483.6985 - val_loss: 9454.9297\n",
      "Epoch 2302/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9483.2361 - val_loss: 9454.4658\n",
      "Epoch 2303/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9482.7722 - val_loss: 9454.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2304/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9482.3105 - val_loss: 9453.5400\n",
      "Epoch 2305/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9481.8461 - val_loss: 9453.0752\n",
      "Epoch 2306/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9481.3827 - val_loss: 9452.6133\n",
      "Epoch 2307/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9480.9208 - val_loss: 9452.1504\n",
      "Epoch 2308/3000\n",
      "742/742 [==============================] - 0s 86us/step - loss: 9480.4569 - val_loss: 9451.6885\n",
      "Epoch 2309/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9479.9937 - val_loss: 9451.2236\n",
      "Epoch 2310/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9479.5312 - val_loss: 9450.7617\n",
      "Epoch 2311/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9479.0674 - val_loss: 9450.2988\n",
      "Epoch 2312/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9478.6044 - val_loss: 9449.8350\n",
      "Epoch 2313/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9478.1419 - val_loss: 9449.3701\n",
      "Epoch 2314/3000\n",
      "742/742 [==============================] - 0s 165us/step - loss: 9477.6776 - val_loss: 9448.9082\n",
      "Epoch 2315/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9477.2156 - val_loss: 9448.4463\n",
      "Epoch 2316/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9476.7521 - val_loss: 9447.9814\n",
      "Epoch 2317/3000\n",
      "742/742 [==============================] - 0s 107us/step - loss: 9476.2880 - val_loss: 9447.5195\n",
      "Epoch 2318/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9475.8264 - val_loss: 9447.0566\n",
      "Epoch 2319/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9475.3622 - val_loss: 9446.5938\n",
      "Epoch 2320/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9474.8990 - val_loss: 9446.1279\n",
      "Epoch 2321/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9474.4365 - val_loss: 9445.6660\n",
      "Epoch 2322/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9473.9725 - val_loss: 9445.2041\n",
      "Epoch 2323/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9473.5100 - val_loss: 9444.7412\n",
      "Epoch 2324/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9473.0477 - val_loss: 9444.2764\n",
      "Epoch 2325/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9472.5829 - val_loss: 9443.8145\n",
      "Epoch 2326/3000\n",
      "742/742 [==============================] - 0s 165us/step - loss: 9472.1204 - val_loss: 9443.3516\n",
      "Epoch 2327/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9471.6578 - val_loss: 9442.8877\n",
      "Epoch 2328/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9471.1937 - val_loss: 9442.4238\n",
      "Epoch 2329/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9470.7312 - val_loss: 9441.9629\n",
      "Epoch 2330/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9470.2677 - val_loss: 9441.4971\n",
      "Epoch 2331/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9469.8040 - val_loss: 9441.0342\n",
      "Epoch 2332/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9469.3418 - val_loss: 9440.5713\n",
      "Epoch 2333/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9468.8777 - val_loss: 9440.1094\n",
      "Epoch 2334/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9468.4153 - val_loss: 9439.6455\n",
      "Epoch 2335/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9467.9531 - val_loss: 9439.1816\n",
      "Epoch 2336/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9467.4881 - val_loss: 9438.7197\n",
      "Epoch 2337/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9467.0257 - val_loss: 9438.2559\n",
      "Epoch 2338/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9466.5634 - val_loss: 9437.7930\n",
      "Epoch 2339/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9466.0988 - val_loss: 9437.3301\n",
      "Epoch 2340/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9465.6361 - val_loss: 9436.8672\n",
      "Epoch 2341/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9465.1735 - val_loss: 9436.4033\n",
      "Epoch 2342/3000\n",
      "742/742 [==============================] - 0s 183us/step - loss: 9464.7098 - val_loss: 9435.9404\n",
      "Epoch 2343/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9464.2483 - val_loss: 9435.4775\n",
      "Epoch 2344/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9463.7836 - val_loss: 9435.0127\n",
      "Epoch 2345/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9463.3203 - val_loss: 9434.5508\n",
      "Epoch 2346/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9462.8582 - val_loss: 9434.0879\n",
      "Epoch 2347/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9462.3937 - val_loss: 9433.6260\n",
      "Epoch 2348/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9461.9309 - val_loss: 9433.1611\n",
      "Epoch 2349/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9461.4690 - val_loss: 9432.6992\n",
      "Epoch 2350/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9461.0042 - val_loss: 9432.2354\n",
      "Epoch 2351/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9460.5413 - val_loss: 9431.7725\n",
      "Epoch 2352/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9460.0793 - val_loss: 9431.3076\n",
      "Epoch 2353/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9459.6152 - val_loss: 9430.8457\n",
      "Epoch 2354/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9459.1524 - val_loss: 9430.3838\n",
      "Epoch 2355/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9458.6894 - val_loss: 9429.9189\n",
      "Epoch 2356/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9458.2254 - val_loss: 9429.4570\n",
      "Epoch 2357/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9457.7637 - val_loss: 9428.9941\n",
      "Epoch 2358/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9457.2996 - val_loss: 9428.5283\n",
      "Epoch 2359/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9456.8358 - val_loss: 9428.0654\n",
      "Epoch 2360/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9456.3740 - val_loss: 9427.6035\n",
      "Epoch 2361/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9455.9100 - val_loss: 9427.1416\n",
      "Epoch 2362/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9455.4474 - val_loss: 9426.6768\n",
      "Epoch 2363/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9454.9845 - val_loss: 9426.2139\n",
      "Epoch 2364/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9454.5204 - val_loss: 9425.7510\n",
      "Epoch 2365/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9454.0579 - val_loss: 9425.2891\n",
      "Epoch 2366/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9453.5952 - val_loss: 9424.8252\n",
      "Epoch 2367/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9453.1314 - val_loss: 9424.3613\n",
      "Epoch 2368/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9452.6689 - val_loss: 9423.8994\n",
      "Epoch 2369/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9452.2052 - val_loss: 9423.4346\n",
      "Epoch 2370/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9451.7410 - val_loss: 9422.9717\n",
      "Epoch 2371/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9451.2794 - val_loss: 9422.5088\n",
      "Epoch 2372/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9450.8154 - val_loss: 9422.0469\n",
      "Epoch 2373/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9450.3529 - val_loss: 9421.5820\n",
      "Epoch 2374/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9449.8903 - val_loss: 9421.1191\n",
      "Epoch 2375/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9449.4257 - val_loss: 9420.6572\n",
      "Epoch 2376/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9448.9631 - val_loss: 9420.1934\n",
      "Epoch 2377/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9448.5011 - val_loss: 9419.7305\n",
      "Epoch 2378/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9448.0363 - val_loss: 9419.2676\n",
      "Epoch 2379/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9447.5736 - val_loss: 9418.8047\n",
      "Epoch 2380/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9447.1109 - val_loss: 9418.3408\n",
      "Epoch 2381/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9446.6472 - val_loss: 9417.8770\n",
      "Epoch 2382/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9446.1851 - val_loss: 9417.4150\n",
      "Epoch 2383/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9445.7211 - val_loss: 9416.9502\n",
      "Epoch 2384/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9445.2578 - val_loss: 9416.4883\n",
      "Epoch 2385/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9444.7958 - val_loss: 9416.0254\n",
      "Epoch 2386/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9444.3311 - val_loss: 9415.5625\n",
      "Epoch 2387/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9443.8688 - val_loss: 9415.0986\n",
      "Epoch 2388/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9443.4063 - val_loss: 9414.6367\n",
      "Epoch 2389/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9442.9419 - val_loss: 9414.1729\n",
      "Epoch 2390/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9442.4788 - val_loss: 9413.7100\n",
      "Epoch 2391/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9442.0167 - val_loss: 9413.2451\n",
      "Epoch 2392/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9441.5526 - val_loss: 9412.7832\n",
      "Epoch 2393/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9441.0899 - val_loss: 9412.3203\n",
      "Epoch 2394/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9440.6267 - val_loss: 9411.8564\n",
      "Epoch 2395/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9440.1629 - val_loss: 9411.3945\n",
      "Epoch 2396/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9439.7012 - val_loss: 9410.9316\n",
      "Epoch 2397/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9439.2371 - val_loss: 9410.4658\n",
      "Epoch 2398/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9438.7733 - val_loss: 9410.0029\n",
      "Epoch 2399/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9438.3115 - val_loss: 9409.5410\n",
      "Epoch 2400/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9437.8469 - val_loss: 9409.0791\n",
      "Epoch 2401/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9437.3849 - val_loss: 9408.6143\n",
      "Epoch 2402/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9436.9219 - val_loss: 9408.1514\n",
      "Epoch 2403/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9436.4580 - val_loss: 9407.6885\n",
      "Epoch 2404/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9435.9951 - val_loss: 9407.2246\n",
      "Epoch 2405/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9435.5324 - val_loss: 9406.7617\n",
      "Epoch 2406/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9435.0683 - val_loss: 9406.2988\n",
      "Epoch 2407/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9434.6064 - val_loss: 9405.8369\n",
      "Epoch 2408/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9434.1426 - val_loss: 9405.3721\n",
      "Epoch 2409/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9433.6789 - val_loss: 9404.9092\n",
      "Epoch 2410/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9433.2167 - val_loss: 9404.4463\n",
      "Epoch 2411/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9432.7528 - val_loss: 9403.9844\n",
      "Epoch 2412/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9432.2901 - val_loss: 9403.5195\n",
      "Epoch 2413/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9431.8278 - val_loss: 9403.0566\n",
      "Epoch 2414/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9431.3631 - val_loss: 9402.5947\n",
      "Epoch 2415/3000\n",
      "742/742 [==============================] - 0s 153us/step - loss: 9430.9006 - val_loss: 9402.1309\n",
      "Epoch 2416/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9430.4383 - val_loss: 9401.6680\n",
      "Epoch 2417/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9429.9736 - val_loss: 9401.2051\n",
      "Epoch 2418/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9429.5111 - val_loss: 9400.7422\n",
      "Epoch 2419/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9429.0484 - val_loss: 9400.2783\n",
      "Epoch 2420/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9428.5840 - val_loss: 9399.8145\n",
      "Epoch 2421/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9428.1228 - val_loss: 9399.3525\n",
      "Epoch 2422/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9427.6586 - val_loss: 9398.8877\n",
      "Epoch 2423/3000\n",
      "742/742 [==============================] - 0s 145us/step - loss: 9427.1952 - val_loss: 9398.4258\n",
      "Epoch 2424/3000\n",
      "742/742 [==============================] - 0s 139us/step - loss: 9426.7326 - val_loss: 9397.9629\n",
      "Epoch 2425/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9426.2685 - val_loss: 9397.5010\n",
      "Epoch 2426/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9425.8060 - val_loss: 9397.0361\n",
      "Epoch 2427/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9425.3438 - val_loss: 9396.5742\n",
      "Epoch 2428/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9424.8788 - val_loss: 9396.1104\n",
      "Epoch 2429/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9424.4163 - val_loss: 9395.6475\n",
      "Epoch 2430/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9423.9543 - val_loss: 9395.1826\n",
      "Epoch 2431/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9423.4900 - val_loss: 9394.7207\n",
      "Epoch 2432/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9423.0273 - val_loss: 9394.2578\n",
      "Epoch 2433/3000\n",
      "742/742 [==============================] - 0s 145us/step - loss: 9422.5641 - val_loss: 9393.7939\n",
      "Epoch 2434/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9422.1004 - val_loss: 9393.3320\n",
      "Epoch 2435/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9421.6388 - val_loss: 9392.8682\n",
      "Epoch 2436/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9421.1746 - val_loss: 9392.4033\n",
      "Epoch 2437/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9420.7108 - val_loss: 9391.9404\n",
      "Epoch 2438/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9420.2489 - val_loss: 9391.4785\n",
      "Epoch 2439/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9419.7843 - val_loss: 9391.0156\n",
      "Epoch 2440/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9419.3220 - val_loss: 9390.5518\n",
      "Epoch 2441/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9418.8593 - val_loss: 9390.0889\n",
      "Epoch 2442/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9418.3954 - val_loss: 9389.6260\n",
      "Epoch 2443/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9417.9326 - val_loss: 9389.1621\n",
      "Epoch 2444/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9417.4698 - val_loss: 9388.6992\n",
      "Epoch 2445/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9417.0057 - val_loss: 9388.2363\n",
      "Epoch 2446/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9416.5437 - val_loss: 9387.7744\n",
      "Epoch 2447/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9416.0800 - val_loss: 9387.3096\n",
      "Epoch 2448/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 118us/step - loss: 9415.6160 - val_loss: 9386.8467\n",
      "Epoch 2449/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9415.1543 - val_loss: 9386.3838\n",
      "Epoch 2450/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9414.6904 - val_loss: 9385.9219\n",
      "Epoch 2451/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9414.2276 - val_loss: 9385.4570\n",
      "Epoch 2452/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9413.7646 - val_loss: 9384.9941\n",
      "Epoch 2453/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9413.3005 - val_loss: 9384.5322\n",
      "Epoch 2454/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9412.8379 - val_loss: 9384.0684\n",
      "Epoch 2455/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9412.3759 - val_loss: 9383.6055\n",
      "Epoch 2456/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9411.9112 - val_loss: 9383.1426\n",
      "Epoch 2457/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9411.4486 - val_loss: 9382.6797\n",
      "Epoch 2458/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9410.9859 - val_loss: 9382.2139\n",
      "Epoch 2459/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9410.5215 - val_loss: 9381.7520\n",
      "Epoch 2460/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9410.0595 - val_loss: 9381.2900\n",
      "Epoch 2461/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9409.5962 - val_loss: 9380.8252\n",
      "Epoch 2462/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9409.1327 - val_loss: 9380.3633\n",
      "Epoch 2463/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9408.6699 - val_loss: 9379.9004\n",
      "Epoch 2464/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9408.2061 - val_loss: 9379.4375\n",
      "Epoch 2465/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9407.7434 - val_loss: 9378.9736\n",
      "Epoch 2466/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9407.2811 - val_loss: 9378.5117\n",
      "Epoch 2467/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9406.8162 - val_loss: 9378.0479\n",
      "Epoch 2468/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9406.3538 - val_loss: 9377.5850\n",
      "Epoch 2469/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9405.8917 - val_loss: 9377.1201\n",
      "Epoch 2470/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9405.4276 - val_loss: 9376.6582\n",
      "Epoch 2471/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9404.9646 - val_loss: 9376.1953\n",
      "Epoch 2472/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9404.5015 - val_loss: 9375.7314\n",
      "Epoch 2473/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9404.0379 - val_loss: 9375.2695\n",
      "Epoch 2474/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9403.5760 - val_loss: 9374.8057\n",
      "Epoch 2475/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9403.1117 - val_loss: 9374.3408\n",
      "Epoch 2476/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9402.6483 - val_loss: 9373.8779\n",
      "Epoch 2477/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9402.1864 - val_loss: 9373.4160\n",
      "Epoch 2478/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9401.7216 - val_loss: 9372.9531\n",
      "Epoch 2479/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9401.2592 - val_loss: 9372.4893\n",
      "Epoch 2480/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9400.7969 - val_loss: 9372.0264\n",
      "Epoch 2481/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9400.3323 - val_loss: 9371.5635\n",
      "Epoch 2482/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9399.8701 - val_loss: 9371.0996\n",
      "Epoch 2483/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9399.4073 - val_loss: 9370.6367\n",
      "Epoch 2484/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9398.9433 - val_loss: 9370.1738\n",
      "Epoch 2485/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9398.4811 - val_loss: 9369.7119\n",
      "Epoch 2486/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9398.0175 - val_loss: 9369.2471\n",
      "Epoch 2487/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9397.5535 - val_loss: 9368.7842\n",
      "Epoch 2488/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 9397.0918 - val_loss: 9368.3213\n",
      "Epoch 2489/3000\n",
      "742/742 [==============================] - 0s 141us/step - loss: 9396.6277 - val_loss: 9367.8564\n",
      "Epoch 2490/3000\n",
      "742/742 [==============================] - 0s 159us/step - loss: 9396.1643 - val_loss: 9367.3945\n",
      "Epoch 2491/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9395.7020 - val_loss: 9366.9316\n",
      "Epoch 2492/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9395.2380 - val_loss: 9366.4697\n",
      "Epoch 2493/3000\n",
      "742/742 [==============================] - 0s 151us/step - loss: 9394.7756 - val_loss: 9366.0049\n",
      "Epoch 2494/3000\n",
      "742/742 [==============================] - 0s 153us/step - loss: 9394.3127 - val_loss: 9365.5430\n",
      "Epoch 2495/3000\n",
      "742/742 [==============================] - 0s 153us/step - loss: 9393.8487 - val_loss: 9365.0791\n",
      "Epoch 2496/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9393.3860 - val_loss: 9364.6172\n",
      "Epoch 2497/3000\n",
      "742/742 [==============================] - 0s 153us/step - loss: 9392.9233 - val_loss: 9364.1533\n",
      "Epoch 2498/3000\n",
      "742/742 [==============================] - 0s 165us/step - loss: 9392.4589 - val_loss: 9363.6895\n",
      "Epoch 2499/3000\n",
      "742/742 [==============================] - 0s 163us/step - loss: 9391.9969 - val_loss: 9363.2275\n",
      "Epoch 2500/3000\n",
      "742/742 [==============================] - 0s 145us/step - loss: 9391.5333 - val_loss: 9362.7627\n",
      "Epoch 2501/3000\n",
      "742/742 [==============================] - 0s 161us/step - loss: 9391.0702 - val_loss: 9362.3008\n",
      "Epoch 2502/3000\n",
      "742/742 [==============================] - 0s 152us/step - loss: 9390.6074 - val_loss: 9361.8379\n",
      "Epoch 2503/3000\n",
      "742/742 [==============================] - 0s 149us/step - loss: 9390.1436 - val_loss: 9361.3750\n",
      "Epoch 2504/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9389.6810 - val_loss: 9360.9111\n",
      "Epoch 2505/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9389.2185 - val_loss: 9360.4463\n",
      "Epoch 2506/3000\n",
      "742/742 [==============================] - 0s 150us/step - loss: 9388.7536 - val_loss: 9359.9854\n",
      "Epoch 2507/3000\n",
      "742/742 [==============================] - 0s 140us/step - loss: 9388.2911 - val_loss: 9359.5225\n",
      "Epoch 2508/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9387.8290 - val_loss: 9359.0576\n",
      "Epoch 2509/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9387.3651 - val_loss: 9358.5957\n",
      "Epoch 2510/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9386.9017 - val_loss: 9358.1328\n",
      "Epoch 2511/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9386.4390 - val_loss: 9357.6689\n",
      "Epoch 2512/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9385.9747 - val_loss: 9357.2051\n",
      "Epoch 2513/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9385.5127 - val_loss: 9356.7432\n",
      "Epoch 2514/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9385.0493 - val_loss: 9356.2783\n",
      "Epoch 2515/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9384.5858 - val_loss: 9355.8154\n",
      "Epoch 2516/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9384.1239 - val_loss: 9355.3535\n",
      "Epoch 2517/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9383.6592 - val_loss: 9354.8906\n",
      "Epoch 2518/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9383.1967 - val_loss: 9354.4268\n",
      "Epoch 2519/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9382.7344 - val_loss: 9353.9639\n",
      "Epoch 2520/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9382.2705 - val_loss: 9353.5010\n",
      "Epoch 2521/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9381.8071 - val_loss: 9353.0371\n",
      "Epoch 2522/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9381.3446 - val_loss: 9352.5742\n",
      "Epoch 2523/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9380.8807 - val_loss: 9352.1113\n",
      "Epoch 2524/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9380.4187 - val_loss: 9351.6494\n",
      "Epoch 2525/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9379.9549 - val_loss: 9351.1846\n",
      "Epoch 2526/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9379.4910 - val_loss: 9350.7217\n",
      "Epoch 2527/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9379.0293 - val_loss: 9350.2588\n",
      "Epoch 2528/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9378.5650 - val_loss: 9349.7939\n",
      "Epoch 2529/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9378.1018 - val_loss: 9349.3320\n",
      "Epoch 2530/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9377.6395 - val_loss: 9348.8691\n",
      "Epoch 2531/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9377.1756 - val_loss: 9348.4062\n",
      "Epoch 2532/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9376.7126 - val_loss: 9347.9424\n",
      "Epoch 2533/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9376.2500 - val_loss: 9347.4805\n",
      "Epoch 2534/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9375.7862 - val_loss: 9347.0166\n",
      "Epoch 2535/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9375.3235 - val_loss: 9346.5547\n",
      "Epoch 2536/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9374.8608 - val_loss: 9346.0889\n",
      "Epoch 2537/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9374.3963 - val_loss: 9345.6270\n",
      "Epoch 2538/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9373.9344 - val_loss: 9345.1650\n",
      "Epoch 2539/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9373.4710 - val_loss: 9344.7002\n",
      "Epoch 2540/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9373.0067 - val_loss: 9344.2383\n",
      "Epoch 2541/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9372.5449 - val_loss: 9343.7744\n",
      "Epoch 2542/3000\n",
      "742/742 [==============================] - 0s 119us/step - loss: 9372.0809 - val_loss: 9343.3125\n",
      "Epoch 2543/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9371.6184 - val_loss: 9342.8486\n",
      "Epoch 2544/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9371.1559 - val_loss: 9342.3838\n",
      "Epoch 2545/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9370.6912 - val_loss: 9341.9229\n",
      "Epoch 2546/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9370.2287 - val_loss: 9341.4600\n",
      "Epoch 2547/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9369.7666 - val_loss: 9340.9951\n",
      "Epoch 2548/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9369.3017 - val_loss: 9340.5332\n",
      "Epoch 2549/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9368.8393 - val_loss: 9340.0703\n",
      "Epoch 2550/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9368.3764 - val_loss: 9339.6055\n",
      "Epoch 2551/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9367.9121 - val_loss: 9339.1426\n",
      "Epoch 2552/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9367.4500 - val_loss: 9338.6807\n",
      "Epoch 2553/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9366.9868 - val_loss: 9338.2158\n",
      "Epoch 2554/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9366.5233 - val_loss: 9337.7529\n",
      "Epoch 2555/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9366.0614 - val_loss: 9337.2910\n",
      "Epoch 2556/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9365.5967 - val_loss: 9336.8281\n",
      "Epoch 2557/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9365.1342 - val_loss: 9336.3643\n",
      "Epoch 2558/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9364.6716 - val_loss: 9335.9014\n",
      "Epoch 2559/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9364.2071 - val_loss: 9335.4385\n",
      "Epoch 2560/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9363.7446 - val_loss: 9334.9746\n",
      "Epoch 2561/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 9363.2821 - val_loss: 9334.5117\n",
      "Epoch 2562/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9362.8182 - val_loss: 9334.0488\n",
      "Epoch 2563/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9362.3557 - val_loss: 9333.5869\n",
      "Epoch 2564/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9361.8924 - val_loss: 9333.1221\n",
      "Epoch 2565/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9361.4285 - val_loss: 9332.6582\n",
      "Epoch 2566/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9360.9668 - val_loss: 9332.1963\n",
      "Epoch 2567/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9360.5026 - val_loss: 9331.7314\n",
      "Epoch 2568/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9360.0390 - val_loss: 9331.2695\n",
      "Epoch 2569/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9359.5770 - val_loss: 9330.8066\n",
      "Epoch 2570/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9359.1129 - val_loss: 9330.3438\n",
      "Epoch 2571/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9358.6498 - val_loss: 9329.8799\n",
      "Epoch 2572/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9358.1874 - val_loss: 9329.4180\n",
      "Epoch 2573/3000\n",
      "742/742 [==============================] - 0s 142us/step - loss: 9357.7236 - val_loss: 9328.9541\n",
      "Epoch 2574/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9357.2607 - val_loss: 9328.4912\n",
      "Epoch 2575/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9356.7981 - val_loss: 9328.0264\n",
      "Epoch 2576/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9356.3339 - val_loss: 9327.5645\n",
      "Epoch 2577/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9355.8719 - val_loss: 9327.1025\n",
      "Epoch 2578/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 9355.4084 - val_loss: 9326.6377\n",
      "Epoch 2579/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9354.9442 - val_loss: 9326.1748\n",
      "Epoch 2580/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9354.4824 - val_loss: 9325.7119\n",
      "Epoch 2581/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9354.0183 - val_loss: 9325.2500\n",
      "Epoch 2582/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9353.5560 - val_loss: 9324.7861\n",
      "Epoch 2583/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9353.0927 - val_loss: 9324.3213\n",
      "Epoch 2584/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9352.6286 - val_loss: 9323.8604\n",
      "Epoch 2585/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9352.1661 - val_loss: 9323.3975\n",
      "Epoch 2586/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9351.7040 - val_loss: 9322.9326\n",
      "Epoch 2587/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9351.2392 - val_loss: 9322.4707\n",
      "Epoch 2588/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9350.7767 - val_loss: 9322.0078\n",
      "Epoch 2589/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9350.3137 - val_loss: 9321.5439\n",
      "Epoch 2590/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9349.8498 - val_loss: 9321.0801\n",
      "Epoch 2591/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9349.3874 - val_loss: 9320.6182\n",
      "Epoch 2592/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 123us/step - loss: 9348.9242 - val_loss: 9320.1533\n",
      "Epoch 2593/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9348.4607 - val_loss: 9319.6904\n",
      "Epoch 2594/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9347.9980 - val_loss: 9319.2285\n",
      "Epoch 2595/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9347.5342 - val_loss: 9318.7656\n",
      "Epoch 2596/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9347.0716 - val_loss: 9318.3008\n",
      "Epoch 2597/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9346.6092 - val_loss: 9317.8379\n",
      "Epoch 2598/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9346.1444 - val_loss: 9317.3760\n",
      "Epoch 2599/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9345.6820 - val_loss: 9316.9121\n",
      "Epoch 2600/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9345.2197 - val_loss: 9316.4492\n",
      "Epoch 2601/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9344.7557 - val_loss: 9315.9863\n",
      "Epoch 2602/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9344.2933 - val_loss: 9315.5234\n",
      "Epoch 2603/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9343.8298 - val_loss: 9315.0596\n",
      "Epoch 2604/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9343.3660 - val_loss: 9314.5957\n",
      "Epoch 2605/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9342.9043 - val_loss: 9314.1338\n",
      "Epoch 2606/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9342.4399 - val_loss: 9313.6689\n",
      "Epoch 2607/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9341.9765 - val_loss: 9313.2070\n",
      "Epoch 2608/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9341.5145 - val_loss: 9312.7441\n",
      "Epoch 2609/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9341.0505 - val_loss: 9312.2822\n",
      "Epoch 2610/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9340.5881 - val_loss: 9311.8174\n",
      "Epoch 2611/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9340.1250 - val_loss: 9311.3555\n",
      "Epoch 2612/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9339.6612 - val_loss: 9310.8926\n",
      "Epoch 2613/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9339.1989 - val_loss: 9310.4297\n",
      "Epoch 2614/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9338.7359 - val_loss: 9309.9658\n",
      "Epoch 2615/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9338.2721 - val_loss: 9309.5020\n",
      "Epoch 2616/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9337.8105 - val_loss: 9309.0400\n",
      "Epoch 2617/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9337.3461 - val_loss: 9308.5752\n",
      "Epoch 2618/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9336.8831 - val_loss: 9308.1133\n",
      "Epoch 2619/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9336.4210 - val_loss: 9307.6504\n",
      "Epoch 2620/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9335.9568 - val_loss: 9307.1885\n",
      "Epoch 2621/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9335.4945 - val_loss: 9306.7246\n",
      "Epoch 2622/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9335.0324 - val_loss: 9306.2617\n",
      "Epoch 2623/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9334.5682 - val_loss: 9305.7988\n",
      "Epoch 2624/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9334.1063 - val_loss: 9305.3369\n",
      "Epoch 2625/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9333.6429 - val_loss: 9304.8721\n",
      "Epoch 2626/3000\n",
      "742/742 [==============================] - 0s 128us/step - loss: 9333.1796 - val_loss: 9304.4092\n",
      "Epoch 2627/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9332.7177 - val_loss: 9303.9463\n",
      "Epoch 2628/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9332.2536 - val_loss: 9303.4854\n",
      "Epoch 2629/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9331.7912 - val_loss: 9303.0225\n",
      "Epoch 2630/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9331.3293 - val_loss: 9302.5576\n",
      "Epoch 2631/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9330.8651 - val_loss: 9302.0957\n",
      "Epoch 2632/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9330.4031 - val_loss: 9301.6338\n",
      "Epoch 2633/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9329.9399 - val_loss: 9301.1689\n",
      "Epoch 2634/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9329.4769 - val_loss: 9300.7070\n",
      "Epoch 2635/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9329.0155 - val_loss: 9300.2441\n",
      "Epoch 2636/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9328.5515 - val_loss: 9299.7822\n",
      "Epoch 2637/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9328.0891 - val_loss: 9299.3203\n",
      "Epoch 2638/3000\n",
      "742/742 [==============================] - 0s 134us/step - loss: 9327.6262 - val_loss: 9298.8564\n",
      "Epoch 2639/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9327.1628 - val_loss: 9298.3936\n",
      "Epoch 2640/3000\n",
      "742/742 [==============================] - 0s 144us/step - loss: 9326.7011 - val_loss: 9297.9307\n",
      "Epoch 2641/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9326.2372 - val_loss: 9297.4697\n",
      "Epoch 2642/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9325.7752 - val_loss: 9297.0059\n",
      "Epoch 2643/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9325.3134 - val_loss: 9296.5430\n",
      "Epoch 2644/3000\n",
      "742/742 [==============================] - 0s 133us/step - loss: 9324.8486 - val_loss: 9296.0801\n",
      "Epoch 2645/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9324.3875 - val_loss: 9295.6182\n",
      "Epoch 2646/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9323.9243 - val_loss: 9295.1533\n",
      "Epoch 2647/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9323.4609 - val_loss: 9294.6904\n",
      "Epoch 2648/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9322.9992 - val_loss: 9294.2295\n",
      "Epoch 2649/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9322.5360 - val_loss: 9293.7666\n",
      "Epoch 2650/3000\n",
      "742/742 [==============================] - 0s 143us/step - loss: 9322.0736 - val_loss: 9293.3047\n",
      "Epoch 2651/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9321.6109 - val_loss: 9292.8408\n",
      "Epoch 2652/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9321.1480 - val_loss: 9292.3770\n",
      "Epoch 2653/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9320.6857 - val_loss: 9291.9160\n",
      "Epoch 2654/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9320.2220 - val_loss: 9291.4541\n",
      "Epoch 2655/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9319.7594 - val_loss: 9290.9912\n",
      "Epoch 2656/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9319.2979 - val_loss: 9290.5264\n",
      "Epoch 2657/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9318.8339 - val_loss: 9290.0645\n",
      "Epoch 2658/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9318.3726 - val_loss: 9289.6025\n",
      "Epoch 2659/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9317.9089 - val_loss: 9289.1406\n",
      "Epoch 2660/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9317.4466 - val_loss: 9288.6768\n",
      "Epoch 2661/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9316.9841 - val_loss: 9288.2139\n",
      "Epoch 2662/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9316.5204 - val_loss: 9287.7510\n",
      "Epoch 2663/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9316.0579 - val_loss: 9287.2891\n",
      "Epoch 2664/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9315.5957 - val_loss: 9286.8252\n",
      "Epoch 2665/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9315.1323 - val_loss: 9286.3623\n",
      "Epoch 2666/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9314.6707 - val_loss: 9285.9004\n",
      "Epoch 2667/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9314.2068 - val_loss: 9285.4385\n",
      "Epoch 2668/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9313.7448 - val_loss: 9284.9746\n",
      "Epoch 2669/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9313.2825 - val_loss: 9284.5117\n",
      "Epoch 2670/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9312.8183 - val_loss: 9284.0498\n",
      "Epoch 2671/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9312.3574 - val_loss: 9283.5879\n",
      "Epoch 2672/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9311.8936 - val_loss: 9283.1250\n",
      "Epoch 2673/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9311.4310 - val_loss: 9282.6621\n",
      "Epoch 2674/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9310.9695 - val_loss: 9282.1992\n",
      "Epoch 2675/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9310.5057 - val_loss: 9281.7363\n",
      "Epoch 2676/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9310.0436 - val_loss: 9281.2744\n",
      "Epoch 2677/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9309.5803 - val_loss: 9280.8096\n",
      "Epoch 2678/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9309.1171 - val_loss: 9280.3486\n",
      "Epoch 2679/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9308.6555 - val_loss: 9279.8838\n",
      "Epoch 2680/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9308.1914 - val_loss: 9279.4229\n",
      "Epoch 2681/3000\n",
      "742/742 [==============================] - 0s 115us/step - loss: 9307.7295 - val_loss: 9278.9600\n",
      "Epoch 2682/3000\n",
      "742/742 [==============================] - 0s 107us/step - loss: 9307.2671 - val_loss: 9278.4971\n",
      "Epoch 2683/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9306.8035 - val_loss: 9278.0332\n",
      "Epoch 2684/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9306.3420 - val_loss: 9277.5713\n",
      "Epoch 2685/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9305.8784 - val_loss: 9277.1104\n",
      "Epoch 2686/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9305.4159 - val_loss: 9276.6475\n",
      "Epoch 2687/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9304.9541 - val_loss: 9276.1826\n",
      "Epoch 2688/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9304.4901 - val_loss: 9275.7207\n",
      "Epoch 2689/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9304.0282 - val_loss: 9275.2588\n",
      "Epoch 2690/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9303.5650 - val_loss: 9274.7969\n",
      "Epoch 2691/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9303.1029 - val_loss: 9274.3320\n",
      "Epoch 2692/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9302.6405 - val_loss: 9273.8701\n",
      "Epoch 2693/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9302.1767 - val_loss: 9273.4082\n",
      "Epoch 2694/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9301.7151 - val_loss: 9272.9463\n",
      "Epoch 2695/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9301.2519 - val_loss: 9272.4814\n",
      "Epoch 2696/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9300.7883 - val_loss: 9272.0186\n",
      "Epoch 2697/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9300.3270 - val_loss: 9271.5566\n",
      "Epoch 2698/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9299.8632 - val_loss: 9271.0947\n",
      "Epoch 2699/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9299.4009 - val_loss: 9270.6309\n",
      "Epoch 2700/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9298.9389 - val_loss: 9270.1689\n",
      "Epoch 2701/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9298.4754 - val_loss: 9269.7061\n",
      "Epoch 2702/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9298.0137 - val_loss: 9269.2432\n",
      "Epoch 2703/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9297.5495 - val_loss: 9268.7812\n",
      "Epoch 2704/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9297.0874 - val_loss: 9268.3174\n",
      "Epoch 2705/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9296.6251 - val_loss: 9267.8555\n",
      "Epoch 2706/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9296.1611 - val_loss: 9267.3926\n",
      "Epoch 2707/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9295.6999 - val_loss: 9266.9307\n",
      "Epoch 2708/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9295.2367 - val_loss: 9266.4658\n",
      "Epoch 2709/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9294.7737 - val_loss: 9266.0029\n",
      "Epoch 2710/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9294.3121 - val_loss: 9265.5410\n",
      "Epoch 2711/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9293.8482 - val_loss: 9265.0791\n",
      "Epoch 2712/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9293.3861 - val_loss: 9264.6172\n",
      "Epoch 2713/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9292.9234 - val_loss: 9264.1533\n",
      "Epoch 2714/3000\n",
      "742/742 [==============================] - 0s 121us/step - loss: 9292.4598 - val_loss: 9263.6895\n",
      "Epoch 2715/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9291.9979 - val_loss: 9263.2275\n",
      "Epoch 2716/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9291.5341 - val_loss: 9262.7666\n",
      "Epoch 2717/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9291.0720 - val_loss: 9262.3037\n",
      "Epoch 2718/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9290.6104 - val_loss: 9261.8389\n",
      "Epoch 2719/3000\n",
      "742/742 [==============================] - 0s 125us/step - loss: 9290.1464 - val_loss: 9261.3770\n",
      "Epoch 2720/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9289.6855 - val_loss: 9260.9150\n",
      "Epoch 2721/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9289.2210 - val_loss: 9260.4502\n",
      "Epoch 2722/3000\n",
      "742/742 [==============================] - 0s 118us/step - loss: 9288.7584 - val_loss: 9259.9893\n",
      "Epoch 2723/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9288.2968 - val_loss: 9259.5264\n",
      "Epoch 2724/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9287.8329 - val_loss: 9259.0645\n",
      "Epoch 2725/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9287.3710 - val_loss: 9258.6016\n",
      "Epoch 2726/3000\n",
      "742/742 [==============================] - 0s 136us/step - loss: 9286.9080 - val_loss: 9258.1377\n",
      "Epoch 2727/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9286.4452 - val_loss: 9257.6758\n",
      "Epoch 2728/3000\n",
      "742/742 [==============================] - 0s 137us/step - loss: 9285.9832 - val_loss: 9257.2129\n",
      "Epoch 2729/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9285.5191 - val_loss: 9256.7510\n",
      "Epoch 2730/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9285.0568 - val_loss: 9256.2871\n",
      "Epoch 2731/3000\n",
      "742/742 [==============================] - 0s 135us/step - loss: 9284.5948 - val_loss: 9255.8252\n",
      "Epoch 2732/3000\n",
      "742/742 [==============================] - 0s 122us/step - loss: 9284.1310 - val_loss: 9255.3613\n",
      "Epoch 2733/3000\n",
      "742/742 [==============================] - 0s 132us/step - loss: 9283.6695 - val_loss: 9254.9004\n",
      "Epoch 2734/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9283.2059 - val_loss: 9254.4385\n",
      "Epoch 2735/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9282.7434 - val_loss: 9253.9736\n",
      "Epoch 2736/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 135us/step - loss: 9282.2812 - val_loss: 9253.5117\n",
      "Epoch 2737/3000\n",
      "742/742 [==============================] - 0s 158us/step - loss: 9281.8182 - val_loss: 9253.0488\n",
      "Epoch 2738/3000\n",
      "742/742 [==============================] - 0s 149us/step - loss: 9281.3558 - val_loss: 9252.5869\n",
      "Epoch 2739/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9280.8930 - val_loss: 9252.1221\n",
      "Epoch 2740/3000\n",
      "742/742 [==============================] - 0s 116us/step - loss: 9280.4299 - val_loss: 9251.6582\n",
      "Epoch 2741/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9279.9678 - val_loss: 9251.1982\n",
      "Epoch 2742/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9279.5046 - val_loss: 9250.7354\n",
      "Epoch 2743/3000\n",
      "742/742 [==============================] - 0s 147us/step - loss: 9279.0423 - val_loss: 9250.2734\n",
      "Epoch 2744/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9278.5796 - val_loss: 9249.8096\n",
      "Epoch 2745/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9278.1160 - val_loss: 9249.3457\n",
      "Epoch 2746/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9277.6552 - val_loss: 9248.8838\n",
      "Epoch 2747/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9277.1903 - val_loss: 9248.4229\n",
      "Epoch 2748/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9276.7287 - val_loss: 9247.9600\n",
      "Epoch 2749/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9276.2666 - val_loss: 9247.4951\n",
      "Epoch 2750/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9275.8026 - val_loss: 9247.0332\n",
      "Epoch 2751/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9275.3407 - val_loss: 9246.5713\n",
      "Epoch 2752/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9274.8774 - val_loss: 9246.1094\n",
      "Epoch 2753/3000\n",
      "742/742 [==============================] - 0s 111us/step - loss: 9274.4147 - val_loss: 9245.6455\n",
      "Epoch 2754/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9273.9532 - val_loss: 9245.1826\n",
      "Epoch 2755/3000\n",
      "742/742 [==============================] - 0s 127us/step - loss: 9273.4892 - val_loss: 9244.7197\n",
      "Epoch 2756/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9273.0268 - val_loss: 9244.2578\n",
      "Epoch 2757/3000\n",
      "742/742 [==============================] - 0s 126us/step - loss: 9272.5643 - val_loss: 9243.7939\n",
      "Epoch 2758/3000\n",
      "742/742 [==============================] - 0s 120us/step - loss: 9272.1008 - val_loss: 9243.3311\n",
      "Epoch 2759/3000\n",
      "742/742 [==============================] - 0s 138us/step - loss: 9271.6395 - val_loss: 9242.8691\n",
      "Epoch 2760/3000\n",
      "742/742 [==============================] - 0s 123us/step - loss: 9271.1756 - val_loss: 9242.4072\n",
      "Epoch 2761/3000\n",
      "742/742 [==============================] - 0s 149us/step - loss: 9270.7139 - val_loss: 9241.9434\n",
      "Epoch 2762/3000\n",
      "742/742 [==============================] - 0s 147us/step - loss: 9270.2511 - val_loss: 9241.4805\n",
      "Epoch 2763/3000\n",
      "742/742 [==============================] - 0s 161us/step - loss: 9269.7877 - val_loss: 9241.0186\n",
      "Epoch 2764/3000\n",
      "742/742 [==============================] - 0s 130us/step - loss: 9269.3261 - val_loss: 9240.5557\n",
      "Epoch 2765/3000\n",
      "742/742 [==============================] - 0s 117us/step - loss: 9268.8622 - val_loss: 9240.0938\n",
      "Epoch 2766/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9268.4000 - val_loss: 9239.6299\n",
      "Epoch 2767/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9267.9374 - val_loss: 9239.1680\n",
      "Epoch 2768/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9267.4738 - val_loss: 9238.7051\n",
      "Epoch 2769/3000\n",
      "742/742 [==============================] - 0s 129us/step - loss: 9267.0124 - val_loss: 9238.2432\n",
      "Epoch 2770/3000\n",
      "742/742 [==============================] - 0s 114us/step - loss: 9266.5490 - val_loss: 9237.7783\n",
      "Epoch 2771/3000\n",
      "742/742 [==============================] - 0s 124us/step - loss: 9266.0859 - val_loss: 9237.3145\n",
      "Epoch 2772/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9265.6249 - val_loss: 9236.8535\n",
      "Epoch 2773/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9265.1601 - val_loss: 9236.3916\n",
      "Epoch 2774/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9264.6983 - val_loss: 9235.9297\n",
      "Epoch 2775/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9264.2358 - val_loss: 9235.4658\n",
      "Epoch 2776/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9263.7723 - val_loss: 9235.0020\n",
      "Epoch 2777/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9263.3105 - val_loss: 9234.5410\n",
      "Epoch 2778/3000\n",
      "742/742 [==============================] - 0s 105us/step - loss: 9262.8474 - val_loss: 9234.0781\n",
      "Epoch 2779/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9262.3847 - val_loss: 9233.6162\n",
      "Epoch 2780/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9261.9229 - val_loss: 9233.1514\n",
      "Epoch 2781/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9261.4588 - val_loss: 9232.6895\n",
      "Epoch 2782/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9260.9969 - val_loss: 9232.2275\n",
      "Epoch 2783/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9260.5338 - val_loss: 9231.7627\n",
      "Epoch 2784/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9260.0705 - val_loss: 9231.3018\n",
      "Epoch 2785/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9259.6092 - val_loss: 9230.8389\n",
      "Epoch 2786/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9259.1455 - val_loss: 9230.3770\n",
      "Epoch 2787/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9258.6830 - val_loss: 9229.9141\n",
      "Epoch 2788/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9258.2206 - val_loss: 9229.4502\n",
      "Epoch 2789/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9257.7575 - val_loss: 9228.9873\n",
      "Epoch 2790/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9257.2950 - val_loss: 9228.5254\n",
      "Epoch 2791/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9256.8317 - val_loss: 9228.0635\n",
      "Epoch 2792/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9256.3695 - val_loss: 9227.6006\n",
      "Epoch 2793/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9255.9076 - val_loss: 9227.1367\n",
      "Epoch 2794/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9255.4442 - val_loss: 9226.6748\n",
      "Epoch 2795/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9254.9822 - val_loss: 9226.2119\n",
      "Epoch 2796/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9254.5183 - val_loss: 9225.7500\n",
      "Epoch 2797/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9254.0560 - val_loss: 9225.2871\n",
      "Epoch 2798/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9253.5946 - val_loss: 9224.8242\n",
      "Epoch 2799/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9253.1299 - val_loss: 9224.3613\n",
      "Epoch 2800/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9252.6684 - val_loss: 9223.8994\n",
      "Epoch 2801/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9252.2056 - val_loss: 9223.4346\n",
      "Epoch 2802/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9251.7421 - val_loss: 9222.9717\n",
      "Epoch 2803/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9251.2803 - val_loss: 9222.5107\n",
      "Epoch 2804/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9250.8171 - val_loss: 9222.0479\n",
      "Epoch 2805/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9250.3541 - val_loss: 9221.5859\n",
      "Epoch 2806/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9249.8921 - val_loss: 9221.1221\n",
      "Epoch 2807/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9249.4285 - val_loss: 9220.6582\n",
      "Epoch 2808/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9248.9668 - val_loss: 9220.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2809/3000\n",
      "742/742 [==============================] - 0s 101us/step - loss: 9248.5030 - val_loss: 9219.7354\n",
      "Epoch 2810/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9248.0407 - val_loss: 9219.2725\n",
      "Epoch 2811/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9247.5791 - val_loss: 9218.8076\n",
      "Epoch 2812/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9247.1152 - val_loss: 9218.3457\n",
      "Epoch 2813/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9246.6539 - val_loss: 9217.8838\n",
      "Epoch 2814/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9246.1899 - val_loss: 9217.4219\n",
      "Epoch 2815/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9245.7279 - val_loss: 9216.9580\n",
      "Epoch 2816/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9245.2655 - val_loss: 9216.4951\n",
      "Epoch 2817/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9244.8017 - val_loss: 9216.0322\n",
      "Epoch 2818/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9244.3392 - val_loss: 9215.5703\n",
      "Epoch 2819/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9243.8766 - val_loss: 9215.1064\n",
      "Epoch 2820/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9243.4134 - val_loss: 9214.6436\n",
      "Epoch 2821/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9242.9519 - val_loss: 9214.1816\n",
      "Epoch 2822/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9242.4881 - val_loss: 9213.7197\n",
      "Epoch 2823/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9242.0257 - val_loss: 9213.2559\n",
      "Epoch 2824/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9241.5638 - val_loss: 9212.7930\n",
      "Epoch 2825/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9241.0996 - val_loss: 9212.3301\n",
      "Epoch 2826/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9240.6383 - val_loss: 9211.8691\n",
      "Epoch 2827/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9240.1747 - val_loss: 9211.4062\n",
      "Epoch 2828/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9239.7124 - val_loss: 9210.9424\n",
      "Epoch 2829/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9239.2501 - val_loss: 9210.4795\n",
      "Epoch 2830/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9238.7870 - val_loss: 9210.0176\n",
      "Epoch 2831/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9238.3249 - val_loss: 9209.5557\n",
      "Epoch 2832/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9237.8615 - val_loss: 9209.0908\n",
      "Epoch 2833/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9237.3983 - val_loss: 9208.6279\n",
      "Epoch 2834/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9236.9365 - val_loss: 9208.1660\n",
      "Epoch 2835/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9236.4728 - val_loss: 9207.7041\n",
      "Epoch 2836/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9236.0107 - val_loss: 9207.2412\n",
      "Epoch 2837/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9235.5483 - val_loss: 9206.7783\n",
      "Epoch 2838/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9235.0851 - val_loss: 9206.3145\n",
      "Epoch 2839/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9234.6233 - val_loss: 9205.8525\n",
      "Epoch 2840/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9234.1592 - val_loss: 9205.3906\n",
      "Epoch 2841/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9233.6974 - val_loss: 9204.9268\n",
      "Epoch 2842/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9233.2348 - val_loss: 9204.4639\n",
      "Epoch 2843/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9232.7713 - val_loss: 9204.0020\n",
      "Epoch 2844/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9232.3094 - val_loss: 9203.5400\n",
      "Epoch 2845/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9231.8463 - val_loss: 9203.0781\n",
      "Epoch 2846/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9231.3841 - val_loss: 9202.6133\n",
      "Epoch 2847/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9230.9217 - val_loss: 9202.1514\n",
      "Epoch 2848/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9230.4580 - val_loss: 9201.6885\n",
      "Epoch 2849/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9229.9962 - val_loss: 9201.2266\n",
      "Epoch 2850/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9229.5331 - val_loss: 9200.7627\n",
      "Epoch 2851/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9229.0695 - val_loss: 9200.2998\n",
      "Epoch 2852/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9228.6084 - val_loss: 9199.8379\n",
      "Epoch 2853/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9228.1445 - val_loss: 9199.3760\n",
      "Epoch 2854/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9227.6820 - val_loss: 9198.9121\n",
      "Epoch 2855/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9227.2198 - val_loss: 9198.4502\n",
      "Epoch 2856/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9226.7569 - val_loss: 9197.9863\n",
      "Epoch 2857/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9226.2939 - val_loss: 9197.5244\n",
      "Epoch 2858/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9225.8308 - val_loss: 9197.0615\n",
      "Epoch 2859/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9225.3686 - val_loss: 9196.5986\n",
      "Epoch 2860/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9224.9062 - val_loss: 9196.1367\n",
      "Epoch 2861/3000\n",
      "742/742 [==============================] - 0s 100us/step - loss: 9224.4430 - val_loss: 9195.6738\n",
      "Epoch 2862/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9223.9809 - val_loss: 9195.2119\n",
      "Epoch 2863/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9223.5178 - val_loss: 9194.7471\n",
      "Epoch 2864/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9223.0550 - val_loss: 9194.2832\n",
      "Epoch 2865/3000\n",
      "742/742 [==============================] - 0s 106us/step - loss: 9222.5933 - val_loss: 9193.8213\n",
      "Epoch 2866/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9222.1288 - val_loss: 9193.3604\n",
      "Epoch 2867/3000\n",
      "742/742 [==============================] - 0s 87us/step - loss: 9221.6672 - val_loss: 9192.8984\n",
      "Epoch 2868/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9221.2046 - val_loss: 9192.4346\n",
      "Epoch 2869/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9220.7410 - val_loss: 9191.9707\n",
      "Epoch 2870/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9220.2793 - val_loss: 9191.5088\n",
      "Epoch 2871/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9219.8157 - val_loss: 9191.0469\n",
      "Epoch 2872/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9219.3530 - val_loss: 9190.5850\n",
      "Epoch 2873/3000\n",
      "742/742 [==============================] - 0s 109us/step - loss: 9218.8915 - val_loss: 9190.1201\n",
      "Epoch 2874/3000\n",
      "742/742 [==============================] - 0s 110us/step - loss: 9218.4275 - val_loss: 9189.6582\n",
      "Epoch 2875/3000\n",
      "742/742 [==============================] - 0s 112us/step - loss: 9217.9657 - val_loss: 9189.1963\n",
      "Epoch 2876/3000\n",
      "742/742 [==============================] - 0s 108us/step - loss: 9217.5023 - val_loss: 9188.7314\n",
      "Epoch 2877/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9217.0394 - val_loss: 9188.2695\n",
      "Epoch 2878/3000\n",
      "742/742 [==============================] - 0s 103us/step - loss: 9216.5781 - val_loss: 9187.8076\n",
      "Epoch 2879/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9216.1142 - val_loss: 9187.3457\n",
      "Epoch 2880/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9215.6517 - val_loss: 9186.8828\n",
      "Epoch 2881/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9215.1893 - val_loss: 9186.4189\n",
      "Epoch 2882/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 0s 98us/step - loss: 9214.7264 - val_loss: 9185.9561\n",
      "Epoch 2883/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9214.2642 - val_loss: 9185.4941\n",
      "Epoch 2884/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9213.8005 - val_loss: 9185.0322\n",
      "Epoch 2885/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9213.3381 - val_loss: 9184.5684\n",
      "Epoch 2886/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9212.8760 - val_loss: 9184.1064\n",
      "Epoch 2887/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9212.4124 - val_loss: 9183.6426\n",
      "Epoch 2888/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9211.9507 - val_loss: 9183.1807\n",
      "Epoch 2889/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9211.4869 - val_loss: 9182.7188\n",
      "Epoch 2890/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9211.0248 - val_loss: 9182.2559\n",
      "Epoch 2891/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9210.5633 - val_loss: 9181.7920\n",
      "Epoch 2892/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9210.0987 - val_loss: 9181.3301\n",
      "Epoch 2893/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9209.6366 - val_loss: 9180.8672\n",
      "Epoch 2894/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9209.1740 - val_loss: 9180.4033\n",
      "Epoch 2895/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9208.7112 - val_loss: 9179.9404\n",
      "Epoch 2896/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9208.2490 - val_loss: 9179.4795\n",
      "Epoch 2897/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9207.7857 - val_loss: 9179.0166\n",
      "Epoch 2898/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9207.3234 - val_loss: 9178.5537\n",
      "Epoch 2899/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9206.8608 - val_loss: 9178.0908\n",
      "Epoch 2900/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9206.3973 - val_loss: 9177.6270\n",
      "Epoch 2901/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9205.9355 - val_loss: 9177.1660\n",
      "Epoch 2902/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9205.4716 - val_loss: 9176.7031\n",
      "Epoch 2903/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9205.0096 - val_loss: 9176.2412\n",
      "Epoch 2904/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9204.5480 - val_loss: 9175.7764\n",
      "Epoch 2905/3000\n",
      "742/742 [==============================] - 0s 102us/step - loss: 9204.0839 - val_loss: 9175.3145\n",
      "Epoch 2906/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9203.6219 - val_loss: 9174.8525\n",
      "Epoch 2907/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9203.1589 - val_loss: 9174.3906\n",
      "Epoch 2908/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9202.6964 - val_loss: 9173.9258\n",
      "Epoch 2909/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9202.2335 - val_loss: 9173.4639\n",
      "Epoch 2910/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9201.7705 - val_loss: 9173.0010\n",
      "Epoch 2911/3000\n",
      "742/742 [==============================] - 0s 113us/step - loss: 9201.3083 - val_loss: 9172.5391\n",
      "Epoch 2912/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9200.8453 - val_loss: 9172.0752\n",
      "Epoch 2913/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9200.3829 - val_loss: 9171.6123\n",
      "Epoch 2914/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9199.9207 - val_loss: 9171.1504\n",
      "Epoch 2915/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9199.4568 - val_loss: 9170.6885\n",
      "Epoch 2916/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9198.9946 - val_loss: 9170.2246\n",
      "Epoch 2917/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9198.5325 - val_loss: 9169.7617\n",
      "Epoch 2918/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9198.0683 - val_loss: 9169.2998\n",
      "Epoch 2919/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9197.6071 - val_loss: 9168.8369\n",
      "Epoch 2920/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9197.1434 - val_loss: 9168.3750\n",
      "Epoch 2921/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9196.6810 - val_loss: 9167.9102\n",
      "Epoch 2922/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9196.2185 - val_loss: 9167.4482\n",
      "Epoch 2923/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9195.7550 - val_loss: 9166.9863\n",
      "Epoch 2924/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9195.2932 - val_loss: 9166.5244\n",
      "Epoch 2925/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9194.8301 - val_loss: 9166.0596\n",
      "Epoch 2926/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9194.3671 - val_loss: 9165.5957\n",
      "Epoch 2927/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9193.9051 - val_loss: 9165.1338\n",
      "Epoch 2928/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9193.4413 - val_loss: 9164.6729\n",
      "Epoch 2929/3000\n",
      "742/742 [==============================] - 0s 99us/step - loss: 9192.9795 - val_loss: 9164.2100\n",
      "Epoch 2930/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9192.5171 - val_loss: 9163.7471\n",
      "Epoch 2931/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9192.0534 - val_loss: 9163.2832\n",
      "Epoch 2932/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9191.5924 - val_loss: 9162.8213\n",
      "Epoch 2933/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9191.1285 - val_loss: 9162.3594\n",
      "Epoch 2934/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9190.6664 - val_loss: 9161.8975\n",
      "Epoch 2935/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9190.2039 - val_loss: 9161.4326\n",
      "Epoch 2936/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9189.7401 - val_loss: 9160.9707\n",
      "Epoch 2937/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9189.2782 - val_loss: 9160.5088\n",
      "Epoch 2938/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9188.8149 - val_loss: 9160.0439\n",
      "Epoch 2939/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9188.3520 - val_loss: 9159.5820\n",
      "Epoch 2940/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9187.8905 - val_loss: 9159.1191\n",
      "Epoch 2941/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9187.4266 - val_loss: 9158.6572\n",
      "Epoch 2942/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9186.9649 - val_loss: 9158.1963\n",
      "Epoch 2943/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9186.5018 - val_loss: 9157.7314\n",
      "Epoch 2944/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9186.0383 - val_loss: 9157.2686\n",
      "Epoch 2945/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9185.5771 - val_loss: 9156.8066\n",
      "Epoch 2946/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9185.1130 - val_loss: 9156.3447\n",
      "Epoch 2947/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9184.6506 - val_loss: 9155.8809\n",
      "Epoch 2948/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9184.1887 - val_loss: 9155.4189\n",
      "Epoch 2949/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9183.7254 - val_loss: 9154.9551\n",
      "Epoch 2950/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9183.2632 - val_loss: 9154.4932\n",
      "Epoch 2951/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9182.7995 - val_loss: 9154.0312\n",
      "Epoch 2952/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9182.3376 - val_loss: 9153.5674\n",
      "Epoch 2953/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9181.8750 - val_loss: 9153.1045\n",
      "Epoch 2954/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9181.4112 - val_loss: 9152.6426\n",
      "Epoch 2955/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9180.9495 - val_loss: 9152.1797\n",
      "Epoch 2956/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9180.4865 - val_loss: 9151.7158\n",
      "Epoch 2957/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9180.0233 - val_loss: 9151.2520\n",
      "Epoch 2958/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9179.5615 - val_loss: 9150.7910\n",
      "Epoch 2959/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9179.0983 - val_loss: 9150.3291\n",
      "Epoch 2960/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9178.6356 - val_loss: 9149.8662\n",
      "Epoch 2961/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9178.1733 - val_loss: 9149.4033\n",
      "Epoch 2962/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9177.7099 - val_loss: 9148.9395\n",
      "Epoch 2963/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9177.2480 - val_loss: 9148.4775\n",
      "Epoch 2964/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9176.7840 - val_loss: 9148.0166\n",
      "Epoch 2965/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9176.3224 - val_loss: 9147.5537\n",
      "Epoch 2966/3000\n",
      "742/742 [==============================] - 0s 96us/step - loss: 9175.8604 - val_loss: 9147.0889\n",
      "Epoch 2967/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9175.3966 - val_loss: 9146.6270\n",
      "Epoch 2968/3000\n",
      "742/742 [==============================] - 0s 89us/step - loss: 9174.9349 - val_loss: 9146.1650\n",
      "Epoch 2969/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9174.4711 - val_loss: 9145.7002\n",
      "Epoch 2970/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9174.0084 - val_loss: 9145.2383\n",
      "Epoch 2971/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9173.5467 - val_loss: 9144.7764\n",
      "Epoch 2972/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9173.0829 - val_loss: 9144.3135\n",
      "Epoch 2973/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9172.6203 - val_loss: 9143.8516\n",
      "Epoch 2974/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9172.1579 - val_loss: 9143.3877\n",
      "Epoch 2975/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9171.6946 - val_loss: 9142.9248\n",
      "Epoch 2976/3000\n",
      "742/742 [==============================] - 0s 98us/step - loss: 9171.2330 - val_loss: 9142.4629\n",
      "Epoch 2977/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9170.7693 - val_loss: 9142.0010\n",
      "Epoch 2978/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9170.3067 - val_loss: 9141.5371\n",
      "Epoch 2979/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9169.8448 - val_loss: 9141.0742\n",
      "Epoch 2980/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9169.3808 - val_loss: 9140.6113\n",
      "Epoch 2981/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9168.9196 - val_loss: 9140.1494\n",
      "Epoch 2982/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9168.4559 - val_loss: 9139.6875\n",
      "Epoch 2983/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9167.9936 - val_loss: 9139.2246\n",
      "Epoch 2984/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9167.5318 - val_loss: 9138.7607\n",
      "Epoch 2985/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9167.0681 - val_loss: 9138.2988\n",
      "Epoch 2986/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9166.6062 - val_loss: 9137.8359\n",
      "Epoch 2987/3000\n",
      "742/742 [==============================] - 0s 95us/step - loss: 9166.1426 - val_loss: 9137.3721\n",
      "Epoch 2988/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9165.6800 - val_loss: 9136.9092\n",
      "Epoch 2989/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9165.2176 - val_loss: 9136.4463\n",
      "Epoch 2990/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9164.7536 - val_loss: 9135.9854\n",
      "Epoch 2991/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9164.2920 - val_loss: 9135.5225\n",
      "Epoch 2992/3000\n",
      "742/742 [==============================] - 0s 97us/step - loss: 9163.8297 - val_loss: 9135.0596\n",
      "Epoch 2993/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9163.3662 - val_loss: 9134.5957\n",
      "Epoch 2994/3000\n",
      "742/742 [==============================] - 0s 92us/step - loss: 9162.9049 - val_loss: 9134.1338\n",
      "Epoch 2995/3000\n",
      "742/742 [==============================] - 0s 94us/step - loss: 9162.4403 - val_loss: 9133.6719\n",
      "Epoch 2996/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9161.9785 - val_loss: 9133.2100\n",
      "Epoch 2997/3000\n",
      "742/742 [==============================] - 0s 91us/step - loss: 9161.5166 - val_loss: 9132.7451\n",
      "Epoch 2998/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9161.0525 - val_loss: 9132.2832\n",
      "Epoch 2999/3000\n",
      "742/742 [==============================] - 0s 93us/step - loss: 9160.5906 - val_loss: 9131.8213\n",
      "Epoch 3000/3000\n",
      "742/742 [==============================] - 0s 90us/step - loss: 9160.1274 - val_loss: 9131.3594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26c468f3eb8>"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(all_feature, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(250, input_length=X_train .shape[1], input_dim=X_train .shape[2],return_sequences=True))\n",
    "model.add(LSTM(100,return_sequences=True))\n",
    "# output shape: (1, 1)\n",
    "model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=3000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470666666666,\n",
       " 10562.470671875,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470671875,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.4706640625,\n",
       " 10562.4706640625,\n",
       " 10562.470666666666,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470658854167,\n",
       " 10562.470674479167,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470671875,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470666666666,\n",
       " 10562.470669270833,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470674479167,\n",
       " 10562.4706640625,\n",
       " 10562.470674479167,\n",
       " 10562.470669270833,\n",
       " 10562.470669270833,\n",
       " 10562.470671875,\n",
       " 10562.4706640625,\n",
       " 10562.470669270833,\n",
       " 10562.470666666666,\n",
       " 10562.470666666666,\n",
       " 10562.470661458334,\n",
       " 10562.470661458334,\n",
       " 10562.4706640625,\n",
       " 10562.470661458334,\n",
       " 10562.470658854167,\n",
       " 10562.470669270833,\n",
       " 10562.470658854167,\n",
       " 10562.470661458334,\n",
       " 10562.470674479167,\n",
       " 10562.470666666666]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35347 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32244 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33287 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 39511 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 35657 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25613 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25976 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35347 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32244 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33287 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 39511 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 35657 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25613 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25976 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#繪圖\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+ 1)\n",
    "plt.plot(epochs, loss,'bo',label='Training loss')\n",
    "plt.plot(epochs, val_loss,'b',label='Validation loss')\n",
    "plt.title('訓練與驗證的損失函數')\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "x = X_scaler.fit_transform(fdsd.iloc[:,1:Significant_factor.shape[1]])\n",
    "# PCA\n",
    "pca = PCA(n_components=0.9)# 保證降維後的資料保持90%的資訊\n",
    "pca.fit(x)\n",
    "asas = pca.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
